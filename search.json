[{"title":"dig命令使用详解","date":"2025-10-23T01:46:09.000Z","url":"/2025/10/23/2025/10/221500/","tags":[["DNS","/tags/DNS/"]],"categories":[["Linux","/categories/Linux/"]],"content":"dig（Domain Information Groper）是一个强大的命令行工具，用于查询 DNS 域名系统。它是网络管理员和开发人员诊断 DNS 问题的首选工具之一。本文将从基础到高级，全面介绍 dig 命令的使用方法。 什么是 dig 命令dig 是 BIND DNS 服务器软件包提供的 DNS 查询工具，可以用来查询 DNS 服务器的响应信息。相比传统的 nslookup 工具，dig 提供更详细的输出和更灵活的功能。 基础用法最简单的查询查询域名的 A 记录： 这个命令会返回有关 baidu.com 域名的详细信息，包括： DNS 查询头部信息 ANSWER 部分（查询结果） AUTHORITY 部分（权威 DNS 服务器） ADDITIONAL 部分（额外信息） 查询特定记录类型 常见参数详解-@ 指定 DNS 服务器 使用 -@ 参数可以指定 DNS 服务器进行查询，这对于测试特定 DNS 服务器的响应非常有用。 +short 简短输出默认情况下，dig 会输出很多详细信息。如果只想要 IP 地址： +noall +answer 只显示答案 这个组合参数只显示查询结果，不显示头部和统计信息。 +trace 跟踪查询过程 从根域名服务器开始，完整显示 DNS 查询的完整路径，包括每一步的查询过程。 +recurse 递归查询 这是默认行为，让本地 DNS 服务器递归查询。 +norecurse 非递归查询 只查询指定的 DNS 服务器，不进行递归查询。 -x 反向查询查询 IP 地址对应的域名： -p 指定端口 指定 DNS 查询使用的端口号（默认是 53）。 +time&#x3D; 设置超时 设置查询超时时间（秒）。 +tries 设置重试次数 设置查询重试次数。 实用技巧1. 批量查询多个域名 2. 监控 DNS 响应时间 3. 只获取 IPv4 地址 4. 只获取 IPv6 地址 5. 查询 TXT 记录（常用于验证域名所有权） 6. 查询 MX 记录（邮件服务器） 7. 测试 DNS 解析速度 8. 查看详细统计信息 高级用法查找域名解析的完整路径 这显示了从根服务器到权威服务器的完整解析路径。 查询 SOA 记录 获取域名的起始授权机构记录，包含管理邮箱、序列号等信息。 使用 dig 调试 DNS 保存查询结果到文件 组合多个选项 这个命令结合了 +noall（不显示标题）、+answer（显示答案）、+short（简短输出），只返回 IP 地址。 实际应用场景场景 1：DNS 故障排查 场景 2：CDN 节点测试 场景 3：验证 DNS 记录修改 场景 4：检查邮件服务器配置 输出信息解读理解 dig 的输出信息很重要： QUESTION SECTION：查询的问题 ANSWER SECTION：DNS 服务器的回答 AUTHORITY SECTION：权威 DNS 服务器信息 ADDITIONAL SECTION：额外的记录 Query time：查询耗时 SERVER：查询的 DNS 服务器 WHEN：查询时间 常见问题排查1. DNS 解析缓慢 查看各个阶段的耗时，定位瓶颈。 2. DNS 污染检测 对比不同 DNS 服务器返回的结果。 3. 解析异常 使用 trace 模式查看完整的解析路径，找出在哪一步出现问题。 dig vs 其他工具dig vs nslookup dig：输出更详细，更适合调试 nslookup：交互式，但输出不够详细 dig vs host dig 提供更详细的信息，host 输出更简洁。 总结dig 命令是 DNS 查询的强大工具，掌握它可以： 快速诊断 DNS 问题 测试 DNS 配置 分析网络性能 调试 DNS 相关故障 通过本文的介绍，相信你已经掌握了 dig 命令的基础和高级用法。在日常工作中，灵活运用这些技巧，能够大大提高 DNS 故障排查的效率。"},{"title":"DNS解析异常导致视频播放缓慢问题排查与解决","date":"2025-10-13T05:42:34.000Z","url":"/2025/10/13/2025/10/131645/","tags":[["dns","/tags/dns/"],["网络排查","/tags/%E7%BD%91%E7%BB%9C%E6%8E%92%E6%9F%A5/"],["视频播放","/tags/%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE/"]],"categories":[["Linux","/categories/Linux/"]],"content":"问题描述测试环境HLS视频播放出现严重延迟问题，用户反馈视频需要多次重试才能正常播放。通过排查发现是DNS解析异常导致的网络延迟问题。 问题现象业务线同学反馈测试环境的m3u8文件访问异常缓慢，视频播放需要多次重试才能成功。 问题URL： 排查过程1. 初步验证 本地播放器测试：使用IINA或VLC播放器无法复现问题 命令行测试：使用curl访问速度正常 2. 网络层排查 Ping测试：ping -c 5 hls.itaimei.test.taimei.com 速度正常 关键发现：不同环境返回的解析IP不一致，怀疑DNS解析问题 3. 浏览器性能分析 性能分析结果： DNS 查找：0 μs（正常） 初始连接：40.56秒 ⚠️ 请求&#x2F;响应：40.56秒 ⚠️ 问题定位：DNS解析耗时过长，导致整个请求延迟。 DNS问题排查1. DNS解析测试 2. 问题确认通过DNS解析测试发现： 不同DNS服务器返回不同的IP地址 部分DNS服务器解析速度极慢 存在异常的DNS记录 3. 解决方案临时解决：修改本地hosts文件，直接指定IP地址 根本解决：联系运维团队清理异常的DNS记录 问题总结根本原因DNS服务器配置异常，存在多个解析记录，其中部分记录指向了不可达或响应缓慢的服务器。 影响范围 主要影响浏览器访问 命令行工具（curl）和本地播放器不受影响 不同网络环境表现不一致 预防措施 DNS监控：建立DNS解析时间监控 多环境测试：在不同网络环境下进行测试 DNS记录管理：定期清理无效DNS记录 备用方案：配置CDN或负载均衡 排查经验 分层排查：从应用层到网络层逐步排查 对比测试：使用不同工具验证问题 环境差异：关注不同环境下的表现差异 性能分析：利用浏览器开发者工具分析网络性能 "},{"title":"图片添加alpha通道&设置透明度","date":"2025-10-11T09:28:09.000Z","url":"/2025/10/11/2025/10/091335/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"如果原图片没有alpha通道，你需要先添加alpha通道，然后设置透明度。以下是几种方法： 方法1：添加alpha通道并设置透明度（一步完成） 方法2：分步处理 方法3：使用alpha选项添加alpha通道 方法4：为不同格式的图片添加alpha通道 方法5：使用colorkey创建alpha通道（适合纯色背景） 关键参数说明： format=rgba：将图像转换为包含alpha通道的RGBA格式 如果原图是RGB格式，这一步会自动添加完全不透明的alpha通道 然后colorchannelmixer=aa=0.3将alpha值设置为30% 推荐命令： 这个命令会： 将图像转换为RGBA格式（自动添加alpha通道） 设置alpha通道值为0.3（30%透明度） 输出为包含透明度的PNG文件 处理完成后，建议用图像查看器检查透明度效果是否符合预期。"},{"title":"替换wps配置文件","date":"2025-09-25T16:00:00.000Z","url":"/2025/09/26/2025/09/261010/","tags":[["wps","/tags/wps/"]],"categories":[["Java","/categories/Java/"]],"content":"替换wps配置文件 前言wps转换pdf时每次都会记录打开的文件到最近历史中,历史文件记录在配置文件中随着文件越来越多,配置文件也会越来越大,造成wps打开过慢. 目标解决wps打开过慢的问题 解题思路因为会每次写入配置文件,那么在每次任务完成之后,还原配置文件即可 而且wps每次写入文件也是写入到临时文件,完成之后使用临时文件替换原文件. 方案设计 复制一份原始文件到指定目录 任务完成之后,将原始文件复制到配置文件夹覆盖 核心代码 效果展示 工具类 配置文件Office.conf [6.0]common\\WindowCount&#x3D;5wps\\Application%20Settings\\AutoRecoverAppInfo&#x3D;wps\\Application%20Settings\\BackupSuccess&#x3D;1wps\\Application%20Settings\\RestartAppInfo&#x3D;11552wps\\Application%20Settings\\UpdateRecoverCheckTag&#x3D;truecommon\\infoGUID&#x3D;{92F3D667-1E3B-31A5-D123-4664AFFE8F77}common\\lastwpsact&#x3D;1737535769638common\\AcceptedEULA&#x3D;truecommon\\Backup\\AutoRecoverFilePath&#x3D;&#x2F;root&#x2F;.local&#x2F;share&#x2F;Kingsoft&#x2F;office6&#x2F;data&#x2F;backupcommon\\Backup\\BackupInitMode&#x3D;0common\\Backup\\EnableBackup&#x3D;1common\\Backup\\EnableIncBackup&#x3D;1common\\Backup\\EnableTimeBackup&#x3D;0common\\Backup\\TimeBackupInterval&#x3D;1common\\LastClearBKTime&#x3D;2025-01-22 10:50:24common\\UserInfo\\AccountId&#x3D;common\\UserInfo\\AccountInitials&#x3D;common\\UserInfo\\AccountName&#x3D;common\\UserInfo\\KSOFirstStartFlag&#x3D;101plugins\\GrabScreen\\HideWhileGrabbingScreen&#x3D;falseplugins\\officespace\\RoamingStyleIndex&#x3D;2plugins\\officespace\\tempfiles&#x3D;plugins\\protecteyes\\wps\\nightmodealpha&#x3D;120plugins\\protecteyes\\wps\\type&#x3D;0plugins\\workarea\\ActivePid&#x3D;0wps\\Application%20Settings\\DocumentSaveAsMaintainCompatibility&#x3D;0wps\\Application%20Settings\\PrintHiddenText&#x3D;0wps\\uifile&#x3D;res&#x2F;wpsongmani.kuiwps\\wpsongmani\\TaskPanelSize&#x3D;@Size(0 0)wps\\wpsongmani\\ToolBarStates&#x3D;@ByteArray(\\0\\0\\0\\xff\\0\\0\\0\\0\\xfd\\0\\0\\0\\x1\\0\\0\\0\\x1\\0\\0\\0\\0\\0\\0\\0\\0\\xfc\\x1\\0\\0\\0\\x1\\xfb\\0\\0\\0&amp;\\0K\\0x\\0T\\0\\x61\\0s\\0k\\0P\\0\\x61\\0n\\0\\x65\\0\\x43\\0o\\0n\\0t\\0\\x61\\0i\\0n\\0\\x65\\0r\\0\\0\\0\\0\\0\\xff\\xff\\xff\\xff\\0\\0\\0\\x1e\\0\\xff\\xff\\xff\\0\\0\\b\\0\\0\\0\\x4\\t\\0\\0\\0\\x4\\0\\0\\0\\x4\\0\\0\\0\\b\\0\\0\\0\\b\\xfc\\0\\0\\0\\0)plugins\\kstartpage\\officenav_new_v1\\seqEntry\\CommonEntrys\\newfromtemplate&#x3D;1plugins\\protecteyes\\wps\\shownightmode&#x3D;0wps\\Application%20Settings\\PrintFieldCodes&#x3D;0plugins\\kstartpage\\officenav_new_v1\\seqEntry\\CommonEntrys\\openfiledialog&#x3D;2wps\\Application%20Settings\\PrintDrawingObjects&#x3D;1plugins\\kstartpage\\officenav_new_v1\\seqEntry\\CommonEntrys\\filelist_noref&#x3D;3wps\\Application%20Settings\\UpdateFieldsAtPrint&#x3D;0plugins\\kstartpage\\officenav_new_v1\\seqEntry\\CommonEntrys\\companymanage&#x3D;4wps\\Application%20Settings\\UpdateLinksAtPrint&#x3D;0plugins\\kstartpage\\officenav_new_v1\\seqEntry\\CommonEntrys\\separator&#x3D;5wps\\Application%20Settings\\PrintReverse&#x3D;0plugins\\kstartpage\\officenav_new_v1\\seqEntry\\AppcenterEntrys\\docershop&#x3D;0wps\\Application%20Settings\\DocumentsPath&#x3D;&#x2F;root&#x2F;Documentsplugins\\kstartpage\\officenav_new_v1\\seqEntry\\AppcenterEntrys\\kpromeeducourse&#x3D;1wps\\Application%20Settings\\UserTemplatePath&#x3D;&#x2F;root&#x2F;.local&#x2F;share&#x2F;Kingsoft&#x2F;office6&#x2F;templates&#x2F;wps&#x2F;en_USplugins\\kstartpage\\officenav_new_v1\\seqEntry\\AppcenterEntrys\\kpromecompanyintro&#x3D;2wps\\Application%20Settings\\ReplaceSelection&#x3D;1plugins\\kstartpage\\officenav_new_v1\\seqEntry\\AppcenterEntrys\\konlinemeeting&#x3D;3wps\\Application%20Settings\\AllowDragAndDrop&#x3D;1plugins\\kstartpage\\officenav_new_v1\\seqEntry\\AppcenterEntrys\\docerchuangkit&#x3D;4wps\\Application%20Settings\\TabIndentKey&#x3D;1plugins\\kstartpage\\officenav_new_v1\\seqEntry\\AppcenterEntrys\\mywallet&#x3D;5wps\\Application%20Settings\\DefaultHighlightColor&#x3D;65535wps\\Application%20Settings\\DisplayGridLines&#x3D;0wps\\Application%20Settings\\MapPaperSize&#x3D;1wps\\Application%20Settings\\CommentsColor&#x3D;-16777216wps\\Application%20Settings\\CtrlClickHyperlinkToOpen&#x3D;1wps\\Application%20Settings\\UseCharacterUnit&#x3D;1wps\\Application%20Settings\\UserName&#x3D;wps\\Application%20Settings\\UserInitials&#x3D;wps\\Application%20Settings\\UserAddress&#x3D;wps\\Application%20Settings\\PrintEvenPagesInAscendingOrder&#x3D;1wps\\Application%20Settings\\PrintOddPagesInAscendingOrder&#x3D;0wps\\Application%20Settings\\DisplayStatusBar&#x3D;1wps\\Application%20Settings\\DisplayTaskPaneAtStartup&#x3D;0wps\\Application%20Settings\\DisplayRecentFiles&#x3D;1wps\\Application%20Settings\\DefaultSaveFormat&#x3D;MSWORD12wps\\Application%20Settings\\BookmarksHidden&#x3D;0wps\\Application%20Settings\\BrowserTarget&#x3D;1wps\\Application%20Settings\\ShowWhat&#x3D;73wps\\Application%20Settings\\ShowEscChar&#x3D;0wps\\Application%20Settings\\FieldShadingShow&#x3D;2wps\\Application%20Settings\\ShowPageBorder&#x3D;0wps\\Application%20Settings\\TOCTabLeader&#x3D;1wps\\Application%20Settings\\SnapToGrid&#x3D;0wps\\Application%20Settings\\SnapToOtherShape&#x3D;0wps\\Application%20Settings\\DisplayRulers&#x3D;0wps\\Application%20Settings\\DisplayVerticalRuler&#x3D;1wps\\Application%20Settings\\PreviewZoom&#x3D;100wps\\Application%20Settings\\PreviewPageFit&#x3D;0wps\\Application%20Settings\\PreviewPageColumns&#x3D;99wps\\Application%20Settings\\PreviewPageRows&#x3D;1wps\\Application%20Settings\\BlueScreen&#x3D;0wps\\Application%20Settings\\BlueScreenBkColor&#x3D;4278190208wps\\Application%20Settings\\BlueScreenFrColor&#x3D;4294967295wps\\Application%20Settings\\InsertedTextMark&#x3D;3wps\\Application%20Settings\\InsertedTextColor&#x3D;-16777216wps\\Application%20Settings\\DeletedTextMark&#x3D;1wps\\Application%20Settings\\DeletedTextColor&#x3D;-16777216wps\\Application%20Settings\\RevisedPropertiesMark&#x3D;0wps\\Application%20Settings\\RevisedPropertiesColor&#x3D;-16777216wps\\Application%20Settings\\RevisedLinesMark&#x3D;3wps\\Application%20Settings\\RevisedLinesColor&#x3D;0wps\\Application%20Settings\\RevisionsBalloonPrintOrientation&#x3D;1wps\\Application%20Settings\\RevisionsBalloonTitle&#x3D;2wps\\Application%20Settings\\RevisionMode&#x3D;0wps\\Application%20Settings\\BalloonWidthAbs&#x3D;1wps\\Application%20Settings\\BalloonWidth&#x3D;5329wps\\Application%20Settings\\BalloonSide&#x3D;1wps\\Application%20Settings\\BalloonShowConnectLine&#x3D;1wps\\Application%20Settings\\DefaultTemplateFile&#x3D;&#x2F;root&#x2F;.local&#x2F;share&#x2F;Kingsoft&#x2F;office6&#x2F;templates&#x2F;wps&#x2F;en_US&#x2F;Normal.dotmwps\\Application%20Settings\\CreateBackupOnFirstSave&#x3D;1wps\\Application%20Settings\\ViewPdfAfterExport&#x3D;0wps\\Application%20Settings\\ExportTOC&#x3D;1wps\\Application%20Settings\\ExportHyperlinks&#x3D;1wps\\Application%20Settings\\ExportCommentsMode&#x3D;3wps\\Application%20Settings\\ExportFootnotes&#x3D;1wps\\Application%20Settings\\ExportEndnotes&#x3D;1wps\\Application%20Settings\\ExportSummaryInfo&#x3D;1wps\\Application%20Settings\\PdfPrintRight&#x3D;0wps\\Application%20Settings\\PdfEditRight&#x3D;0wps\\Application%20Settings\\PdfRights&#x3D;0wps\\Application%20Settings\\ExportTitleStyle&#x3D;1wps\\Application%20Settings\\ExportBuildinStyle&#x3D;0wps\\Application%20Settings\\ExportCustomStyle&#x3D;0wps\\Application%20Settings\\BuildinStyleLevel&#x3D;-1wps\\Application%20Settings\\CustomStyleLevel&#x3D;-1wps\\Application%20Settings\\ConvertBookmark&#x3D;0wps\\Application%20Settings\\ConvertBookmarkLevel&#x3D;-1wps\\Application%20Settings\\DisplayScreenTips&#x3D;1wps\\Application%20Settings\\NewFileBehavior&#x3D;0wps\\Application%20Settings\\TCSCDirection&#x3D;0wps\\Application%20Settings\\TCSCTransUnit&#x3D;1wps\\Application%20Settings\\TCSCUseVariants&#x3D;0wps\\Application%20Settings\\SplChkIgnoreUppercase&#x3D;1wps\\Application%20Settings\\SplChkIgnoreMixedDigits&#x3D;1wps\\Application%20Settings\\SplChkSuggestFromMainDictionaryOnly&#x3D;1wps\\Application%20Settings\\SplChkSuggestSpellingCorrections&#x3D;1wps\\Application%20Settings\\SaveInterval&#x3D;60000wps\\Application%20Settings\\NewDocumentDocmapCared&#x3D;0wps\\Application%20Settings\\NewDocumentDocmapDrawLine&#x3D;1wps\\Application%20Settings\\NewDocumentDocmapShowBmk&#x3D;0wps\\Application%20Settings\\DisplayPageBoundaries&#x3D;1wps\\Application%20Settings\\AllowClickAndType&#x3D;1wps\\Application%20Settings\\DiscernPerson&#x3D;0wps\\Application%20Settings\\DiscernLocation&#x3D;0wps\\Application%20Settings\\MatchFuzzyCase&#x3D;1wps\\Application%20Settings\\MatchFuzzyByte&#x3D;1wps\\Application%20Settings\\MatchFuzzyHiragana&#x3D;1wps\\Application%20Settings\\MatchFuzzySmallKana&#x3D;1wps\\Application%20Settings\\MatchFuzzyDash&#x3D;1wps\\Application%20Settings\\MatchFuzzyIterationMark&#x3D;1wps\\Application%20Settings\\MatchFuzzyKanji&#x3D;1wps\\Application%20Settings\\MatchFuzzyOldKana&#x3D;1wps\\Application%20Settings\\MatchFuzzyProlongedSoundMark&#x3D;1wps\\Application%20Settings\\MatchFuzzyDZ&#x3D;1wps\\Application%20Settings\\MatchFuzzyBV&#x3D;1wps\\Application%20Settings\\MatchFuzzyTC&#x3D;1wps\\Application%20Settings\\MatchFuzzyHF&#x3D;1wps\\Application%20Settings\\MatchFuzzyZJ&#x3D;1wps\\Application%20Settings\\MatchFuzzyAY&#x3D;1wps\\Application%20Settings\\MatchFuzzyKiKu&#x3D;1wps\\Application%20Settings\\MatchFuzzyPunctuation&#x3D;1wps\\Application%20Settings\\MatchFuzzySpace&#x3D;1wps\\Application%20Settings\\AutoFormatApplyLists&#x3D;1wps\\Application%20Settings\\CompareSideBySide_Wnd1_LeftTop&#x3D;0wps\\Application%20Settings\\CompareSideBySide_Wnd1_WidthHeight&#x3D;0wps\\Application%20Settings\\CompareSideBySide_Wnd2_LeftTop&#x3D;0wps\\Application%20Settings\\CompareSideBySide_Wnd2_WidthHeight&#x3D;0wps\\Application%20Settings\\AutoCorrect_CorrectSentenceCaps&#x3D;0wps\\Application%20Settings\\AutoCorrect_CorrectCapsLock&#x3D;0wps\\Application%20Settings\\DblClk_ToggleHeaderFooterView&#x3D;1wps\\Application%20Settings\\AutoCorrect_CorrectDays&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeReplaceOrdinals&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeReplaceHyperlinks&#x3D;1wps\\Application%20Settings\\PasteType&#x3D;0wps\\Application%20Settings\\AutoApplyAsYouTypeAdjustWordWrap&#x3D;1wps\\Application%20Settings\\ShowSpellingIgnoredWords&#x3D;0wps\\Application%20Settings\\CheckSpellingAsYouType&#x3D;0wps\\Application%20Settings\\CheckSpellingIgnoreInternetAndFileAddresses&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeApplyNumberedLists&#x3D;1wps\\Application%20Settings\\DisplayPasteOptions&#x3D;1wps\\Application%20Settings\\PasteFormatMode&#x3D;16wps\\Application%20Settings\\HideTaskPanelAfterDocumentOpen&#x3D;1wps\\Application%20Settings\\NewDocumentDocmapShow&#x3D;0wps\\Application%20Settings\\RecentPrintMode&#x3D;0wps\\Application%20Settings\\SmartParaSelection&#x3D;1wps\\Application%20Settings\\AutoCircleNumber&#x3D;1wps\\Application%20Settings\\DocumentTab&#x3D;29wps\\Application%20Settings\\ShowTableAdjustLabel&#x3D;1wps\\Application%20Settings\\ShowParaAdjustButton&#x3D;1wps\\Application%20Settings\\StartupShowNavigationPane&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeReplaceQuotes&#x3D;1wps\\Application%20Settings\\CheckGrammarWithSpelling&#x3D;1wps\\Application%20Settings\\ShowMarkupOpenSave&#x3D;1wps\\Application%20Settings\\CheckGrammarAsYouType&#x3D;1wps\\Application%20Settings\\SmartCutPaste&#x3D;0wps\\Application%20Settings\\UserCaptionLabels&#x3D;wps\\Application%20Settings\\EnableBackup&#x3D;1wps\\Application%20Settings\\PageNumVisible&#x3D;1wps\\Application%20Settings\\PageAreaVisible&#x3D;1wps\\Application%20Settings\\SectionVisible&#x3D;1wps\\Application%20Settings\\LineNumVisible&#x3D;1wps\\Application%20Settings\\ColumnVisible&#x3D;1wps\\Application%20Settings\\WordCountVisible&#x3D;1wps\\Application%20Settings\\OvertypeVisible&#x3D;0wps\\Application%20Settings\\SpellCheckVisible&#x3D;1wps\\Application%20Settings\\UnitVisible&#x3D;0wps\\Application%20Settings\\ViewShortcutVisible&#x3D;1wps\\Application%20Settings\\ZoomSliderVisible&#x3D;1wps\\Application%20Settings\\ZoomRatioVisible&#x3D;1wps\\Application%20Settings\\TrackVisible&#x3D;0wps\\Application%20Settings\\ButtonFieldClicks&#x3D;2wps\\Application%20Settings\\PictureWrapType&#x3D;0wps\\Application%20Settings\\CheckHangulEndings&#x3D;1wps\\Application%20Settings\\AutoCorrect_CorrectInitialCaps&#x3D;1wps\\Application%20Settings\\AutoCorrect_CorrectKeyboardSetting&#x3D;0wps\\Application%20Settings\\AutoCorrect_CorrectTableCells&#x3D;1wps\\Application%20Settings\\AutoCorrect_DisplayAutoCorrectOptions&#x3D;1wps\\Application%20Settings\\AutoCorrect_ReplaceText&#x3D;1wps\\Application%20Settings\\AutoCorrect_ReplaceTextFromSpellingChecker&#x3D;1wps\\Application%20Settings\\AutoCorrect_FirstLetterAutoAdd&#x3D;1wps\\Application%20Settings\\AutoCorrect_TwoInitialCapsAutoAdd&#x3D;1wps\\Application%20Settings\\AutoCorrect_CorrectHangulAndAlphabet&#x3D;0wps\\Application%20Settings\\AutoCorrect_HangulAndAlphabetAutoAdd&#x3D;1wps\\Application%20Settings\\AutoCorrect_OtherCorrectionsAutoAdd&#x3D;1wps\\Application%20Settings\\WPHelp&#x3D;0wps\\Application%20Settings\\WPDocNavKeys&#x3D;0wps\\Application%20Settings\\Pagination&#x3D;0wps\\Application%20Settings\\EnableSound&#x3D;0wps\\Application%20Settings\\ConfirmConversions&#x3D;0wps\\Application%20Settings\\UpdateLinksAtOpen&#x3D;1wps\\Application%20Settings\\SendMailAttach&#x3D;1wps\\Application%20Settings\\ShortMenuNames&#x3D;0wps\\Application%20Settings\\RTFInClipboard&#x3D;1wps\\Application%20Settings\\PrintProperties&#x3D;0wps\\Application%20Settings\\EnvelopeFeederInstalled&#x3D;0wps\\Application%20Settings\\PrintBackground&#x3D;1wps\\Application%20Settings\\DefaultTray&#x3D;wps\\Application%20Settings\\DefaultTrayID&#x3D;0wps\\Application%20Settings\\AllowFastSave&#x3D;1wps\\Application%20Settings\\SavePropertiesPrompt&#x3D;0wps\\Application%20Settings\\SaveNormalPrompt&#x3D;0wps\\Application%20Settings\\AutoWordSelection&#x3D;0wps\\Application%20Settings\\INSKeyForPaste&#x3D;0wps\\Application%20Settings\\PictureEditor&#x3D;wps\\Application%20Settings\\AnimateScreenMovements&#x3D;1wps\\Application%20Settings\\VirusProtection&#x3D;0wps\\Application%20Settings\\SnapToShapes&#x3D;0wps\\Application%20Settings\\InlineConversion&#x3D;1wps\\Application%20Settings\\IMEAutomaticControl&#x3D;1wps\\Application%20Settings\\AutoFormatApplyHeadings&#x3D;1wps\\Application%20Settings\\AutoFormatApplyBulletedLists&#x3D;1wps\\Application%20Settings\\AutoFormatApplyOtherParas&#x3D;1wps\\Application%20Settings\\AutoFormatReplaceQuotes&#x3D;1wps\\Application%20Settings\\AutoFormatReplaceSymbols&#x3D;1wps\\Application%20Settings\\AutoFormatReplaceOrdinals&#x3D;1wps\\Application%20Settings\\AutoFormatReplaceFractions&#x3D;0wps\\Application%20Settings\\AutoFormatReplacePlainTextEmphasis&#x3D;1wps\\Application%20Settings\\AutoFormatPreserveStyles&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeApplyHeadings&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeApplyBorders&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeApplyBulletedLists&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeReplaceSymbols&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeReplaceFractions&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeReplacePlainTextEmphasis&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeFormatListItemBeginning&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeDefineStyles&#x3D;0wps\\Application%20Settings\\AutoFormatPlainTextWordMail&#x3D;1wps\\Application%20Settings\\AutoFormatReplaceHyperlinks&#x3D;1wps\\Application%20Settings\\ShowReadabilityStatistics&#x3D;0wps\\Application%20Settings\\PrintDraft&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeApplyTables&#x3D;1wps\\Application%20Settings\\AutoFormatApplyFirstIndents&#x3D;1wps\\Application%20Settings\\AutoFormatMatchParentheses&#x3D;1wps\\Application%20Settings\\AutoFormatReplaceFarEastDashes&#x3D;1wps\\Application%20Settings\\AutoFormatDeleteAutoSpaces&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeApplyFirstIndents&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeApplyDates&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeApplyClosings&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeMatchParentheses&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeReplaceFarEastDashes&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeDeleteAutoSpaces&#x3D;0wps\\Application%20Settings\\AutoFormatAsYouTypeInsertClosings&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeAutoLetterWizard&#x3D;1wps\\Application%20Settings\\AutoFormatAsYouTypeInsertOvers&#x3D;1wps\\Application%20Settings\\ApplyFarEastFontsToAscii&#x3D;1wps\\Application%20Settings\\ConvertHighAnsiToFarEast&#x3D;1wps\\Application%20Settings\\EnableMisusedWordsDictionary&#x3D;1wps\\Application%20Settings\\AllowCombinedAuxiliaryForms&#x3D;1wps\\Application%20Settings\\HangulHanjaFastConversion&#x3D;1wps\\Application%20Settings\\EnableHangulHanjaRecentOrdering&#x3D;1wps\\Application%20Settings\\MultipleWordConversionsMode&#x3D;0wps\\Application%20Settings\\AllowCompoundNounProcessing&#x3D;1wps\\Application%20Settings\\AutoKeyboardSwitching&#x3D;1wps\\Application%20Settings\\ShowDiacritics&#x3D;1wps\\Application%20Settings\\ShowControlCharacters&#x3D;1wps\\Application%20Settings\\AddControlCharacters&#x3D;1wps\\Application%20Settings\\AddBiDirectionalMarksWhenSavingTextFile&#x3D;1wps\\Application%20Settings\\StrictInitialAlefHamza&#x3D;1wps\\Application%20Settings\\StrictFinalYaa&#x3D;1wps\\Application%20Settings\\UseGermanSpellingReform&#x3D;1wps\\Application%20Settings\\AddHebDoubleQuote&#x3D;1wps\\Application%20Settings\\UseDiffDiacColor&#x3D;1wps\\Application%20Settings\\DiacriticColorVal&#x3D;-16777216wps\\Application%20Settings\\OptimizeForWord97byDefault&#x3D;1wps\\Application%20Settings\\LocalNetworkFile&#x3D;1wps\\Application%20Settings\\TypeNReplace&#x3D;1wps\\Application%20Settings\\SequenceCheck&#x3D;1wps\\Application%20Settings\\BackgroundOpen&#x3D;1wps\\Application%20Settings\\DisableFeaturesbyDefault&#x3D;1wps\\Application%20Settings\\PasteAdjustWordSpacing&#x3D;1wps\\Application%20Settings\\PasteAdjustParagraphSpacing&#x3D;1wps\\Application%20Settings\\PasteAdjustTableFormatting&#x3D;1wps\\Application%20Settings\\PasteSmartStyleBehavior&#x3D;1wps\\Application%20Settings\\PasteMergeFromPPT&#x3D;1wps\\Application%20Settings\\PasteMergeFromXL&#x3D;1wps\\Application%20Settings\\PasteSmartCutPaste&#x3D;1wps\\Application%20Settings\\PromptUpdateStyle&#x3D;1wps\\Application%20Settings\\LabelSmartTags&#x3D;0wps\\Application%20Settings\\DisplaySmartTagButtons&#x3D;1wps\\Application%20Settings\\WarnBeforeSavingPrintingSendingMarkup&#x3D;1wps\\Application%20Settings\\StoreRSIDOnSave&#x3D;1wps\\Application%20Settings\\ShowFormatError&#x3D;1wps\\Application%20Settings\\FormatScanning&#x3D;1wps\\Application%20Settings\\PasteMergeLists&#x3D;1wps\\Application%20Settings\\AutoCreateNewDrawings&#x3D;1wps\\Application%20Settings\\PrintXMLTag&#x3D;1wps\\Application%20Settings\\PrintBackgrounds&#x3D;0wps\\Application%20Settings\\AllowReadingMode&#x3D;1wps\\Application%20Settings\\SmartCursoring&#x3D;1wps\\Application%20Settings\\ShowSelectionFloaties&#x3D;1wps\\Application%20Settings\\ShowMenuFloaties&#x3D;1wps\\Application%20Settings\\ShowDevTools&#x3D;1wps\\Application%20Settings\\EnableLivePreview&#x3D;1wps\\Application%20Settings\\OMathAutoBuildUp&#x3D;1wps\\Application%20Settings\\AlwaysUseClearType&#x3D;0wps\\Application%20Settings\\PasteOptionKeepBulletsAndNumbers&#x3D;1wps\\Application%20Settings\\INSKeyForOvertype&#x3D;1wps\\Application%20Settings\\RepeatWord&#x3D;1wps\\Application%20Settings\\ContextualSpeller&#x3D;1wps\\Application%20Settings\\OMathCopyLF&#x3D;1wps\\Application%20Settings\\UseNormalStyleForList&#x3D;0wps\\Application%20Settings\\AllowOpenInDraftView&#x3D;0wps\\Application%20Settings\\EnableLegacyIMEMode&#x3D;0wps\\Application%20Settings\\DoNotPromptForConvert&#x3D;0wps\\Application%20Settings\\PrecisePositioning&#x3D;0wps\\Application%20Settings\\StrictTaaMarboota&#x3D;1wps\\Application%20Settings\\StrictRussianE&#x3D;0wps\\Application%20Settings\\UpdateFieldsWithTrackedChangesAtPrint&#x3D;1wps\\Application%20Settings\\DisplayAlignmentGuides&#x3D;1wps\\Application%20Settings\\PageAlignmentGuides&#x3D;1wps\\Application%20Settings\\MarginAlignmentGuides&#x3D;1wps\\Application%20Settings\\ParagraphAlignmentGuides&#x3D;1wps\\Application%20Settings\\EnableLiveDrag&#x3D;1wps\\Application%20Settings\\UseSubPixelPositioning&#x3D;1wps\\Application%20Settings\\AlertIfNotDefault&#x3D;0wps\\Application%20Settings\\EnableProofingToolsAdvertisement&#x3D;1wps\\Application%20Settings\\PreferCloudSaveLocations&#x3D;1wps\\Application%20Settings\\SkyDriveSignInOption&#x3D;1wps\\Application%20Settings\\ExpandHeadingsOnOpen&#x3D;0wps\\Application%20Settings\\UseLocalUserInfo&#x3D;0wps\\Application%20Settings\\ArabicNumeral&#x3D;0wps\\Application%20Settings\\MonthNames&#x3D;0wps\\Application%20Settings\\CursorMovement&#x3D;0wps\\Application%20Settings\\VisualSelection&#x3D;0wps\\Application%20Settings\\HebrewMode&#x3D;0wps\\Application%20Settings\\ArabicMode&#x3D;2wps\\Application%20Settings\\InterpretHighAnsi&#x3D;2wps\\Application%20Settings\\DisableFeaturesIntroducedAfterbyDefault&#x3D;-1wps\\Application%20Settings\\DefaultTextEncoding&#x3D;936wps\\Application%20Settings\\MoveToTextColor&#x3D;-1wps\\Application%20Settings\\MoveFromTextColor&#x3D;-1wps\\Application%20Settings\\PrintComments&#x3D;0wps\\Application%20Settings\\CreateBackup&#x3D;0wps\\Application%20Settings\\BackgroundSave&#x3D;0wps\\Application%20Settings\\DefaultOpenFormat&#x3D;6wps\\Application%20Settings\\AllowPixelUnits&#x3D;0wps\\Application%20Settings\\AllowAccentedUppercase&#x3D;0wps\\Application%20Settings\\AutoDeleteParaIndentOnAlignCenter&#x3D;0wps\\Application%20Settings\\EmbedFontsRemind&#x3D;1wps\\Application%20Settings\\DefaultTableSeparator&#x3D;wps\\Application%20Settings\\IsUsingRecommendedBalloonWidth&#x3D;1wps\\Application%20Settings\\MaxUndoStep&#x3D;128wps\\Application%20Settings\\ShowMarkupAreaHighlight&#x3D;1wps\\Application%20Settings\\ShowFontPreviewLabel&#x3D;1wps\\Application%20Settings\\EmbedPrivateFonts&#x3D;0wps\\Application%20Settings\\StyleAreaWidth&#x3D;0wps\\Application%20Settings\\IndexTabLeader&#x3D;1wps\\Application%20Settings\\ShowStylePreviews&#x3D;0wps\\Application%20Settings\\RestrictLinkedStyles&#x3D;0wps\\Application%20Settings\\OpenAttachmentsInFullScreen&#x3D;0wps\\Application%20Settings\\CheckLanguage&#x3D;1wps\\Application%20Settings\\DefaultLegalBlackline&#x3D;0wps\\Application%20Settings\\DisplayAutoCompleteTips&#x3D;1wps\\Application%20Settings\\MoveToTextMark&#x3D;2wps\\Application%20Settings\\MoveFromTextMark&#x3D;6wps\\Application%20Settings\\InsertedCellColor&#x3D;2wps\\Application%20Settings\\DeletedCellColor&#x3D;1wps\\Application%20Settings\\MergedCellColor&#x3D;3wps\\Application%20Settings\\SplitCellColor&#x3D;5wps\\Application%20Settings\\PasteFormatWithinDocument&#x3D;0wps\\Application%20Settings\\PasteFormatBetweenDocuments&#x3D;0wps\\Application%20Settings\\PasteFormatBetweenStyledDocuments&#x3D;3wps\\Application%20Settings\\PasteFormatFromExternalSource&#x3D;0wps\\Application%20Settings\\FrenchReform&#x3D;0wps\\Application%20Settings\\SpanishMode&#x3D;0wps\\Application%20Settings\\PortugalReform&#x3D;1wps\\Application%20Settings\\BrazilReform&#x3D;2wps\\Application%20Settings\\UpdateStyleListBehavior&#x3D;0wps\\Application%20Settings\\StartupPath&#x3D;wps\\Application%20Settings\\ReadingLayoutActualView&#x3D;1wps\\Application%20Settings\\ReadingLayoutAllowMultiplePages&#x3D;1wps\\Application%20Settings\\ReadingLayoutAllowEditing&#x3D;0wps\\Application%20Settings\\ReadingLayoutTruncateMargins&#x3D;0wps\\Application%20Settings\\ShowCropMarks&#x3D;1wps\\Application%20Settings\\PageColor&#x3D;0wps\\Application%20Settings\\ColumnWidth&#x3D;2wps\\Application%20Settings\\WrapToWindowView&#x3D;0wps\\Application%20Settings\\DraftView&#x3D;0wps\\Application%20Settings\\EmbedFontsWhenNoRemind&#x3D;0wps\\Application%20Settings\\OpCompareTables&#x3D;1wps\\Application%20Settings\\OpCompareComments&#x3D;1wps\\Application%20Settings\\OpCompareFootAndEndNotes&#x3D;1wps\\Application%20Settings\\OpCompareTextBox&#x3D;1wps\\Application%20Settings\\OpCompareCaseChange&#x3D;1wps\\Application%20Settings\\OpCompareField&#x3D;1wps\\Application%20Settings\\OpCompareBlankRegion&#x3D;1wps\\Application%20Settings\\OpCompareLevel&#x3D;2wps\\Application%20Settings\\OpCompareLocation&#x3D;3wps\\Application%20Settings\\DisplayLeftScrollBar&#x3D;0wps\\Application%20Settings\\ShowAnimation&#x3D;1wps\\Application%20Settings\\ShowHyphens&#x3D;0wps\\Application%20Settings\\AutoRecoverFilePath&#x3D;&#x2F;root&#x2F;.local&#x2F;share&#x2F;Kingsoft&#x2F;office6&#x2F;data&#x2F;backupwps\\Application%20Settings\\ShowBalloonTitle&#x3D;15wps\\Application%20Settings\\ShowWindowsInTaskbar&#x3D;0wps\\Application%20Settings\\ShowXMLMarkup&#x3D;1wps\\Application%20Settings\\XMLHideNamespaces&#x3D;0wps\\Application%20Settings\\ShowSelectionMiniToolBar&#x3D;1wps\\Application%20Settings\\ShowRightClickMiniToolBar&#x3D;1wps\\Application%20Settings\\ShowTaskPane&#x3D;0wps\\Application%20Settings\\ZoomOfFirstView&#x3D;100wps\\Application%20Settings\\PageFitOfFirstView&#x3D;0wps\\Application%20Settings\\PageColumnsOfFirstView&#x3D;99wps\\Application%20Settings\\PageRowsOfFirstView&#x3D;1wps\\Application%20Settings\\AutoCorrect_ReplaceAsYouType&#x3D;1wps\\Application%20Settings\\PageIsMaxColumnsiPageFit&#x3D;0wps\\Application%20Settings\\EnableAutoSave&#x3D;0wps\\Application%20Settings\\ShowSourceDocumentsType&#x3D;3wps\\Application%20Settings\\PrintHiddenUnderline&#x3D;1wps\\Application%20Settings\\ShowDocumentField&#x3D;1wps\\Application%20Settings\\InsertToDocFld&#x3D;1wps\\Application%20Settings\\OfdViewOfdAfterExport&#x3D;0wps\\Application%20Settings\\OfdExportFootnotes&#x3D;1wps\\Application%20Settings\\OfdExportEndnote&#x3D;1wps\\Application%20Settings\\OfdExportSummaryInfo&#x3D;1wps\\Application%20Settings\\OfdExportHyperlinks&#x3D;1wps\\Application%20Settings\\OfdConvertBookmark&#x3D;1wps\\Application%20Settings\\OfdExportTitleStyle&#x3D;1wps\\Application%20Settings\\OfdExportBuildinStyle&#x3D;0wps\\Application%20Settings\\OfdExportCustomStyle&#x3D;0wps\\Application%20Settings\\OfdExportCommentsMode&#x3D;3wps\\Application%20Settings\\OfdRevisionMode&#x3D;1wps\\Application%20Settings\\OfdRights&#x3D;0wps\\Application%20Settings\\OfdServiceProvider&#x3D;0wps\\Application%20Settings\\OfdUpdateFields&#x3D;0wps\\Application%20Settings\\ShowEnterHeaderOrFooterTip&#x3D;1wps\\Application%20Settings\\PrintBackgroundsEx&#x3D;0wps\\Application%20Settings\\OfdEmbedFont&#x3D;1wps\\Application%20Settings\\PagePositionVisible&#x3D;1wps\\Application%20Settings\\ShowPlainWatermark&#x3D;0wps\\Application%20Settings\\LastOpenFilePath&#x3D;wps\\Application%20Settings\\SQLSecurityCheck&#x3D;1wps\\Application%20Settings\\ShowCommentShading&#x3D;1wps\\Application%20Settings\\OpenChineseSpellingCheck&#x3D;0wps\\Application%20Settings\\ClickTheBoxToTick&#x3D;1wps\\Application%20Settings\\ReadingDivision&#x3D;0wps\\Application%20Settings\\ReadingZoom&#x3D;100wps\\Application%20Settings\\ReadingBalloonWidthAbs&#x3D;1wps\\Application%20Settings\\ReadingBalloonWidth&#x3D;5329wps\\Application%20Settings\\HighQualityPrinting&#x3D;1wps\\Application%20Settings\\NormalPageColumns&#x3D;3wps\\Application%20Settings\\EnableMidBtnPaste&#x3D;0wps\\Application%20Settings\\ForceAutoBackupEnabled&#x3D;1wps\\Application%20Settings\\AutoBackupEnabled&#x3D;1wps\\Application%20Settings\\OfdPreviewZoom&#x3D;100wps\\Application%20Settings\\OpenAutoOfficialListSettings&#x3D;1wps\\Application%20Settings\\OfdPreviewPageFit&#x3D;0wps\\Application%20Settings\\OfdPreviewPageColumns&#x3D;99wps\\Application%20Settings\\OfdPreviewPageRows&#x3D;1wps\\Application%20Settings\\OfficialListLevel1FontName&#x3D;wps\\Application%20Settings\\OfficialListLevel1FontSize&#x3D;-1wps\\Application%20Settings\\OfficialListLevel2FontName&#x3D;wps\\Application%20Settings\\OfficialListLevel2FontSize&#x3D;-1wps\\Application%20Settings\\OfficialListLevel3FontName&#x3D;wps\\Application%20Settings\\OfficialListLevel3FontSize&#x3D;-1wps\\Application%20Settings\\OfficialListLevel4FontName&#x3D;wps\\Application%20Settings\\OfficialListLevel4FontSize&#x3D;-1wps\\Application%20Settings\\ShowNewOfficialDocumentDialog&#x3D;1wps\\Application%20Settings\\OfficialSpecialWordsEnable&#x3D;1wps\\Application%20Settings\\EmbedForceAutoBackupEnabled&#x3D;wps\\Application%20Settings\\EmbedAutoBackupEnabled&#x3D;wps\\Application%20Settings\\ShowSmartContentFileOpen&#x3D;0wps\\Application%20Settings\\EnableIncBackup&#x3D;1wps\\Application%20Settings\\BackupKeepCacheDays&#x3D;90wps\\Application%20Settings\\IsShowPicCompressDlg&#x3D;1wps\\Application%20Settings\\ProtectEyesBkColor&#x3D;4291619023wps\\Application%20Settings\\AutoBackupVisible&#x3D;0wps\\Application%20Settings\\OfdExportOleObject&#x3D;0wps\\Application%20Settings\\OfdExportOriginOleObject&#x3D;0common\\loginSafeVersion&#x3D;trueplugins\\officespace\\netswitch_updatetime&#x3D;0plugins\\officespace\\transformguide\\withSrc&#x3D;0plugins\\wpsbox\\DllExist&#x3D;trueplugins\\DllPath&#x3D;&#x2F;opt&#x2F;kingsoft&#x2F;wps-office&#x2F;office6&#x2F;addons&#x2F;wpsbox&#x2F;libwpsbox.socommon\\SymbolListOnTb\\SymbolItem%200\\DisplayChar&#x3D;\\xff0bet\\Application%20Settings\\AutoRecoverAppInfo&#x3D;et\\Application%20Settings\\BackupSuccess&#x3D;1et\\Application%20Settings\\DefaultDir&#x3D;&#x2F;root&#x2F;Documentset\\Application%20Settings\\FmlNameBoxWidth&#x3D;150et\\Application%20Settings\\RestartAppInfo&#x3D;0et\\Application%20Settings\\UpdateRecoverCheckTag&#x3D;trueet\\RecentFunction\\RecentFunction&#x3D;”SUMIF;SIN;MAX;COUNT;IF;AVERAGE;SUM;”et\\etongmani\\TaskPanelSize&#x3D;@Size(0 0)et\\etongmani\\ToolBarStates&#x3D;@ByteArray(\\0\\0\\0\\xff\\0\\0\\0\\0\\xfd\\0\\0\\0\\x1\\0\\0\\0\\x1\\0\\0\\0\\0\\0\\0\\0\\0\\xfc\\x1\\0\\0\\0\\x1\\xfb\\0\\0\\0&amp;\\0K\\0x\\0T\\0\\x61\\0s\\0k\\0P\\0\\x61\\0n\\0\\x65\\0\\x43\\0o\\0n\\0t\\0\\x61\\0i\\0n\\0\\x65\\0r\\0\\0\\0\\0\\0\\xff\\xff\\xff\\xff\\0\\0\\0\\x1e\\0\\xff\\xff\\xff\\0\\0\\b\\0\\0\\0\\x4\\n\\0\\0\\0\\x4\\0\\0\\0\\x4\\0\\0\\0\\b\\0\\0\\0\\b\\xfc\\0\\0\\0\\0)et\\etongmani\\taskpane\\barTextShow_v2&#x3D;trueet\\etongmani\\taskpane\\needRestore-v2&#x3D;falseet\\uifile&#x3D;res&#x2F;etongmani.kuiplugins\\konlinetaskpane\\AIRecommend&#x3D;trueplugins\\protecteyes\\et\\nightmodealpha&#x3D;190plugins\\protecteyes\\et\\type&#x3D;0common\\SymbolListOnTb\\SymbolItem%200\\FontName&#x3D;Arialcommon\\lastetact&#x3D;1686555464670et\\Application%20Settings\\NextCellDirAfterEnter&#x3D;8et\\etongmani\\taskpane\\dragMenu&#x3D;3plugins\\protecteyes\\et\\shownightmode&#x3D;0common\\SymbolListOnTb\\SymbolItem%200\\InsertChar&#x3D;\\xff0bet\\Application%20Settings\\ShowStartupDialog&#x3D;0et\\etongmani\\taskpane\\custSeq&#x3D;falsecommon\\SymbolListOnTb\\SymbolItem%200\\ShortcutKeys&#x3D;et\\Application%20Settings\\DisplayFormulaBar&#x3D;1et\\etongmani\\taskpane\\custCmd\\TpCloudLink&#x3D;truecommon\\SymbolListOnTb\\SymbolItem%200\\Visible&#x3D;-1et\\Application%20Settings\\DisplayStatusBar&#x3D;1et\\etongmani\\taskpane\\custCmd\\TpPivotTable&#x3D;truecommon\\SymbolListOnTb\\SymbolItem%201\\DisplayChar&#x3D;\\xff0det\\Application%20Settings\\MaxIterations&#x3D;100et\\etongmani\\taskpane\\custCmd\\TpFormatting&#x3D;truecommon\\SymbolListOnTb\\SymbolItem%201\\FontName&#x3D;Arialet\\Application%20Settings\\MaxChange&#x3D;0.001000et\\etongmani\\taskpane\\custCmd\\TpBackupFile&#x3D;truecommon\\SymbolListOnTb\\SymbolItem%201\\InsertChar&#x3D;\\xff0det\\Application%20Settings\\MoveAfterReturn&#x3D;1et\\etongmani\\taskpane\\custCmd\\TpSelectShape&#x3D;truecommon\\SymbolListOnTb\\SymbolItem%201\\ShortcutKeys&#x3D;et\\Application%20Settings\\MoveAfterReturnDirection&#x3D;8common\\SymbolListOnTb\\SymbolItem%201\\Visible&#x3D;-1et\\Application%20Settings\\FixedDecimal&#x3D;0common\\SymbolListOnTb\\SymbolItem%202\\DisplayChar&#x3D;\\xd7et\\Application%20Settings\\FixedDecimalPlaces&#x3D;2common\\SymbolListOnTb\\SymbolItem%202\\FontName&#x3D;Arialet\\Application%20Settings\\CopyObjectsWithCells&#x3D;1common\\SymbolListOnTb\\SymbolItem%202\\InsertChar&#x3D;\\xd7et\\Application%20Settings\\StandardFont&#x3D;Calibricommon\\SymbolListOnTb\\SymbolItem%202\\ShortcutKeys&#x3D;et\\Application%20Settings\\StandardFontSize&#x3D;0common\\SymbolListOnTb\\SymbolItem%202\\Visible&#x3D;-1et\\Application%20Settings\\SheetsInNewWorkbook&#x3D;1common\\SymbolListOnTb\\SymbolItem%203\\DisplayChar&#x3D;\\xf7et\\Application%20Settings\\ShowChartTipNames&#x3D;1common\\SymbolListOnTb\\SymbolItem%203\\FontName&#x3D;Arialet\\Application%20Settings\\ShowChartTipValues&#x3D;1common\\SymbolListOnTb\\SymbolItem%203\\InsertChar&#x3D;\\xf7et\\Application%20Settings\\ReferenceStyle&#x3D;0common\\SymbolListOnTb\\SymbolItem%203\\ShortcutKeys&#x3D;et\\Application%20Settings\\SmartTipsType&#x3D;560common\\SymbolListOnTb\\SymbolItem%203\\Visible&#x3D;-1et\\Application%20Settings\\RecentFiles&#x3D;1common\\SymbolListOnTb\\SymbolItem%204\\DisplayChar&#x3D;\\x338fet\\Application%20Settings\\SaveAsFileType&#x3D;.xlsxcommon\\SymbolListOnTb\\SymbolItem%204\\FontName&#x3D;Arialet\\Application%20Settings\\EnableSpecialDiagonal&#x3D;0common\\SymbolListOnTb\\SymbolItem%204\\InsertChar&#x3D;\\x338fet\\Application%20Settings\\DefaultBookName&#x3D;common\\SymbolListOnTb\\SymbolItem%204\\ShortcutKeys&#x3D;et\\Application%20Settings\\DefaultSheetName&#x3D;common\\SymbolListOnTb\\SymbolItem%204\\Visible&#x3D;-1et\\Application%20Settings\\SaveBackupFileOnFirstSave&#x3D;1common\\SymbolListOnTb\\SymbolItem%205\\DisplayChar&#x3D;\\x339cet\\Application%20Settings\\DefaultTemplateName&#x3D;common\\SymbolListOnTb\\SymbolItem%205\\FontName&#x3D;Arialet\\Application%20Settings\\DefaultNewFileButtonIndex&#x3D;0common\\SymbolListOnTb\\SymbolItem%205\\InsertChar&#x3D;\\x339cet\\Application%20Settings\\EnableAutoSave&#x3D;0common\\SymbolListOnTb\\SymbolItem%205\\ShortcutKeys&#x3D;et\\Application%20Settings\\AutoSaveTimeCycle&#x3D;1common\\SymbolListOnTb\\SymbolItem%205\\Visible&#x3D;-1et\\Application%20Settings\\AutoListSwitch&#x3D;1common\\SymbolListOnTb\\SymbolItem%206\\DisplayChar&#x3D;\\x339det\\Application%20Settings\\AlwaysSuggest&#x3D;1common\\SymbolListOnTb\\SymbolItem%206\\FontName&#x3D;Arialet\\Application%20Settings\\IgnoreCaps&#x3D;1common\\SymbolListOnTb\\SymbolItem%206\\InsertChar&#x3D;\\x339det\\Application%20Settings\\IgnoreMixedDigits&#x3D;1common\\SymbolListOnTb\\SymbolItem%206\\ShortcutKeys&#x3D;et\\Application%20Settings\\DocumentTab&#x3D;29common\\SymbolListOnTb\\SymbolItem%206\\Visible&#x3D;-1et\\Application%20Settings\\ReadingLayoutBackColor&#x3D;0common\\SymbolListOnTb\\SymbolItem%207\\DisplayChar&#x3D;\\x33a1et\\Application%20Settings\\ShowSelectionMiniToolBar&#x3D;1common\\SymbolListOnTb\\SymbolItem%207\\FontName&#x3D;Arialet\\Application%20Settings\\ShowRightClickMiniToolBar&#x3D;1common\\SymbolListOnTb\\SymbolItem%207\\InsertChar&#x3D;\\x33a1et\\Application%20Settings\\ShowTaskPane&#x3D;0common\\SymbolListOnTb\\SymbolItem%207\\ShortcutKeys&#x3D;et\\Application%20Settings\\ProtectEyes&#x3D;0common\\SymbolListOnTb\\SymbolItem%207\\Visible&#x3D;-1et\\Application%20Settings\\ProtectEyesBkColor&#x3D;-3348273common\\SymbolListOnTb\\SymbolItem%208\\DisplayChar&#x3D;\\x251et\\Application%20Settings\\OfdRights&#x3D;0common\\SymbolListOnTb\\SymbolItem%208\\FontName&#x3D;Arialet\\Application%20Settings\\OfdEmbedFont&#x3D;1common\\SymbolListOnTb\\SymbolItem%208\\InsertChar&#x3D;\\x251et\\Application%20Settings\\OfdOptions&#x3D;188common\\SymbolListOnTb\\SymbolItem%208\\ShortcutKeys&#x3D;et\\Application%20Settings\\ShowSubwinInTaskbar&#x3D;0common\\SymbolListOnTb\\SymbolItem%208\\Visible&#x3D;-1et\\Application%20Settings\\EnableLivePreview&#x3D;1common\\SymbolListOnTb\\SymbolItem%209\\DisplayChar&#x3D;\\x261et\\Application%20Settings\\EnableSmoothScroll&#x3D;1common\\SymbolListOnTb\\SymbolItem%209\\FontName&#x3D;Arialet\\Application%20Settings\\ThemeStandardFont&#x3D;common\\SymbolListOnTb\\SymbolItem%209\\InsertChar&#x3D;\\x261common\\SymbolListOnTb\\SymbolItem%209\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%209\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2010\\DisplayChar&#x3D;\\x3b1et\\Application%20Settings\\TextLoadType&#x3D;3common\\SymbolListOnTb\\SymbolItem%2010\\FontName&#x3D;Arialet\\Application%20Settings\\TextLoadSwitch&#x3D;31common\\SymbolListOnTb\\SymbolItem%2010\\InsertChar&#x3D;\\x3b1et\\Application%20Settings\\CommentDisplayMode&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2010\\ShortcutKeys&#x3D;et\\Application%20Settings\\SmartTipsStyleShow&#x3D;4common\\SymbolListOnTb\\SymbolItem%2010\\Visible&#x3D;-1et\\Application%20Settings\\CellDragAndDrop&#x3D;1common\\SymbolListOnTb\\SymbolItem%2011\\DisplayChar&#x3D;\\x3b2et\\Application%20Settings\\RecentPrintMode&#x3D;0common\\SymbolListOnTb\\SymbolItem%2011\\FontName&#x3D;Arialet\\Application%20Settings\\PrintScope&#x3D;2common\\SymbolListOnTb\\SymbolItem%2011\\InsertChar&#x3D;\\x3b2et\\Application%20Settings\\ErrorCheckingOptions&#x3D;4294967167common\\SymbolListOnTb\\SymbolItem%2011\\ShortcutKeys&#x3D;et\\Application%20Settings\\AutoIdentifyHyperlink&#x3D;1common\\SymbolListOnTb\\SymbolItem%2011\\Visible&#x3D;-1et\\Application%20Settings\\TraceHyperlinkWithAlt&#x3D;0common\\SymbolListOnTb\\SymbolItem%2012\\DisplayChar&#x3D;\\x3b8et\\Application%20Settings\\AskToUpdateLinks&#x3D;1common\\SymbolListOnTb\\SymbolItem%2012\\FontName&#x3D;Arialet\\Application%20Settings\\EditDirectlyInCell&#x3D;1common\\SymbolListOnTb\\SymbolItem%2012\\InsertChar&#x3D;\\x3b8et\\Application%20Settings\\IsCommaAsDec&#x3D;0common\\SymbolListOnTb\\SymbolItem%2012\\ShortcutKeys&#x3D;et\\Application%20Settings\\IsFormulabarExpanded&#x3D;1common\\SymbolListOnTb\\SymbolItem%2012\\Visible&#x3D;-1et\\Application%20Settings\\EnableBackup&#x3D;1common\\SymbolListOnTb\\SymbolItem%2013\\DisplayChar&#x3D;\\x2103et\\Application%20Settings\\ThrottleInterval&#x3D;2000common\\SymbolListOnTb\\SymbolItem%2013\\FontName&#x3D;Arialet\\Application%20Settings\\AutoRecoverFilePath&#x3D;&#x2F;root&#x2F;.local&#x2F;share&#x2F;Kingsoft&#x2F;office6&#x2F;data&#x2F;backupcommon\\SymbolListOnTb\\SymbolItem%2013\\InsertChar&#x3D;\\x2103et\\Application%20Settings\\BackupKeepCacheDays&#x3D;90common\\SymbolListOnTb\\SymbolItem%2013\\ShortcutKeys&#x3D;et\\Application%20Settings\\DisplayFunctionToolTips&#x3D;1common\\SymbolListOnTb\\SymbolItem%2013\\Visible&#x3D;-1et\\Application%20Settings\\GenerateGetPivotData&#x3D;1common\\SymbolListOnTb\\SymbolItem%2014\\DisplayChar&#x3D;\\x3016et\\Application%20Settings\\EnableAutoFilterCount&#x3D;1common\\SymbolListOnTb\\SymbolItem%2014\\FontName&#x3D;Arialet\\Application%20Settings\\ExternalContent&#x3D;1common\\SymbolListOnTb\\SymbolItem%2014\\InsertChar&#x3D;\\x3016\\x3017et\\Application%20Settings\\FilterType&#x3D;1common\\SymbolListOnTb\\SymbolItem%2014\\ShortcutKeys&#x3D;et\\Application%20Settings\\MutiFilterType&#x3D;3common\\SymbolListOnTb\\SymbolItem%2014\\Visible&#x3D;-1et\\Application%20Settings\\PinyinFilterType&#x3D;6common\\SymbolListOnTb\\SymbolItem%2015\\DisplayChar&#x3D;\\xff5bet\\Application%20Settings\\AutoRecommendFilterType&#x3D;1common\\SymbolListOnTb\\SymbolItem%2015\\FontName&#x3D;Arialet\\Application%20Settings\\EnableFormatCheck&#x3D;1common\\SymbolListOnTb\\SymbolItem%2015\\InsertChar&#x3D;\\xff5b\\xff5det\\Application%20Settings\\MergeCellOption&#x3D;65792common\\SymbolListOnTb\\SymbolItem%2015\\ShortcutKeys&#x3D;et\\Application%20Settings\\DisplayFilterCondition&#x3D;0common\\SymbolListOnTb\\SymbolItem%2015\\Visible&#x3D;-1et\\Application%20Settings\\EnableFilterMergeCells&#x3D;0common\\SymbolListOnTb\\SymbolItem%2016\\DisplayChar&#x3D;\\xa9et\\Application%20Settings\\FormatTableStyleOnly&#x3D;1common\\SymbolListOnTb\\SymbolItem%2016\\FontName&#x3D;Arialet\\Application%20Settings\\AfPaneDescendByCount&#x3D;0common\\SymbolListOnTb\\SymbolItem%2016\\InsertChar&#x3D;\\xa9et\\Application%20Settings\\RestoreScrollRepeat&#x3D;0common\\SymbolListOnTb\\SymbolItem%2016\\ShortcutKeys&#x3D;et\\Application%20Settings\\DisplayAfTotalItemCount&#x3D;0common\\SymbolListOnTb\\SymbolItem%2016\\Visible&#x3D;-1et\\Application%20Settings\\EnableAfAdvanceMode&#x3D;0common\\SymbolListOnTb\\SymbolItem%2017\\DisplayChar&#x3D;\\xaeet\\Application%20Settings\\ExpandFilterRgWhenBlankRows&#x3D;1common\\SymbolListOnTb\\SymbolItem%2017\\FontName&#x3D;Arialet\\Application%20Settings\\EnableAfRangeAutoExpand&#x3D;1common\\SymbolListOnTb\\SymbolItem%2017\\InsertChar&#x3D;\\xaeet\\Application%20Settings\\AfRangeExpandRule&#x3D;0common\\SymbolListOnTb\\SymbolItem%2017\\ShortcutKeys&#x3D;et\\Application%20Settings\\AfRangeExpandSameClickBtnCnt&#x3D;0common\\SymbolListOnTb\\SymbolItem%2017\\Visible&#x3D;-1et\\Application%20Settings\\NeverSelectNumFmtWhenOpenCsvFile&#x3D;1common\\SymbolListOnTb\\SymbolItem%2018\\DisplayChar&#x3D;\\xff20et\\Application%20Settings\\HighQualityPrinting&#x3D;1common\\SymbolListOnTb\\SymbolItem%2018\\FontName&#x3D;Arialet\\Application%20Settings\\EnableMidBtnPaste&#x3D;0common\\SymbolListOnTb\\SymbolItem%2018\\InsertChar&#x3D;\\xff20et\\Application%20Settings\\LastOpenFilePath&#x3D;common\\SymbolListOnTb\\SymbolItem%2018\\ShortcutKeys&#x3D;et\\Application%20Settings\\AltStartup&#x3D;common\\SymbolListOnTb\\SymbolItem%2018\\Visible&#x3D;-1et\\Application%20Settings\\ShowAfContextTab&#x3D;0common\\SymbolListOnTb\\SymbolItem%2019\\DisplayChar&#x3D;\\xff06et\\Application%20Settings\\PasteToFilterHiddenOption&#x3D;2common\\SymbolListOnTb\\SymbolItem%2019\\FontName&#x3D;Arialet\\Application%20Settings\\ShowAfToolsFeedbackDlg&#x3D;1common\\SymbolListOnTb\\SymbolItem%2019\\InsertChar&#x3D;\\xff06et\\Application%20Settings\\EnableAfContextShowWithFlash&#x3D;1common\\SymbolListOnTb\\SymbolItem%2019\\ShortcutKeys&#x3D;et\\Application%20Settings\\NumberFormatNewTagMask&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2019\\Visible&#x3D;-1et\\Application%20Settings\\EscRecoverSwitch&#x3D;1common\\SymbolListOnTb\\SymbolItem%2020\\DisplayChar&#x3D;\\xff0aet\\Application%20Settings\\IsShowOpenLargeHtmlFileDlg&#x3D;0common\\SymbolListOnTb\\SymbolItem%2020\\FontName&#x3D;Arialet\\Application%20Settings\\DefaultOpenLargeHtmlFileMode&#x3D;1common\\SymbolListOnTb\\SymbolItem%2020\\InsertChar&#x3D;\\xff0aet\\Application%20Settings\\EmbedForceAutoBackupEnabled&#x3D;1common\\SymbolListOnTb\\SymbolItem%2020\\ShortcutKeys&#x3D;et\\Application%20Settings\\ForceAutoBackupEnabled&#x3D;1common\\SymbolListOnTb\\SymbolItem%2020\\Visible&#x3D;-1et\\Application%20Settings\\AutoBackupEnabled&#x3D;1common\\SymbolListOnTb\\SymbolItem%2021\\DisplayChar&#x3D;\\xff03et\\Application%20Settings\\EmbedAutoBackupEnabled&#x3D;common\\SymbolListOnTb\\SymbolItem%2021\\FontName&#x3D;Arialet\\Application%20Settings\\EmbedUserName&#x3D;common\\SymbolListOnTb\\SymbolItem%2021\\InsertChar&#x3D;\\xff03et\\Application%20Settings\\IsShowPicCompressDlg&#x3D;1common\\SymbolListOnTb\\SymbolItem%2021\\ShortcutKeys&#x3D;et\\Application%20Settings\\EnableBlueBackground&#x3D;0common\\SymbolListOnTb\\SymbolItem%2021\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2022\\DisplayChar&#x3D;\\xff05common\\SymbolListOnTb\\SymbolItem%2022\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2022\\InsertChar&#x3D;\\xff05common\\SymbolListOnTb\\SymbolItem%2022\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2022\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2023\\DisplayChar&#x3D;\\x2030common\\SymbolListOnTb\\SymbolItem%2023\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2023\\InsertChar&#x3D;\\x2030common\\SymbolListOnTb\\SymbolItem%2023\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2023\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2024\\DisplayChar&#x3D;\\xff04common\\SymbolListOnTb\\SymbolItem%2024\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2024\\InsertChar&#x3D;\\xff04common\\SymbolListOnTb\\SymbolItem%2024\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2024\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2025\\DisplayChar&#x3D;\\xffe5common\\SymbolListOnTb\\SymbolItem%2025\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2025\\InsertChar&#x3D;\\xffe5common\\SymbolListOnTb\\SymbolItem%2025\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2025\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2026\\DisplayChar&#x3D;\\xa7common\\SymbolListOnTb\\SymbolItem%2026\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2026\\InsertChar&#x3D;\\xa7common\\SymbolListOnTb\\SymbolItem%2026\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2026\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2027\\DisplayChar&#x3D;\\xa6common\\SymbolListOnTb\\SymbolItem%2027\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2027\\InsertChar&#x3D;\\xa6common\\SymbolListOnTb\\SymbolItem%2027\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2027\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2028\\DisplayChar&#x3D;\\x203bcommon\\SymbolListOnTb\\SymbolItem%2028\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2028\\InsertChar&#x3D;\\x203bcommon\\SymbolListOnTb\\SymbolItem%2028\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2028\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2029\\DisplayChar&#x3D;\\x25cecommon\\SymbolListOnTb\\SymbolItem%2029\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2029\\InsertChar&#x3D;\\x25cecommon\\SymbolListOnTb\\SymbolItem%2029\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2029\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2030\\DisplayChar&#x3D;\\x2248common\\SymbolListOnTb\\SymbolItem%2030\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2030\\InsertChar&#x3D;\\x2248common\\SymbolListOnTb\\SymbolItem%2030\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2030\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2031\\DisplayChar&#x3D;\\x2026common\\SymbolListOnTb\\SymbolItem%2031\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2031\\InsertChar&#x3D;\\x2026common\\SymbolListOnTb\\SymbolItem%2031\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2031\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2032\\DisplayChar&#x3D;\\x2264common\\SymbolListOnTb\\SymbolItem%2032\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2032\\InsertChar&#x3D;\\x2264common\\SymbolListOnTb\\SymbolItem%2032\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2032\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2033\\DisplayChar&#x3D;\\x2265common\\SymbolListOnTb\\SymbolItem%2033\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2033\\InsertChar&#x3D;\\x2265common\\SymbolListOnTb\\SymbolItem%2033\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2033\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2034\\DisplayChar&#x3D;\\x2266common\\SymbolListOnTb\\SymbolItem%2034\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2034\\InsertChar&#x3D;\\x2266common\\SymbolListOnTb\\SymbolItem%2034\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2034\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2035\\DisplayChar&#x3D;\\x2267common\\SymbolListOnTb\\SymbolItem%2035\\FontName&#x3D;Arialcommon\\SymbolListOnTb\\SymbolItem%2035\\InsertChar&#x3D;\\x2267common\\SymbolListOnTb\\SymbolItem%2035\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2035\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2036\\DisplayChar&#x3D;\\xa3common\\SymbolListOnTb\\SymbolItem%2036\\FontName&#x3D;Wingdings 2common\\SymbolListOnTb\\SymbolItem%2036\\InsertChar&#x3D;\\xa3common\\SymbolListOnTb\\SymbolItem%2036\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2036\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2037\\DisplayChar&#x3D;Rcommon\\SymbolListOnTb\\SymbolItem%2037\\FontName&#x3D;Wingdings 2common\\SymbolListOnTb\\SymbolItem%2037\\InsertChar&#x3D;Rcommon\\SymbolListOnTb\\SymbolItem%2037\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2037\\Visible&#x3D;-1common\\SymbolListOnTb\\SymbolItem%2038\\DisplayChar&#x3D;Qcommon\\SymbolListOnTb\\SymbolItem%2038\\FontName&#x3D;Wingdings 2common\\SymbolListOnTb\\SymbolItem%2038\\InsertChar&#x3D;Qcommon\\SymbolListOnTb\\SymbolItem%2038\\ShortcutKeys&#x3D;common\\SymbolListOnTb\\SymbolItem%2038\\Visible&#x3D;-1plugins\\kstartpage\\officenav_new_v1\\seqEntry\\CommonEntrys\\newdocumentwps&#x3D;0 [kdcsdk]NotFirstOpen&#x3D;true "},{"title":"word表格批量设置允许跨页断行","date":"2025-09-23T05:36:09.000Z","url":"/2025/09/23/2025/09/231034/","tags":[["openssl","/tags/openssl/"]],"categories":[["work","/categories/work/"]],"content":"在处理包含多个表格的Word文档时，批量设置“允许跨页断行”能有效避免表格在页面底部被生硬截断，提升文档美观度和阅读体验。 🧾 跨页断行与相关设置Word表格的跨页行为，主要通过“允许跨页断行”这一选项控制。它决定了表格行是否可以在页面底部被切断，并将剩余部分显示在下一页。 与之相关的另一个实用设置是“在各页顶端以标题行形式重复出现”（或称“重复标题行”）。建议优先设置此功能，它能确保跨页断行后的表格在每一新页的开头都显示指定的标题行，极大方便了阅读和数据处理。你可以在 表格属性 &gt; 行 选项卡中勾选此选项。 🔧 批量设置方法你可以通过以下流程图了解批量设置的两种主要方式及其选择参考： 以下是每种方法的具体操作步骤。 方法一：使用 VBA 宏（推荐用于大量表格）此方法通过运行一段 Visual Basic for Applications (VBA) 代码来批量处理文档中的所有表格，效率最高。 启用“开发工具”选项卡（若尚未启用）： 打开 Word 文档，点击 文件 &gt; 选项 &gt; 自定义功能区。 在右侧的“主选项卡”列表中，找到并勾选 开发工具，然后点击“确定”。 打开 VBA 编辑器并插入模块： 在 开发工具 选项卡中，点击 Visual Basic（或按 Alt + F11）。 在弹出的 VBA 编辑器窗口中，点击菜单栏的 插入 &gt; 模块。 粘贴并运行代码： 将以下 VBA 代码复制粘贴到新出现的代码窗口中： 粘贴后，点击工具栏上的绿色“运行”三角按钮（或按 F5）执行宏。 处理安全警告与保存： 首次运行宏时，Word可能会提示“宏已被禁用”。点击 启用内容 或根据提示调整宏安全设置（文件 &gt; 选项 &gt; 信任中心 &gt; 信任中心设置 &gt; 宏设置 &gt; 选择“启用所有宏”（请注意临时启用，使用后建议恢复设置））。 运行完成后会弹出提示框。请注意，包含宏的文件需要保存为 .docm 格式。 方法二：手动批量设置（表格数量较少时）如果文档中表格数量不多，或者对 VBA 感到陌生，可以尝试以下手动方法。 全选文档中的所有表格： 按 Ctrl + A 全选文档中的所有内容。 统一设置表格属性： 在选中的内容上右键单击，选择 表格属性。 在弹出的“表格属性”对话框中，切换到 行 选项卡。 勾选 允许跨页断行。 点击“确定”应用设置。 ⚠️ 注意：此方法通过全选文本来间接选中所有表格并进行统一设置，在某些 Word 版本或文档格式复杂时可能不一定对所有表格生效。操作后最好检查一下关键表格的设置。 ⚠️ 重要注意事项 “允许跨页断行”与“不允许跨页断行”：勾选“允许跨页断行”，Word 会在页面底部断开表格；取消勾选，Word 会尽力将整行保持在同一页，如果当前页放不下，会将整行移到下一页。请根据你的排版需求决定。 标题行的重复：如前所述，强烈建议为跨页的表格设置“重复标题行”。你可以在 VBA 代码中设置（见上面代码注释），也可以手动在 表格属性 &gt; 行 选项卡中勾选 在各页顶端以标题行形式重复出现。此功能仅对表格首行或选中的连续首几行有效。 检查段落分页设置：如果表格单元格内文字很多，还需检查文字自身的段落设置。选中单元格内文字，右键进入 段落 &gt; 换行和分页 选项卡，确保没有勾选 段中不分页，否则会影响跨页。 版本差异：不同版本的 Word（如 Microsoft Word 365, 2021, 2019 或 WPS Office）操作界面可能略有不同，但核心选项名称基本一致。 💎 总结批量处理Word文档中多个表格的跨页断行，推荐使用VBA宏的方法，它速度快且能确保所有表格都被统一处理。手动全选的方法可以作为表格数量较少时的备选方案。"},{"title":"生成svg格式的图片","date":"2025-09-22T06:06:09.000Z","url":"/2025/09/22/2025/09/190401/","tags":[["svg","/tags/svg/"]],"categories":[["Java","/categories/Java/"]],"content":"SVG（Scalable Vector Graphics）是一种基于XML的矢量图形格式，在现代Web开发中扮演着重要角色。与传统的位图格式（如PNG、JPEG）不同，SVG使用数学公式来描述图形，这使得它具有无限缩放而不失真的特性。 什么是SVG？SVG是一种用XML描述的二维矢量图形格式，由W3C制定标准。它允许我们使用文本编辑器创建和编辑图形，同时支持动画、交互性和样式化。 SVG的主要特点： 矢量图形：基于数学公式，可无限缩放而不失真 文件小：相比位图格式，文件体积通常更小 可编辑：可以用文本编辑器直接编辑 可搜索：文本内容可以被搜索引擎索引 可样式化：支持CSS样式 可交互：支持JavaScript交互 可动画：支持CSS和SMIL动画 SVG vs 其他格式 特性 SVG PNG JPEG GIF 类型 矢量 位图 位图 位图 缩放 无损 有损 有损 有损 文件大小 小 中等 小 小 动画支持 是 否 否 是 透明度 是 是 否 是 交互性 是 否 否 否 SVG基本语法基本结构 常用元素1. 矩形 (rect) 2. 圆形 (circle) 3. 椭圆 (ellipse) 4. 线条 (line) 5. 路径 (path) 6. 文本 (text) 实际应用示例1. 创建简单的图标 2. 响应式SVG 3. 带CSS样式的SVG 在Web开发中使用SVG1. 内联SVG 2. 作为图片使用 3. 作为背景图片 4. 使用SVG Sprite SVG动画1. CSS动画 2. SMIL动画 优化技巧1. 压缩SVG 2. 移除不必要的属性 3. 使用路径优化 浏览器支持现代浏览器对SVG的支持已经非常完善： Chrome: 完全支持 Firefox: 完全支持 Safari: 完全支持 Edge: 完全支持 IE: 9+支持（部分功能） 最佳实践 使用viewBox属性：确保SVG在不同尺寸下正确显示 优化文件大小：移除不必要的属性和空白 使用语义化的ID和类名：便于维护和样式化 考虑可访问性：添加适当的alt文本和ARIA属性 测试不同浏览器：确保兼容性 总结SVG作为一种强大的矢量图形格式，在现代Web开发中具有重要地位。它的可缩放性、小文件体积、丰富的交互性和动画支持，使其成为创建图标、图表、插图和复杂图形的理想选择。掌握SVG的使用技巧，能够显著提升Web应用的用户体验和性能。 随着Web技术的不断发展，SVG的应用场景也在不断扩展，从简单的图标到复杂的数据可视化，SVG都展现出了其独特的优势。对于前端开发者来说，熟练掌握SVG的使用是必不可少的技能之一。"},{"title":"cursor使用mcp连接数据库","date":"2025-09-15T07:15:09.000Z","url":"/2025/09/15/2025/09/151515/","tags":[["AI","/tags/AI/"],["mcp","/tags/mcp/"]],"categories":[["Linux","/categories/Linux/"]],"content":"vscode like使用mcp连接数据库 安装uv 安装mysql_mcp_server 初始化 虚拟环境 激活环境 安装服务 使用--directory后面的参数即为第一步中的绝对路径 参考文档 github:mysql_mcp_server "},{"title":"解决IllegalStateException","date":"2025-09-04T06:06:09.000Z","url":"/2025/09/04/2025/09/011621/","tags":[["Spring","/tags/Spring/"]],"categories":[["Java","/categories/Java/"]],"content":"解决 java.lang.IllegalStateException: ut010005: can… IllegalStateException此错误通常发生在尝试同时调用 HttpServletResponse 的 getOutputStream() 和 getWriter() 方法时。根据 Servlet 规范，这两者不能同时使用，因为它们分别用于二进制数据和字符数据的输出。 示例问题 运行上述代码会抛出 IllegalStateException，因为在调用 getWriter() 后再调用 getOutputStream() 是不允许的。 解决方案 统一使用 getWriter() 或 getOutputStream() 确保在整个响应处理中只使用一种输出方式。如果需要输出文本内容，使用 getWriter()；如果需要输出二进制数据（如文件下载），使用 getOutputStream()。 示例： 使用 ResponseEntity 替代直接操作 HttpServletResponse 在 Spring 框架中，可以通过返回 ResponseEntity 来避免直接操作响应流。 示例： 避免在 JSP 中混用输出流 如果在 JSP 页面中使用了 out 对象（由 getWriter() 提供），不要再调用 getOutputStream()。可以通过清空缓存并更新上下文来解决冲突。 示例： 通过以上方法，可以有效避免和解决该异常，提高代码的稳定性和可维护性。"},{"title":"Mybatist转义的配置","date":"2025-08-21T16:00:00.000Z","url":"/2025/08/22/2025/08/221128/","tags":[["mybatis","/tags/mybatis/"]],"categories":[["Java","/categories/Java/"]],"content":" 【Mybatis错误】Caused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 584; The content of elements must consist of well-formed character data or markup 编译器自动把sql语句中的小于号当成了开始标签 替换为： "},{"title":"pptx添加备注","date":"2025-08-11T16:00:00.000Z","url":"/2025/08/12/2025/08/181022/","tags":[["java","/tags/java/"],["poi","/tags/poi/"],["pptx","/tags/pptx/"]],"categories":[["Java","/categories/Java/"]],"content":"AI给pptx文件添加演讲时的备注 前言在演讲准备过程中，为PPT添加备注是一个重要的环节。备注可以帮助演讲者更好地组织思路，提醒关键要点，确保演讲的流畅性。然而，手动为每张幻灯片添加备注往往耗时且容易遗漏。本文介绍如何使用Java和Apache POI库，结合AI技术，自动化地为PPTX文件添加智能备注。 背景传统备注添加方式的痛点 耗时费力：需要逐张幻灯片手动添加备注 内容重复：相似内容的幻灯片备注往往重复 缺乏智能性：备注内容缺乏针对性和个性化 维护困难：修改PPT内容后，备注需要同步更新 AI技术的应用前景 内容理解：AI可以分析PPT内容，生成相关备注 智能推荐：基于内容自动推荐合适的备注内容 批量处理：一次性处理整个PPT文件 内容优化：根据演讲场景优化备注内容 目标 自动化备注添加：实现PPTX文件的自动备注添加功能 智能内容生成：基于PPT内容自动生成相关备注 批量处理能力：支持单个或多个PPTX文件的批量处理 格式兼容性：确保生成的备注在PowerPoint中正常显示 错误处理：提供完善的错误处理和文件验证机制 解题思路技术选型 Apache POI：Java生态中最成熟的Office文档处理库 POI-XSLF：专门用于处理PPTX文件的扩展库 文件验证：确保输入文件的完整性和有效性 异常处理：提供多层次的错误处理机制 核心策略 文件验证：首先验证PPTX文件的有效性 备注页处理：优先使用标准的备注页机制 备用方案：当备注页不可用时，直接在幻灯片上添加备注 内容优化：根据幻灯片内容智能生成备注文本 方案设计整体架构 核心模块 文件验证模块：检查文件存在性、大小、格式等 备注添加模块：处理备注页和直接备注两种方式 文本处理模块：处理备注文本的格式和样式 输出保存模块：保存修改后的PPTX文件 处理流程 验证输入文件的有效性 打开PPTX文件并创建幻灯片展示对象 获取目标幻灯片（默认第一张） 尝试添加备注到备注页 如果备注页不可用，使用备用方案 保存修改后的文件 技术架构依赖库 Apache POI：核心文档处理库 POI-XSLF：PPTX文件处理扩展 Lombok：简化Java代码 Apache Commons Lang3：字符串处理工具 核心类设计 PptxTest：主类，包含主要的处理逻辑 文件验证方法：validatePptxFile() 备注添加方法：addNotesToSlide() 文本框创建方法：createNotesTextBox() 直接备注方法：addNotesToSlideDirectly() 异常处理策略 文件不存在或损坏的处理 备注页创建失败时的备用方案 文件保存失败的错误处理 资源释放的确保机制 实现细节文件验证机制 检查文件是否存在 验证文件大小（非空文件） 检查文件头是否为ZIP格式（PPTX本质上是ZIP文件） 使用魔数验证文件格式 备注页处理策略 优先使用标准备注页：通过getNotesSlide()获取现有备注页 占位符查找：查找备注体占位符（Placeholder.BODY） 文本设置：在找到的占位符中设置备注文本 备用文本框：如果没有占位符，创建新的文本框 直接备注方案当备注页不可用时，直接在幻灯片上添加备注文本框： 设置合适的位置和大小 使用灰色字体区分备注内容 添加”备注：”前缀标识 样式设置 字体大小：备注页14pt，直接备注12pt 字体族：使用宋体确保中文显示 颜色：直接备注使用灰色 位置：合理布局避免遮挡主要内容 核心代码 效果展示成功添加备注后的效果 在PowerPoint中打开生成的PPTX文件 切换到”备注页”视图（视图 → 备注页） 可以看到每张幻灯片下方都有相应的备注内容 备注内容清晰可读，格式规范 文件结构变化 原始PPTX文件保持不变 生成新的带备注的PPTX文件 文件大小略有增加（包含备注信息） 兼容性良好，可在各种PowerPoint版本中打开 示例展示使用场景示例 会议演示：为会议PPT自动添加关键要点备注 培训材料：为培训PPT添加讲解要点和注意事项 学术报告：为学术报告添加数据说明和引用信息 产品介绍：为产品PPT添加销售话术和客户关注点 备注内容示例 技术类PPT：技术要点、注意事项、扩展阅读 商务类PPT：关键数据、市场分析、竞争对比 教育类PPT：学习目标、重点难点、课后思考 营销类PPT：产品优势、客户痛点、转化策略 批量处理示例 参考附录相关技术文档 Apache POI官方文档 POI XSLF用户指南 PPTX文件格式规范 扩展功能建议 AI内容分析：集成NLP服务分析PPT内容 模板系统：支持不同类型的备注模板 批量处理：支持目录级别的批量处理 备注优化：根据演讲时长优化备注内容 多语言支持：支持中英文等多种语言 性能优化建议 使用流式处理减少内存占用 实现增量更新避免重复处理 添加缓存机制提高处理速度 支持并行处理多个文件 常见问题解决 文件损坏：使用文件验证机制提前发现 备注页缺失：提供直接备注的备用方案 格式不兼容：确保使用标准的POI API 内存溢出：及时释放资源，使用try-with-resources 未来发展方向 集成大语言模型生成更智能的备注 支持更多Office文档格式（Word、Excel） 开发Web界面简化操作流程 提供云服务支持在线处理 "},{"title":"PPT添加水印功能","date":"2025-07-31T16:00:00.000Z","url":"/2025/08/01/2025/08/011646/","tags":[["poi","/tags/poi/"]],"categories":[["Java","/categories/Java/"]],"content":"在Java中实现PPT添加水印功能，可以借助Apache POI库操作PPTX文件。以下是详细实现方案： 核心思路 ​使用母版添加水印​：在幻灯片母版中插入文本框 ​样式控制​：设置文本透明度、旋转角度、位置 ​跨平台兼容​：处理PPTX格式的现代Office文件 代码实现 关键实现说明 ​母版定位​： getSlideMasters().get(0) 获取第一张主母版 BLANK 版式确保不影响已有布局（可根据需要改用 TITLE_AND_CONTENT 等） ​水印特性​： 透明度控制：通过 setAlpha() 方法设置RGB透明度值 旋转角度：setRotation(30) 实现30度倾斜效果 精确定位：Rectangle2D.Double 控制水印位置和大小 ​排除标题页​： 默认只在普通版式添加水印 如需标题页水印，取消注释代码中的标题母版处理 ​样式扩展​： 注意事项 ​库依赖​： ​兼容性问题​： 仅支持PPTX格式（Office 2007+） 若需要支持PPT旧格式，需使用HSLFSlideShow，但功能受限 ​位置调整技巧​： 使用ppt.getPageSize().getWidth()/Height()获取幻灯片尺寸 推荐居中定位公式： 此方法生成的PPT水印具有Office原生水印特性： 无法被普通用户直接删除（需进入母版视图） 自动应用至所有匹配版式的幻灯片 保留矢量特性可无损缩放 参考方案 在 PowerPoint 中，可以在幻灯片中放置文本背景以获取该水印效果。 若要向所有幻灯片添加水印，选择“视图”&gt;“幻灯片母版”。 滚动到左侧缩略图窗格的顶部，选择第一项“幻灯片母版”。 选择“插入”&gt;“文本框”，然后在幻灯片母版上单击并拖动鼠标，画出文本框。 在文本框中键入水印文本 (，例如“DRAFT”、“CONFIDENTIAL”或公司名称) 。 若要更改水印文本的对齐方式，请在文本框顶部单击并按住旋转图柄，然后向左或向右移动鼠标。 选中文本框中的文本。 选择浅色字体填充颜色，然后对字体和样式进行任何其他更改。 (如果看不到“ 格式 ”选项卡，请确保已选择文本框。) 退出“幻灯片母版”。 除标题页外的所有幻灯片都会具有该文本。 "},{"title":"onlyoffice字体加载慢的解决方案","date":"2025-07-25T10:09:09.000Z","url":"/2025/07/25/2025/07/251808/","tags":[["linux","/tags/linux/"],["font","/tags/font/"],["onlyoffice","/tags/onlyoffice/"]],"categories":[["Linux","/categories/Linux/"]],"content":"OnlyOffice作为一款优秀的开源办公套件，在Docker部署环境中经常遇到字体加载缓慢的问题。本文将介绍一种高效的静态文件托管优化方案，通过OSS&#x2F;CDN加速解决字体加载瓶颈。 前言OnlyOffice在Docker容器中运行时，首次加载需要从服务器下载大量字体文件，这个过程往往耗时很长，严重影响用户体验。特别是在网络环境不佳的情况下，字体下载可能需要几十秒甚至更长时间。 背景字体加载慢的核心问题： Docker容器内字体文件体积大 - 单个字体文件可达几MB，总体积可能超过100MB 网络带宽限制 - 服务器带宽有限，多用户同时访问时更加缓慢 缺乏CDN加速 - 字体文件直接从应用服务器加载，没有利用CDN优势 无缓存机制 - 每次重启容器都需要重新下载字体 目标通过静态文件托管优化，实现： 将字体加载时间从30-60秒降至3-5秒 利用OSS&#x2F;CDN的全球加速能力 减轻应用服务器带宽压力 提供稳定可靠的字体服务 解题思路核心思路是将体积较大的字体&#x2F;JS文件迁移至高速存储（如OSS&#x2F;CDN），通过Nginx转发请求，避免服务器直连加载瓶颈。 静态文件托管优化方案（推荐）操作步骤步骤一：定位静态文件进入Docker容器查找字体目录： 常见字体目录结构： 步骤二：上传至OSS&#x2F;CDN将容器内字体复制到宿主机： 上传至阿里云OSS（示例）： 步骤三：修改Nginx配置编辑容器内Nginx配置文件： 添加字体重定向规则： 步骤四：重启服务 高级配置OSS跨域配置在阿里云OSS控制台设置跨域规则： CDN缓存优化配置CDN缓存规则： 容器启动脚本优化创建启动脚本自动配置： 技术架构静态文件托管架构 优化前后对比 实现细节1. 自动化部署脚本完整的字体迁移脚本 2. 监控和维护字体加载性能监控脚本 3. 故障排查工具诊断脚本 核心代码Docker Compose一键部署 Nginx配置模板 自动化运维脚本 效果展示性能对比分析 指标 优化前 优化后 改善幅度 首次加载时间 30-60秒 3-5秒 85-90% 字体文件大小 128MB 128MB 无变化 服务器带宽占用 100% 5% 95% 用户体验评分 2&#x2F;10 9&#x2F;10 350% 并发支持能力 10用户 100+用户 1000% 实际测试数据 成本效益分析 示例展示优化前的问题现象 优化后的正常状态 配置验证命令 参考附录相关配置文件路径Docker容器内路径： OnlyOffice配置: /etc/onlyoffice/documentserver/ Nginx配置: /etc/nginx/includes/ds-docservice.conf 字体目录: /var/www/onlyoffice/documentserver/fonts/ 日志目录: /var/log/onlyoffice/documentserver/ 宿主机路径： Docker Compose文件: ./docker-compose.yml 自定义Nginx配置: ./nginx-custom.conf 数据持久化: ./data/ 日志持久化: ./logs/ 常用命令 故障排查步骤1. 基础检查 2. 配置验证 3. 网络连通性测试 4. 日志分析 常见问题解决 问题 原因 解决方案 301重定向不生效 Nginx配置未生效 重启容器或重载配置 OSS访问403错误 权限配置问题 设置Bucket公共读权限 字体加载仍然很慢 CDN未生效 检查CDN配置和缓存策略 容器启动失败 配置文件语法错误 检查Nginx配置语法 性能监控脚本 通过以上静态文件托管优化方案，可以将OnlyOffice的字体加载时间从30-60秒降至3-5秒，显著提升用户体验。建议在生产环境中逐步实施，并持续监控性能指标。"},{"title":"AdGuard生成自签名证书","date":"2025-07-22T09:06:09.000Z","url":"/2025/07/22/2025/07/221740/","tags":[["openssl","/tags/openssl/"],["adguard","/tags/adguard/"]],"categories":[["Linux","/categories/Linux/"]],"content":"为AdGuard设置自签名证书加密可有效保护局域网DNS通信安全，避免流量被窃听或篡改。以下是详细操作指南，涵盖证书生成、AdGuard配置及客户端信任设置： 🔐 一、生成自签名证书（以OpenSSL为例） 安装OpenSSLsudo apt update &amp;&amp; sudo apt install openssl # Debian&#x2F;Ubuntu系统 生成证书和私钥sudo openssl req -new -newkey rsa:2048 -sha256 -days 3650 -nodes -x509 -subj “&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Organization&#x2F;CN&#x3D;adguardhome.com” -out &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;adguardhome.crt -keyout &#x2F;etc&#x2F;ssl&#x2F;private&#x2F;adguardhome.key -extensions SAN -config &lt;(cat &#x2F;etc&#x2F;ssl&#x2F;openssl.cnf &lt;(printf “[SAN]\\nsubjectAltName&#x3D;IP:192.168.50.200”)) 关键参数说明：• subjectAltName&#x3D;IP:192.168.50.200：替换为AdGuard Home服务器的实际IP。 • 证书路径：&#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;adguardhome.crt（证书）和 &#x2F;etc&#x2F;ssl&#x2F;private&#x2F;adguardhome.key（私钥）。 验证证书openssl x509 -in &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;adguardhome.crt -text -noout 检查输出中是否包含IP地址（如 X509v3 Subject Alternative Name: IP:192.168.50.200）。 ⚙️ 二、AdGuard Home服务端配置 启用加密设置• 登录AdGuard Home管理界面（http:&#x2F;&#x2F;:3000）。 • 进入 Settings → Encryption Settings。 • 勾选 Enable encryption。 • 填写证书路径： ◦ Certificate chain: &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;adguardhome.crt ◦ Private key: &#x2F;etc&#x2F;ssl&#x2F;private&#x2F;adguardhome.key 。 重启服务sudo systemctl restart AdGuardHome 完成后，管理界面将变为 https:&#x2F;&#x2F;:3000，浏览器会提示证书不安全（需后续信任操作）。 📱 三、客户端设备信任证书Windows&#x2F;macOS&#x2F;Linux： 将证书文件 adguardhome.crt 复制到客户端。 导入证书：• Windows：双击证书 → 选择“安装证书” → 存储位置选“本地计算机” → 选择“受信任的根证书颁发机构”。 • macOS：双击证书 → 钥匙串访问 → 拖入“系统”钥匙串 → 右键选择“始终信任”。 • Linux：复制到 &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates&#x2F; → 执行 sudo update-ca-certificates。 Android： 将证书文件发送到手机。 进入 设置 → 安全 → 加密与凭据 → 安装证书 → 选择文件并确认安装。 在AdGuard App中启用 HTTPS过滤（需在设置中单独安装CA证书）。 iOS： 通过邮件或网页下载证书文件。 进入 设置 → 通用 → VPN与设备管理 → 点击证书文件 → 选择“安装”。 ⚠️ 四、常见问题解决 浏览器提示“不安全连接”• 原因：证书未导入客户端受信任存储区。 • 解决：重新检查证书导入步骤，确保导入到 “受信任的根证书颁发机构” ,重启浏览器。 AdGuard报“证书链无效”• 原因：生成证书时未正确配置SAN扩展。 • 解决：重新生成证书，确保包含 subjectAltName&#x3D;IP:&lt;服务器IP&gt;。 移动端无法过滤HTTPS流量• 原因：未在AdGuard App中启用HTTPS过滤。 • 解决：进入AdGuard移动端设置 → 过滤 → HTTPS过滤 → 安装CA证书。 OpenWrt&#x2F;LXC容器环境问题• 需将证书追加到系统的CA信任链： sudo cat &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;adguardhome.crt &gt;&gt; &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;ca-certificates.crt 。 💎 五、进阶建议• 替代方案（生产环境）： 使用Let’s Encrypt免费证书（支持域名验证），避免自签名证书的信任警告。• 证书自动续期： 通过certbot配置自动续期脚本（需绑定域名）。• 安全性强化： 在路由器设置中强制DNS流量指向AdGuard Home的加密端口（如853&#x2F;TLS），避免设备绕过。 提示：自签名证书适用于内网环境，若需公网访问或更高安全性，建议使用受信任CA签发的证书。操作前备份原配置文件，避免服务中断。"},{"title":"crates.io-index阿里云镜像","date":"2025-07-17T16:00:00.000Z","url":"/2025/07/18/2025/07/181548/","tags":[["Rust","/tags/Rust/"]],"categories":[["wuw","/categories/wuw/"]],"content":"crates.io-index 镜像简介Cargo 是 Rust 的构建系统和包管理器。 Rust Crates Registry 源是 Rust 的代码仓库。 下载地址： 配置方法查看 cargo 版本 配置 crates.io 镜像在 cargo 配置文件： ~&#x2F;.cargo&#x2F;config.toml 中，添加以下内容： （Windows 系统配置文件地址默认为：%USERPROFILE%.cargo\\config.toml） 目前该镜像仅支持稀疏索引配置，需要您的 cargo 版本 &gt;&#x3D;1.68。 稀疏索引配置 相关链接 官方主页： "},{"title":"基于GreptimeDB与Spring Boot构建高性能服务告警中心的设计","date":"2025-07-13T16:00:00.000Z","url":"/2025/07/14/2025/07/101107/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 基于GreptimeDB与Spring Boot构建高性能服务告警中心的设计1. 执行摘要本报告旨在为构建一个高性能、可扩展的服务告警中心提供架构设计和实施策略。该告警中心将充分利用GreptimeDB作为统一的时序数据存储，并结合Spring Boot构建健壮的应用逻辑。所提出的系统能够摄取多样化的可观测性数据，实时评估动态告警规则，并分发多渠道通知，同时确保高可用性和容错性。该设计的核心优势在于统一的可观测性数据管理、实时洞察能力、灵活的规则定义、解耦的通知分发以及云原生可扩展性。 2. 实时服务告警简介告警的关键作用在当今复杂且日益增长的微服务架构中，实时告警对于主动识别事件、最小化停机时间以及维护服务健康至关重要。它将原始的可观测性数据转化为可操作的洞察，从而能够对异常行为做出快速响应。一个设计良好的告警系统是确保系统稳定性和业务连续性的基石。 时序数据库为何对可观测性数据至关重要时序数据（包括指标、日志和追踪）本质上具有时间序列性、高容量和追加写入的特性，这使得传统关系型数据库在存储和查询此类数据时效率低下 1。时序数据库（TSDB）是专为此类数据设计的，通过优化摄取、基于时间的查询、高效的存储压缩和专业索引来处理这些挑战 1。 选择时序数据库并非仅仅为了方便，而是基于性能的根本性决策，因为告警系统严重依赖于历史和实时时序数据的快速检索和分析。传统的关系型数据库由于其面向行的存储和针对事务性更新优化的索引，难以有效处理追加写入、高容量和时间有序的可观测性数据。相比之下，GreptimeDB等时序数据库采用列式存储并针对追加操作进行优化，这在基于时间的查询和聚合方面带来了显著的性能提升。这种底层的架构差异对于需要快速数据检索和时间窗口分析的实时告警系统来说至关重要。 GreptimeDB统一可观测性能力的概述GreptimeDB是一个开源、云原生、高性能的时序数据库，旨在提供灵活的时序数据管理解决方案 3。它能够在一个数据库中统一处理指标、日志、事件和追踪数据，将它们视为“带有时间戳和上下文的事件” 3。这种统一的方法简化了数据集成，并实现了全面的系统监控和分析 3。 GreptimeDB支持多种协议进行数据写入和查询，包括SQL、PromQL、Prometheus远程写入、InfluxDB、OpenTSDB和OpenTelemetry（OTLP） 3。这种广泛的兼容性降低了学习成本，并促进了数据迁移 3。 3. GreptimeDB：可观测性数据的基础GreptimeDB中的统一数据模型GreptimeDB将所有数据组织成“时序表”，这些表由三种语义类型的列组成：Tag（标签）、Timestamp（时间戳）和Field（字段） 4。 Timestamp：这是主要的时间索引，对于基于时间的查询和数据组织至关重要。每个表都必须且只能有一个Timestamp列 15。 Tag：这些列用于唯一标识一个时序数据，类似于Prometheus中的标签。GreptimeDB允许对特定标签进行选择性索引，这比Prometheus中所有标签都默认索引的方式提供了更大的灵活性 4。 Field：这些列包含实际的数据值，可以是数值、字符串或其他类型。GreptimeDB采用多值模型，允许一行数据包含多个字段列，这与OpenTSDB和Prometheus等采用的单值模型不同。多值模型可以提高写入&#x2F;读取效率并减少传输流量 16。 GreptimeDB能够自动将多个Prometheus指标分组到物理表中 4。 GreptimeDB的“无模式”方法 15（针对字段）与显式标签和单一时间戳相结合，提供了高度的灵活性。可观测性数据（指标、日志、追踪）的模式通常是多变且不断演进的。如果采用严格的模式，将需要频繁的模式迁移，从而阻碍系统的敏捷性。针对字段的无模式特性允许不同类型的事件（例如，CPU指标与日志条目）共存或轻松摄取，而无需严格的模式强制。这种灵活性对于告警中心至关重要，因为它需要从各种来源摄取数据而无需持续调整模式，从而支持新监控目标的快速迭代和集成。 GreptimeDB的模式设计最佳实践为了优化数据存储和检索效率，遵循GreptimeDB的模式设计最佳实践至关重要： 表命名：表名通常与指标名称、日志源名称或度量名称保持一致 15。 主键（标签）：在Tag列上定义PRIMARY KEY约束，以唯一标识时序数据。这会隐式地将时间索引列添加到键的末尾，用于排序和去重 16。 时间索引：始终指定一个TIME INDEX列 16。 索引策略：利用GreptimeDB丰富的索引选项（倒排索引、全文索引、跳跃索引和向量索引）来加速查询，特别是对于高基数数据或日志分析 5。对于日志，可以将 FULLTEXT INDEX应用于字符串字段 18。 合并模式：理解last_row（默认，保留相同主键和时间戳的最新行）和last_non_null（保留每个字段的最新非空值）合并模式，以管理数据更新和去重 18。 TTL（Time-To-Live）：为表配置TTL以自动管理数据保留和存储成本，例如with(ttl=&#39;7d&#39;) 18。这对于高容量时序数据至关重要，因为较旧的数据可能查询频率较低或需要聚合。 以下是一些GreptimeDB统一数据模型的示例： 列名 GreptimeDB语义类型 数据类型 示例值 目的&#x2F;描述 host Tag STRING server-1 标识数据来源的主机名。 idc Tag STRING us-east-1 标识数据中心。 ts Timestamp TIMESTAMP 2024-07-20 10:00:00 数据生成的时间戳，表的唯一时间索引。 cpu_util Field DOUBLE 0.75 CPU利用率，实际指标值。 memory_util Field DOUBLE 0.60 内存利用率，实际指标值。 log_message Field STRING Request processed successfully. 应用程序日志内容。 log_level Tag STRING INFO 日志级别，用于过滤和分类。 trace_id Tag STRING abcdef123456 分布式追踪的唯一ID。 span_id Tag STRING 7890abcd 追踪中单个操作的唯一ID。 duration_nano Field BIGINT 150000000 操作持续时间（纳秒）。 service_name Tag STRING user-service 产生追踪或日志的服务名称。 示例模式： 系统指标：CREATE TABLE IF NOT EXISTS system_metrics (host STRING, idc STRING, cpu_util DOUBLE, memory_util DOUBLE, disk_util DOUBLE, ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(host, idc), TIME INDEX(ts)); 16。在此示例中， host和idc是标签，ts是时间戳，cpu_util、memory_util、disk_util是字段。 应用程序日志：CREATE TABLE access_logs (access_time TIMESTAMP TIME INDEX, remote_addr STRING, http_status STRING, http_method STRING, http_refer STRING, user_agent STRING, request STRING) with (&#39;append_mode&#39;=&#39;true&#39;); 17。这是一个仅追加的表，针对日志存储进行了优化，查询速度通常更快 17。 Spring Boot的数据摄取策略为了将可观测性数据高效地摄取到GreptimeDB中，Spring Boot应用程序可以采用多种策略： OpenTelemetry (OTLP) 用于标准化数据收集： GreptimeDB是一个OpenTelemetry原生的可观测性数据库，能够通过HTTP协议原生消费OTLP指标、日志和追踪数据 5。Spring Boot应用程序可以利用OpenTelemetry SDK（例如，Java的 io.opentelemetry:opentelemetry-exporter-otlp）将遥测数据直接导出到GreptimeDB的OTLP端点 12。配置时需要指定GreptimeDB的主机、数据库名称（ X-Greptime-DB-Name头）和认证（基本认证） 12。对于追踪， x-greptime-pipeline-name应配置为greptime_trace_v1 12。日志可以指定 X-Greptime-Log-Table-Name 12。 采用OTLP进行数据摄取，告警中心获得了供应商中立性，并使其数据收集面向未来。OpenTelemetry正在成为可观测性领域的行业标准。如果后端时序数据库未来发生变化，应用程序的仪表化将基本不受影响。这种方法减少了供应商锁定，并简化了未来的迁移或多供应商可观测性策略。这增强了系统在快速演进的可观测性环境中的长期可维护性和适应性。Spring Boot 3通过Micrometer Tracing支持OpenTelemetry 19。使用OpenTelemetry Java代理可以实现零代码仪表化，它会自动仪表化支持的库（如Spring MVC、JDBC、Kafka） 21。 Prometheus远程写入兼容性： GreptimeDB与Prometheus完全兼容，可以作为Prometheus指标的远程写入端点 4。Spring Boot应用程序可以通过Micrometer的Prometheus注册表暴露指标（ /actuator/prometheus端点） 20。然后，Prometheus服务器可以抓取这些指标并将其远程写入GreptimeDB 4。 GreptimeDB与Prometheus远程写入的兼容性 4 使得现有系统能够更平滑地过渡或共存。许多现有系统依赖Prometheus进行指标收集。这意味着已经为Prometheus进行仪表化的应用程序不需要立即重新仪表化，从而降低了使用GreptimeDB的门槛。这种能力对于大型企业环境至关重要，因为它支持增量式采用并避免了“大爆炸”式的迁移。 JDBC&#x2F;SQL直接集成： GreptimeDB支持MySQL协议，允许Spring Boot应用程序通过JDBC使用MySQL驱动程序进行连接 25。标准的Spring Data JDBC或JPA（类似于Go中的GORM模型，表明Java中也有类似ORM模式）可用于数据插入和查询 25。 Spring Boot数据库性能最佳实践 26： 连接池：对于管理连接开销和提高性能至关重要。根据应用程序负载配置连接池大小、连接超时和空闲超时 26。 查询优化：定期分析和优化SQL查询以获得更好的执行计划 26。 索引：在频繁访问的列上创建索引以加快检索时间 26。GreptimeDB的标签列是隐式索引的 15。 事务管理：正确处理事务以确保数据完整性 26。 GreptimeDB内置流引擎的实时数据聚合GreptimeDB包含一个轻量级的内置流引擎，无需外部流系统（如Kafka Streams或Flink）即可实现连续数据聚合 9。用户可以使用简单的SQL定义实时“流”（Flow）任务 9。 内置流引擎 11 直接解决了管理独立流平台（如Kafka、Flink）进行实时聚合的复杂性。这减少了整体架构中的组件数量，简化了部署、维护和调试。对于告警系统而言，预聚合数据可以显著降低告警评估期间的查询负载。这种架构选择能够降低总拥有成本（TCO），并加速新告警定义（需要聚合数据）的上市时间。 用例： 在1分钟窗口内从追踪跨度计算RED（速率、错误、持续时间）指标 11。 通过聚合服务间的调用次数来执行服务依赖分析 11。 4. 告警中心的Spring Boot应用架构核心组件告警中心将由以下核心微服务组件构成： 数据摄取服务：负责接收原始可观测性数据（例如，通过OTLP、Prometheus远程写入或自定义API），并将其写入GreptimeDB。 告警规则管理服务：提供用于定义、更新和查询告警规则的API。将规则存储在单独的、高可用的关系型数据库中。 告警评估引擎：持续查询GreptimeDB中的时序数据，根据活跃告警规则进行评估，并识别告警条件。 通知分发服务：处理告警到各种渠道（电子邮件、短信、Webhook）的发送，并管理告警的升级、去重和抑制。 架构模式 微服务架构： 将告警中心分解为更小、可独立部署的服务（如摄取、规则管理、评估、通知）可增强模块化、可扩展性和故障隔离 28。每个服务专注于单一职责，从而简化开发、测试和维护。 事件驱动架构： 利用消息中间件（如Kafka、RabbitMQ）实现微服务之间的异步通信 30。这种方法可以解耦组件，提高系统响应能力，并通过缓冲消息提供容错能力 33。Spring Cloud Stream简化了使用共享消息系统构建事件驱动微服务的过程 30。 通过使用消息队列（如Kafka、RabbitMQ）进行关键流程（如向通知服务发送告警）的通信，系统变得更具弹性。微服务之间的直接同步API调用会造成紧密耦合，并可能导致级联故障。如果通知服务暂时停机，消息会在队列中缓冲，并在服务恢复后进行处理 33。这种设计显著提高了告警交付的可靠性，防止了在瞬时服务中断期间遗漏通知。 Spring Boot可扩展性和可维护性最佳实践 单一职责原则（SRP）：将SRP应用于类和服务（例如，将用户验证与用户创建分离），以确保模块化、更易于维护、测试和扩展 27。 依赖注入（DI）和控制反转（IoC）：Spring Boot的核心原则，促进松耦合和可测试性 3。 连接池：如第3节所述，为所有数据库交互（GreptimeDB、用于规则的关系型数据库）配置连接池 26。 查询优化和索引：持续监控和调整查询，并确保适当的索引 26。 外部化配置：使用环境变量、Kubernetes ConfigMaps或Spring Boot的外部化配置功能管理敏感信息（数据库凭据、API密钥） 32。 5. 告警规则管理与评估引擎告警定义的数据模型告警规则通常是静态配置，不常更改，且需要事务一致性。因此，将其存储在关系型数据库（例如PostgreSQL、MySQL）中比时序数据库更合适 1。关系型数据库提供健壮的ACID特性、复杂的规则管理查询能力以及成熟的模式演进和备份机制。 将关系型数据库用于告警规则，并将时序数据库（GreptimeDB）用于可观测性数据，是“多态持久化”的典型范例。这种方法认识到不同类型的数据具有不同的存储、访问和一致性要求。告警规则是结构化的、事务性的且数据量较小，非常适合关系型数据库管理系统（RDBMS）。而可观测性数据是高容量、追加写入且时间有序的，非常适合时序数据库（TSDB）。这种混合方法优化了每种数据类型的性能和可管理性，从而构建了一个更高效、更健壮的整体系统。 以下是告警规则定义模式的示例： 列名 数据类型 约束&#x2F;描述 rule_id UUID &#x2F; BIGINT 主键，告警规则的唯一标识符。 rule_name VARCHAR(255) 告警规则的名称，应唯一。 description TEXT 告警规则的详细描述。 severity ENUM 告警的严重程度（例如：CRITICAL、WARNING、INFO）。 expression TEXT &#x2F; JSON 告警评估的表达式（例如：PromQL-like语法或自定义规则引擎语法）。 evaluation_interval_seconds INT 告警条件评估的频率（秒）。 for_duration_seconds INT 告警条件必须持续为真才能触发告警的时间（秒）。 enabled BOOLEAN 告警规则是否启用。 notification_channels JSONB &#x2F; TEXT 告警通知发送到的渠道列表（例如：&#96;&#96;）。 created_at TIMESTAMP 规则创建时间。 updated_at TIMESTAMP 规则最后更新时间。 last_triggered_at TIMESTAMP 规则上次触发告警的时间（可为空）。 status ENUM 告警规则的当前状态（例如：ACTIVE、FIRING、RESOLVED）。 owner_id UUID &#x2F; BIGINT 规则所有者的ID（外键，指向用户&#x2F;团队表）。 使用Spring Boot实现规则引擎 规则引擎微服务： 将业务规则外部化到专用的微服务中，以提高灵活性、适应性和可维护性 28。这使得规则逻辑与核心应用程序代码分离。 将告警逻辑直接嵌入代码中需要每次规则更改时都进行重新部署。规则引擎微服务 28 允许通过API动态定义、修改和激活规则，而无需触及核心应用程序代码。这对于告警系统至关重要，因为阈值和条件可能需要根据不断变化的系统行为或业务需求进行频繁调整。这种方法能够更快地响应不断变化的监控需求，并减少操作摩擦。 与Spring Boot集成： Spring Boot本身不提供内置规则引擎，但与外部规则引擎集成良好 38。 Easy Rules：一个轻量级的、基于POJO的规则引擎，可以使用注解（@Rule、@Condition、@Action）轻松集成 39。它简化了业务规则的定义和管理 39。 Drools：一个功能强大的开源业务规则管理系统（BRMS），适用于更复杂的规则集，提供Web编写和DMN模型支持等功能 38。 规则存储库：规则可以存储在关系型数据库中（如上定义），并加载到规则引擎的工作内存中 38。 API层：暴露RESTful API，用于管理规则（例如，保存简单或复合规则）以及通过规则处理实体 28。 实时告警评估 流处理用于持续评估： 利用Spring Cloud Stream或Kafka Streams对来自GreptimeDB的传入时序数据进行实时处理 30。 Spring Cloud Stream：简化了构建连接到消息中间件（Kafka、RabbitMQ）的事件驱动微服务 30。它处理生产者和消费者的样板配置 30。 Kafka Streams：一个轻量级库，用于直接在Kafka上构建流应用程序。它提供了KStream（独立的事件流）和KTable（数据快照，有状态，跟踪每个键的最新值）抽象，非常适合聚合和连接操作 31。 评估逻辑： 评估引擎将定期查询GreptimeDB以获取特定指标、日志或追踪在特定时间窗口内的数据。例如，它可以查询CPU利用率（来自system_metrics表的cpu_util）是否在持续时间内超过某个阈值。利用GreptimeDB的SQL和PromQL支持进行灵活查询 3。将检索到的数据（事实）集成到所选的规则引擎（例如Easy Rules）中，以根据定义的条件进行评估 38。 定义复杂告警条件： 规则可以使用模仿PromQL的表达式定义，允许在时间窗口上进行聚合（例如，rate、sum、avg） 11。GreptimeDB的内置流引擎（Flows）可以预聚合常见RED指标或服务依赖关系的数据，从而简化实时评估逻辑 11。 6. 多渠道通知服务异步通知分发实现一个专用的通知分发服务，该服务从消息队列（例如，用于alert_events的Kafka主题）消费告警事件 33。这确保了与告警评估引擎的解耦，并提供了可靠性；如果通知服务宕机，告警将被排队并在服务恢复后处理 33。 与通知渠道集成 电子邮件（Spring Mail）： Spring Boot提供了spring-boot-starter-mail以便于集成 42。在 application.properties中配置SMTP服务器详细信息（主机、端口、用户名、密码、TLS） 37。使用 JavaMailSender发送SimpleMailMessage（纯文本）或MimeMessage（HTML和附件） 37。实现电子邮件模板以实现一致的告警格式 42。 短信（Twilio）： 集成Twilio Java Library以发送短信 44。使用 ACCOUNT_SID和AUTH_TOKEN初始化Twilio（最好存储为环境变量） 44。使用 Message.creator()构建和发送短信 44。 Webhook（Slack，通用）： 对于Slack，使用Incoming Webhooks（传入Webhook）和唯一URL发送JSON负载 46。Slack Java SDK简化了此过程 47。对于通用Webhook，使用Spring的 WebClient向指定URL发送带有JSON负载的POST请求 48。实现自定义Spring事件（ ApplicationEventPublisher，@EventListener）以从特定操作触发Webhook逻辑 48。 处理告警升级、去重和抑制 去重：在通知分发服务中实现逻辑，以防止为同一持续告警发送多个重复通知。这可以通过在缓存或专用数据库表中跟踪活动告警来实现。 抑制：允许用户定义规则，以抑制已知问题或维护期间的通知。此逻辑也将驻留在通知分发服务中。 升级：定义升级策略（例如，5分钟后，发送到不同的渠道或不同的团队成员）。这需要对活动告警进行状态管理和定期检查。 以下是通知渠道配置的示例： 渠道类型 配置参数 示例值 安全注意事项 Spring Boot配置属性 EMAIL SMTP Host smtp.gmail.com 环境变量&#x2F;秘密管理 spring.mail.host SMTP Port 587 spring.mail.port Username my.gmail@gmail.com 环境变量&#x2F;秘密管理 spring.mail.username Password app_password 环境变量&#x2F;秘密管理 spring.mail.password SMS Twilio Account SID ACxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 环境变量&#x2F;秘密管理 twilio.account-sid Twilio Auth Token your_auth_token 环境变量&#x2F;秘密管理 twilio.auth-token Twilio Phone Number +15017122661 twilio.phone-number SLACK_WEBHOOK Webhook URL  环境变量&#x2F;秘密管理 slack.webhook.url GENERIC_WEBHOOK Endpoint URL  环境变量&#x2F;秘密管理 webhook.generic.endpoint API Key Header X-Custom-Auth 环境变量&#x2F;秘密管理 webhook.generic.api-key-header API Key Value your_api_key_value 环境变量&#x2F;秘密管理 webhook.generic.api-key-value 7. 确保高可用性、可扩展性和容错性GreptimeDB的云原生优势GreptimeDB的云原生设计，特别是计算存储分离和Kubernetes集成 5，直接有助于告警系统的整体高可用性和可扩展性。通过将数据持久化卸载到高可用的对象存储，并允许计算节点独立扩展，数据库层成为一个强大的基础，能够处理海量数据和波动的负载，而不会成为单点故障。这种数据库层面的架构选择简化了Spring Boot应用程序的高可用性设计，因为它能够依赖底层数据存储固有的弹性。 计算存储分离：GreptimeDB的架构将计算（Frontend、Datanode、Metasrv）与存储（S3、对象存储）分离，允许资源独立扩展 4。这显著降低了存储成本（与EBS相比可降低高达75%，运营&#x2F;存储成本降低50倍）并实现了基于数据负载的弹性扩展 3。 Kubernetes原生：专为Kubernetes构建，促进了无限的水平扩展和云环境中的高效管理 4。 高性能和成本效益：采用Rust编写，具有分布式查询引擎、丰富的索引和优化的列式存储，可在PB级数据集上实现亚秒级响应 5。 数据持久性：数据持久化存储在经济高效的对象存储中，并使用基于SSD的云卷用于WAL和缓存，以确保可靠性和性能 11。 读写分离（企业版）：企业版支持写入节点独立扩展，以实现高吞吐量摄取而不影响查询性能 11。这对于写入密集型可观测性工作负载是一个关键考虑因素。 Spring Boot应用弹性模式 负载均衡：使用Nginx或云原生负载均衡器将传入流量分配到Spring Boot微服务的多个实例（例如，摄取、规则管理） 29。这可以防止单个实例过载并提高容错能力 50。 服务发现：使用Eureka等工具使微服务能够动态地相互定位 50。 数据库复制：对于存储告警规则的关系型数据库，配置主从或多主复制，以确保即使数据库实例发生故障，数据也可用 29。 容错机制（Resilience4j）：在Spring Boot服务中实现断路器、重试、速率限制器和时间限制器 50。 断路器：通过暂时停止对缓慢或易出错服务的请求来防止级联故障，使其能够恢复 51。 重试：自动重试因瞬时问题而失败的操作 51。 时间限制器：为服务调用设置时间限制，防止长时间挂起 51。 集中式日志记录和监控： 实施ELK Stack或Grafana Loki等工具进行集中式日志管理 20。使用Prometheus和Grafana监控应用程序性能，并为告警系统本身设置告警 23。 容器化和编排：将Spring Boot应用程序作为Docker容器部署在Kubernetes上，以自动化部署、扩展和管理，确保高可用性和容错性 29。 8. 结论与未来增强健壮告警中心设计的总结所提出的架构利用了GreptimeDB统一、高性能的时序数据能力，用于处理指标、日志和追踪数据，并结合了Spring Boot健壮的框架来构建模块化和弹性的微服务。关键组件包括通过OpenTelemetry和Prometheus实现的灵活数据摄取、由关系型数据库支持的规则管理系统与集成规则引擎以实现动态条件，以及异步、多渠道的通知服务。高可用性和可扩展性通过GreptimeDB的云原生设计和Spring Boot的弹性模式（负载均衡、断路器、消息队列）得到保障。 潜在的未来增强 AI辅助洞察：集成机器学习模型进行异常检测（例如，预测GreptimeDB数据中的异常模式），以减少误报并提高告警精度。 高级分析：开发自定义仪表板和报告工具（例如，使用Grafana与GreptimeDB作为数据源），以深入分析告警趋势、通知有效性和系统健康状况随时间的变化 23。 自动化修复：扩展通知服务，以响应特定的关键告警触发自动化操作（例如，运行脚本、重启服务）。 可定制仪表板：允许用户在告警中心的UI中创建和定制自己的告警仪表板，直接从GreptimeDB拉取数据。 与事件管理系统集成：与PagerDuty或Opsgenie等工具无缝集成，以简化事件响应工作流程。 告警调优反馈循环：实施机制，允许用户提供告警相关性的反馈，帮助完善规则定义并减少告警疲劳。 "},{"title":"使用docx4j将docx转换为pdf","date":"2025-06-26T16:00:00.000Z","url":"/2025/06/27/2025/06/201724/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":"📚 Docx4J 简介Docx4J 是一个基于 Java 的开源库，支持： 读取&#x2F;写入 .docx 文件 转换为 PDF（通过 Apache FOP 或其他渲染引擎） 处理复杂格式（表格、图表、图片、样式等） ​优势​：纯 Java 实现，无需外部依赖（如 LaTeX），适合集成到 Java 项目中。 Maven项目添加相关依赖 示例 字体映射"},{"title":"MySQL中GROUP_CONCAT长度限制","date":"2025-06-19T16:00:00.000Z","url":"/2025/06/20/2025/06/201045/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":"在开发一个用户权限矩阵的功能时,遇到了一个奇怪的问题,数据结果集比预期要少很多. 代码如下 这条SQL的作用是聚合查询出不同cluster下的所有study,然后与授权资源进行匹配,之后将用户与资源study关联保存. 这条 SQL 语句在语法上是正确的，但在实际使用中可能存在以下潜在问题： ​1. GROUP_CONCAT 长度限制（潜在数据截断风险）​​ ​问题说明​：MySQL 中 GROUP_CONCAT 的默认最大长度是 1024 字节（通过系统变量 group_concat_max_len控制）。如果 study_ids 连接后的字符串超过该长度，结果会被静默截断。 ​解决方案​： ​2. 未指定分隔符导致歧义​ ​问题说明​：GROUP_CONCAT 默认使用逗号 , 分隔值。如果 study_id 本身包含逗号（如 GUID 或其他复合值），会导致拼接后的结果无法准确区分原始值。 ​解决方案​：显式指定其他分隔符（例如分号 ;）： ​3. 大量 DISTINCT 可能导致性能问题​ ​问题说明​：当单个 cluster_id 包含大量唯一的 study_id 时，DISTINCT 操作可能消耗较多内存和 CPU。 ​解决方案​：确保 (cluster_id, study_id) 上有索引，减少临时表操作： ​4. 缺少 NULL 值处理​ ​问题说明​：如果 study_id 存在 NULL 值，DISTINCT 会直接忽略它，但业务逻辑可能需要记录缺失值。 ​解决方案​：使用 COALESCE() 替换 NULL（例如标记为 ‘N&#x2F;A’）： ​优化后的完整查询​ ​建议​ ​评估数据规模​：通过抽样检查最大 study_ids 长度，确保 group_concat_max_len 足够大。 ​验证分隔符​：确保分隔符（如 ;）不出现在 study_id 原始值中。 ​添加索引​：在 (cluster_id, study_id) 上创建索引提升分组性能。 ​业务逻辑检查​：确认是否需处理 study_id 为 NULL 的情况。 其他解决方案使用全量查询,在代码中进行聚合操作. "},{"title":"centos-vault镜像源","date":"2025-05-21T16:00:00.000Z","url":"/2025/05/22/2025/05/221004/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"CentOS Vault 是一个用于存储已经停止支持的 CentOS 版本的软件仓库。它包含了这些版本的历史归档，方便用户在需要时访问和使用这些旧版本的软件包。 CentOS8使用 CentOS Vault 的方法centos8（centos8官方源已下线，建议切换centos-vault源 当 CentOS 的某个版本停止支持后，官方会将该版本的软件包移到 CentOS Vault 中。用户可以通过修改系统的 yum 源配置来使用 CentOS Vault 中的软件包 或者 centos6 centos7 清华源 运行 yum makecache 生成缓存"},{"title":"KubernetesClientException: An error has occurred","date":"2025-05-16T08:15:48.000Z","url":"/2025/05/16/2025/05/161548/","tags":[["java","/tags/java/"],["kubernetes","/tags/kubernetes/"],["istio","/tags/istio/"]],"categories":[["Java","/categories/Java/"]],"content":"问题背景最近在升级kubernetes的istio服务,在使用java客户端访问kubernetes集群时,出现如下异常: 问题分析注意到日志中出现了2025-05-16T11:22:34.417528586+08:00 stdout F Caused by: java.util.concurrent.TimeoutException: null,这是因为在等待kubernetes集群响应时,超时了，首先想到的解决办法是增加超时时间, 再次执行之后,出现了新的错误 也就是说之前的超时错误只是表面现象,真正的原因是kubernetes集群中升级后的istio服务无法访问到,所以超时了. 解决方案 检查kubernetes集群中相关的istio服务,修改为正确的配置. "},{"title":"五月记录","date":"2025-05-05T16:00:00.000Z","url":"/2025/05/06/2025/05/061732/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"开局不利,感冒了 过完五一,静待端午 工作很不舒服"},{"title":"导航网站","date":"2025-04-29T16:00:00.000Z","url":"/2025/04/30/2025/04/291351/","tags":[["Navigation","/tags/Navigation/"]],"categories":[["Linux","/categories/Linux/"]],"content":"导航网站搜索引擎 Google Bing DuckDuckGo 百度 社交媒体 GitHub Twitter 微博 知乎 开发工具 Stack Overflow MDN Web Docs Docker Hub NPM 常用工具 在线 JSON 格式化 Can I use TinyPNG 设计资源 Unsplash - 免费高质量图片 Pexels - 免费素材图片 Iconfont - 阿里巴巴矢量图标库 音乐网站 网易云音乐 QQ音乐 Spotify 视频网站 YouTube Bilibili 腾讯视频 爱奇艺 "},{"title":"命令方式检查笔记本电池使用情况","date":"2025-04-27T16:00:00.000Z","url":"/2025/04/28/2025/04/281428/","tags":[["windows","/tags/windows/"]],"categories":[["work","/categories/work/"]],"content":"在powershell或者cmd下输入以下命令 打开报告文件,一般位于用户文件目录下"},{"title":"网段不能互通的问题排查记录","date":"2025-04-22T16:00:00.000Z","url":"/2025/04/23/2025/04/231607/","tags":[["docker","/tags/docker/"],["ArchLinux","/tags/ArchLinux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"本地一个服务器的ip地址为192.168.72.5,需要访问内网中的其他服务,比如git(ip为192.168.99.69). 问题分析当192.168.72.5无法ping通192.168.99.69时，通常是由于网络配置或连接问题导致的 子网不同，但是可以确定两个子网间有正确的路由配置（如路由器或三层交换机）。 物理连接正常,检查网线路由器接口等 检查本地主机IP配置,ip a输出检查IP,子网掩码和默认网关正常 验证路由可达性 检查防火墙(这一步也很重要) 使用网络诊断工具 Traceroute 验证路由表 发现网络路由配置存在 ​严重冲突，尤其是大量 Docker 网桥和自定义子网的路由条目干扰了主网络的路由决策，导致流量被错误引导。 解决步骤 清理冲突的 Docker 网桥路由​ ​永久解决方案​： 停止docker服务 sudo systemctl stop docker 禁用docker自动创建网桥 重启docker "},{"title":"rsync用法","date":"2025-04-17T16:00:00.000Z","url":"/2025/04/18/2025/04/181042/","tags":[["linux","/tags/linux/"],["shell","/tags/shell/"]],"categories":[["Java","/categories/Java/"]],"content":" rsync 是一个高效的文件同步和传输工具，支持本地和远程同步，并通过增量传输减少数据传输量。以下是其基本用法和常见场景： ​一、安装 rsync​ ​Linux​：多数发行版已预装，或通过包管理器安装： ​macOS​：通常预装，或通过 Homebrew 安装：brew install rsync ​Windows​：可通过 Cygwin 或 WSL 使用。 ​二、基础命令格式​ ​常用选项​ 选项 说明 -a 归档模式（保留权限、时间、软链接等，相当于 -rlptgoD） -v 输出详细信息（可叠加 -vv 更详细） -z 压缩传输（节省带宽） -P 显示进度，支持断点续传（等同于 --progress --partial） --delete 删除目标中多余文件（保持严格同步） --exclude 排除指定文件&#x2F;目录（支持通配符） --bwlimit=KBPS 限制传输带宽（单位：KB&#x2F;s） -n 或 --dry-run 模拟运行（不实际传输） ​三、常见使用场景​1. ​本地同步目录​ 2. ​同步到远程服务器​ 3. ​排除特定文件&#x2F;目录​ 4. ​限速传输（如限制为 500KB&#x2F;s）​​ 5. ​镜像同步（严格保持目标与源一致）​​ 6. ​通过 SSH 自定义端口同步​ ​四、实际示例​1. 备份网站目录到远程服务器 2. 同步开发代码，排除临时文件 3. 增量备份本地文档 ​五、注意事项​ ​路径格式​：源路径末尾的 / 会影响同步行为（是否包含目录本身）。 ​权限问题​：远程同步需确保 SSH 权限正确（推荐使用密钥认证）。 ​首次同步​：建议先用 -n 模拟运行，确认无误后再执行。 ​大文件传输​：使用 -P 支持断点续传，避免网络中断重传。 "},{"title":"uv包管理器","date":"2025-03-28T06:06:09.000Z","url":"/2025/03/28/2025/03/281515/","tags":[["python","/tags/python/"],["anaconda","/tags/anaconda/"],["pip","/tags/pip/"]],"categories":[["Python","/categories/Python/"]],"content":"uv 是 Astral 公司开发的一款革命性的 Python 包管理工具，被誉为 Python 包管理的未来。它以其极致的性能和现代化的设计理念，正在改变 Python 开发者的工作流程。 什么是 uv？uv 是 Astral 公司开发的一款高性能的 Python 包管理工具，对标 Rust 的 Cargo 包管理器。它旨在替代传统的 pip、pip-tools 和 virtualenv，提供更快的依赖解析和安装速度，同时支持虚拟环境管理、依赖锁定等现代化功能。 为什么选择 uv？ 🚀 极速性能：比 pip 快 10-100 倍 🔒 确定性构建：确保可重现的依赖环境 🛠️ 统一工具链：一个工具解决所有包管理需求 🌍 跨平台支持：Windows、Linux、macOS 完美支持 📦 现代化设计：借鉴 Rust Cargo 的优秀设计理念 安装 uv方法一：使用官方安装脚本（推荐） 方法二：使用 pip 安装 方法三：使用包管理器 验证安装 核心功能详解1. 包安装与管理基本安装命令 高级安装选项 2. 虚拟环境管理创建和管理虚拟环境 激活虚拟环境 虚拟环境管理命令 3. 依赖锁定与编译生成依赖锁定文件 同步依赖 4. 项目初始化与管理创建新项目 项目依赖管理 性能对比与优势速度对比根据官方基准测试，uv 在各个方面都显著优于传统工具： 操作 pip uv 提升倍数 依赖解析 2.8s 0.1s 28x 包安装 45s 0.5s 90x 虚拟环境创建 3.2s 0.1s 32x 核心优势 🚀 极速性能：比 pip 快 10-100 倍，大幅提升开发效率 🔒 确定性构建：通过锁定文件确保完全可重现的构建环境 🛠️ 统一工具链：一个工具替代 pip、virtualenv、pip-tools 🌍 跨平台支持：Windows、Linux、macOS 完美支持 📦 现代化设计：借鉴 Rust Cargo 的优秀设计理念 💾 智能缓存：避免重复下载和编译 🔍 更好的错误信息：清晰的错误提示和解决建议 实际使用场景场景一：新项目开发 场景二：现有项目迁移 场景三：CI&#x2F;CD 环境 常用命令速查基础命令 项目命令 环境命令 最佳实践1. 项目结构建议 2. 依赖管理策略 3. 团队协作建议 使用 uv pip compile 生成锁定文件 将锁定文件提交到版本控制 在 CI&#x2F;CD 中使用 uv pip sync 确保环境一致性 定期更新依赖并测试兼容性 常见问题与解决方案Q1: 如何从 pip 迁移到 uv？A: 可以逐步迁移，无需一次性替换： Q2: uv 与 Poetry&#x2F;PDM 有什么区别？ 特性 uv Poetry PDM 速度 极快 中等 快 功能范围 包管理 全栈项目管理 包管理+发布 学习曲线 简单 中等 简单 生态成熟度 新 成熟 中等 Q3: 如何处理私有包仓库？ Q4: 如何解决依赖冲突？ 总结uv 作为新一代 Python 包管理工具，以其卓越的性能和现代化的设计理念，正在成为 Python 开发者的首选。无论是新项目开发还是现有项目迁移，uv 都能显著提升开发效率和构建速度。 推荐使用场景： 新项目开发 需要快速构建的 CI&#x2F;CD 环境 大型项目的依赖管理 团队协作开发 开始使用 uv，体验 Python 包管理的未来！ 🚀 更多详细信息请参考 uv 官方文档 或运行 uv --help 查看帮助信息。"},{"title":"uBlock Origin","date":"2025-03-16T16:00:00.000Z","url":"/2025/03/17/2025/03/171343/","tags":[["tool","/tags/tool/"]],"categories":[["Linux","/categories/Linux/"]],"content":"uBlock Origin uBlock Origin 鉴于一些浏览器的应用商店中下架了uBlock Origin，所以需要手动安装。 uBlock Origin是一款高效的开源广告拦截器和内容过滤工具。它具有以下特点： 低内存占用，性能优异 默认过滤规则完善，开箱即用 支持自定义过滤规则和白名单 可以拦截广告、追踪器、恶意软件等 支持多种浏览器，包括Chrome、Firefox、Edge等 完全免费且无广告 安装方法： 访问浏览器的扩展商店 搜索”uBlock Origin” 点击安装即可 使用建议： 建议保持默认过滤规则 可根据需要添加其他规则列表 遇到网站显示异常时可临时禁用 定期更新过滤规则以保持最佳效果 注意事项： 请使用官方版本，避免使用第三方修改版 某些网站可能需要将其加入白名单 "},{"title":"安装 HEVC 视频扩展","date":"2025-03-16T16:00:00.000Z","url":"/2025/03/17/2025/03/171348/","tags":[["encode","/tags/encode/"],["windows","/tags/windows/"]],"categories":[["wuw","/categories/wuw/"]],"content":"HEVC（高效视频编码）是一种视频压缩标准，可实现更小文件大小的高质量视频播放。 虽然 Windows 11 支持 HEVC 播放，但某些设备可能没有本机支持。 在这种情况下，用户可以从设备制造商下载 HEVC 视频扩展，或从 Microsoft Store 下载 HEVC 视频扩展（HEVC 编解码器）。 但是，从 Microsoft Store 下载文件可能有点棘手。幸运的是，Adguard 是一个第三方网络服务和在线链接生成器，可以轻松地从 Microsoft Store 下载 appx、appxbundle 和 msixbundle 文件。 第 1 步：在 Microsoft Store 上查找应用程序 URL首先，您需要在 Microsoft Store 上找到应用程序 URL。在浏览器上打开 Microsoft Store 并搜索 HEVC Video Extension。找到它后，从地址栏中复制应用程序 URL，包括产品 ID（或直接复制下面的链接）： 第 2 步：使用 Adguard 生成下载链接接下来，转到 Adguard 页面： 然后将应用程序 ProductId(9n4wgh0z6vhq)或者URL 粘贴到提供的空白处。然后，单击复选标记按钮以生成直接下载链接 第 3 步：下载 HEVC 视频扩展最后，单击 .appxbundle 链接下载文件： Microsoft.HEVCVideoExtension_2.2.33.0_neutral_~_8wekyb3d8bbwe.appxbundle Microsoft Edge 有时可能会因不安全而阻止下载。在这种情况下，请检查浏览器的“下载历史记录”并选择“保留”文件，或者您可以使用 Google Chrome 或 Firefox 等其他浏览器完成下载。 按照上述步骤，您可以轻松下载 HEVC 视频扩展 （HEVC 编解码器） 并增强您在 Windows 11 上的视频体验 参考文档 How to Download HEVC Video Extension for Free "},{"title":"Redis内存管理机制","date":"2025-03-11T16:00:00.000Z","url":"/2025/03/12/2025/03/101432/","tags":[["redis","/tags/redis/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Redis的所有数据库共享同一内存空间，并非按库平均分配内存。 一、Redis内存管理机制 统一内存池Redis的所有数据库（默认16个，编号db0-db15）共享同一个内存池。数据按键值对存储，无论属于哪个库，均占用同一内存空间。例如，db0中存储的键user:1和db1中的order:1会共同竞争内存资源。 内存分配策略 内存分配基于全局配置参数（如maxmemory），当总内存达到上限时，触发淘汰策略（LRU、LFU等），不区分库； 内存优化手段（如SDS动态调整、整型存储）对所有库生效。 二、关键特性与影响 无隔离性风险若某个库数据量激增，可能挤占其他库的内存，导致数据被淘汰。例如： 若db0写入大量数据，可能触发全局内存淘汰机制，删除db1中的键。 内存统计维度Redis的INFO memory命令返回的是实例级内存使用情况，而非按库统计。需通过MEMORY STATS或MEMORY USAGE key分析具体键的占用。 三、优化建议 分实例隔离若需严格隔离不同业务的数据内存，应部署多个Redis实例，而非依赖多库。 精细化淘汰策略配置maxmemory-policy为allkeys-lru或volatile-ttl，平衡不同库的数据保留优先级。 四、与SDS内存优化的关联Redis通过**SDS（简单动态字符串）**优化字符串存储，例如： 预分配和惰性释放策略减少内存重分配次数； 根据字符串长度动态选择结构体（如sdshdr8、sdshdr16）。这些优化对所有库生效，进一步提升共享内存的利用率。 如需验证内存分配，可通过以下命令： "},{"title":"部署 AdGuard Home","date":"2025-03-11T16:00:00.000Z","url":"/2025/03/12/2025/03/121442/","tags":[["DNS","/tags/DNS/"],["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 部署adguard以下是使用 Docker Compose 部署 AdGuard Home 并配置 HTTPS 的详细步骤： 1. 准备工作创建所需目录和文件，用于持久化配置和存储证书： 2. 生成 SSL 证书方案一：自签名证书（测试用） 生成的证书 (adguard.crt) 和私钥 (adguard.key) 会保存在 ~/adguard/conf 目录。 方案二：Let’s Encrypt 证书（推荐有域名） 使用 Certbot 获取证书（需域名已解析到服务器）： 将证书复制到 AdGuard 目录： 3. 编写 docker-compose.yml 3.1 原始 Docker 命令（已转换为 docker-compose） 以下为原始的 docker run 命令，现已完全转换为上述 docker-compose.yml 配置： 4. 启动 AdGuard Home 5. 初始化配置 访问 （自签名证书需忽略浏览器警告）。 按照向导完成初始设置（管理员账号、DNS 上游等）。 6. 配置 HTTPS 进入 AdGuard 管理界面，转到 Settings → Encryption Settings。 启用 HTTPS： Certificate chain: 选择证书文件 /opt/adguardhome/conf/adguard.crt Private key: 选择私钥文件 /opt/adguardhome/conf/adguard.key 保存配置，AdGuard 将重启服务。 7. 验证 HTTPS重新访问 ，确认浏览器显示安全锁标志。 8. 可选优化HTTP 重定向到 HTTPS在 AdGuard 的 Web 界面中： 转到 Settings → Web UI Settings。 启用 Redirect HTTP to HTTPS。 防火墙开放端口 常见问题 证书路径错误检查 docker-compose.yml 中的卷挂载路径，确保证书位于 ~/adguard/conf。 端口冲突确保宿主机端口（53、443）未被其他服务占用。可运行 netstat -tuln | grep &lt;端口&gt; 检查。 自签名证书不受信任手动将 adguard.crt 导入操作系统或浏览器的受信任根证书颁发机构。 Let’s Encrypt 证书续期添加定时任务自动续期（需调整路径）： 完整目录结构 其他相关 Ubuntu18.04搭建docker的DNS解析服务 常用服务的docker部署记录 "},{"title":"Linux 统计文件行数的方法","date":"2025-03-06T16:00:00.000Z","url":"/2025/03/07/2025/03/071126/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在 Linux 系统中，统计文件的行数是一个常见的需求，尤其是在处理日志文件、源代码或任何文本文件时。有多种方法可以实现这一目的，每种方法都有其特点和适用场景。 使用 wc 命令最简单直接的方法是使用 wc 命令，它是 “word count”的缩写，但它也可以用来计算行数。使用 -l 选项可以直接输出文件的行数。例如，要统计 test1.sh 文件的行数，可以使用以下命令： 这将输出文件的行数和文件名。如果只需要行数，可以通过重定向文件内容到 wc 命令来避免显示文件名： 使用 awk 命令awk 是一个强大的文本处理工具，它也可以用来统计行数。awk 的 END 模式可以在处理完所有输入行之后执行操作，NR 变量保存了当前的记录号，也就是行数。以下是使用 awk 统计行数的两种方式： 方法一：打印每行的行号，然后使用 tail 命令获取最后一行的行号 方法二：直接在 END 模式下打印 NR，即最终的行数 使用 sed 命令sed 是另一个文本处理工具，它也可以用来统计行数。sed 的 -n 选项和 $&#x3D; 表达式可以一起使用来打印文件的最后一行的行号，也就是总行数： 使用 grep 命令grep 命令通常用于搜索文本，但它也可以用来统计行数。grep -c 可以计算匹配特定模式的行数。如果使用 grep -c “^”，它将计算所有非空行的数量： 使用脚本统计特定目录下的文件行数如果需要统计一个目录下所有文件的行数，可以编写一个简单的脚本来递归地处理每个文件。以下是一个示例脚本，它使用 sed 命令来统计每个文件的行数，并累加到总行数中： 运行这个脚本将输出目录中的文件数量和所有文件的总行数。可以通过直接执行脚本或传递一个或多个目录作为参数来使用它。 注意事项在使用这些方法时，需要注意一些可能影响结果的因素。例如，如果文件的最后一行没有换行符，wc -l 命令可能不会计算这一行。此外，对于非常大的文件，某些方法可能会消耗较多的资源或需要较长的时间来执行。因此，在选择方法时，应根据文件的大小和自己的需求来决定使用哪种方法。"},{"title":"使用 Conda 创建虚拟环境","date":"2025-03-06T06:06:09.000Z","url":"/2025/03/06/2025/04/151548/","tags":[["conda","/tags/conda/"]],"categories":[["Python","/categories/Python/"]],"content":"以下是使用 Conda 创建虚拟环境的详细步骤： 1. 创建虚拟环境基础命令： 示例：创建名为 myenv、Python 3.8 的环境： 如果不指定 Python 版本，会默认使用 Conda 的最新版本。 2. 激活&#x2F;停用虚拟环境 激活环境： 停用环境： 3. 安装包 在激活的环境中安装包： 使用 pip 安装（如果 Conda 源中没有该包）： 4. 查看虚拟环境 列出所有环境： 当前激活的环境会显示星号（*）。 5. 删除虚拟环境 6. 其他常用命令 查看环境中已安装的包： 更新包： 导出环境配置（用于迁移或分享）： 从 YAML 文件创建环境： 常见问题 网络问题：如果下载缓慢，可配置清华镜像源： 环境冲突：创建时若提示依赖冲突，尝试指定更宽松的版本（如 python=3.8 而非 python=3.8.5）。 通过虚拟环境，可以隔离不同项目的依赖，避免版本冲突。"},{"title":"缩略图技术方案设计","date":"2025-03-06T06:06:09.000Z","url":"/2025/03/06/2025/09/081424/","tags":[["docker","/tags/docker/"],["nginx","/tags/nginx/"],["image","/tags/image/"]],"categories":[["Java","/categories/Java/"]],"content":"缩略图技术方案设计 项目背景业务需求 支持多种图片格式的缩略图生成（JPEG、PNG、GIF、WebP） 提供灵活的尺寸调整能力，支持按需生成 保证高并发访问性能，响应时间 &lt; 200ms 降低存储成本，避免预生成大量固定尺寸缩略图 技术挑战 大量图片文件的存储管理和访问优化 不同尺寸缩略图的动态生成和缓存策略 高并发场景下的性能优化和资源控制 多租户环境下的资源隔离和权限管理 技术方案对比方案一：预生成缩略图实现方式在文件上传时预生成固定尺寸的缩略图，存储到文件系统中。 API接口上传接口 查询缩略图接口 返回结果 优化建议 支持动态分辨率：添加上传时参数，支持设置缩略图分辨率（可选） 智能返回：在返回文件信息时查询该文件是否存在缩略图地址，如果有则同时返回 降级处理：没有缩略图时前端设置默认占位图，尝试再次请求 优缺点分析优点： 访问速度快，无需实时处理 服务器负载低 实现简单，易于维护 缺点： 存储成本高，需要预生成多种尺寸 灵活性差，无法满足动态尺寸需求 上传时延增加 方案二：Nginx动态缩略图实现方式使用Nginx的http_image_filter_module模块，支持动态图片尺寸调整。 功能特性 ✅ 支持动态图片尺寸调整 ✅ 支持多种图片格式（JPEG、PNG、GIF、WebP） ✅ 支持URL参数控制尺寸 ✅ 支持动态文件预览链接 ✅ 支持反向代理到后端服务 ✅ 支持静态文件和动态文件流 ✅ 基于Alpine Linux，镜像体积小 ✅ 包含健康检查 测试示例 测试文件地址800x500 测试文件地址400x400 测试文件地址200x200 优缺点分析优点： 存储成本低，按需生成 灵活性高，支持任意尺寸 实时处理，满足动态需求 缺点： 首次访问延迟较高 服务器CPU负载增加 需要额外的缓存策略 系统架构整体架构图 请求流程 客户端发起缩略图请求 Nginx接收请求并解析尺寸参数 检查缓存中是否存在对应尺寸的缩略图 如果缓存命中，直接返回 如果缓存未命中，代理到后端服务获取原图 使用image_filter模块调整图片尺寸 将处理后的图片返回给客户端并缓存 详细实现Docker镜像构建Dockerfile Nginx配置主配置文件 性能分析性能指标 响应时间：首次访问 &lt; 500ms，缓存命中 &lt; 50ms 并发处理：支持1000+ QPS 内存使用：单worker进程 &lt; 100MB CPU使用：图片处理时CPU使用率 &lt; 80% 优化策略1. 缓存策略 2. 压缩优化 3. 连接优化 监控指标 请求响应时间分布 缓存命中率 错误率统计 资源使用情况 部署指南快速开始方法1：使用构建脚本 方法2：手动构建 使用示例静态图片尺寸调整 动态文件预览链接 目录结构 运维监控健康检查 日志监控 性能监控 故障排查常见问题 图片处理失败：检查GD库是否正确安装 代理超时：调整proxy_timeout配置 内存不足：增加image_filter_buffer大小 缓存不生效：检查缓存配置和权限 调试命令 技术细节基础信息 基础镜像：Alpine Linux 3.18 Nginx版本：1.24.0 编译模块：–with-http_image_filter_module 依赖库：GD库（支持图片处理） 支持格式：JPEG、PNG、GIF、WebP 安全考虑 限制图片处理的最大尺寸 设置合理的超时时间 实施访问频率限制 定期更新基础镜像 扩展性 支持水平扩展（多实例部署） 支持CDN集成 支持微服务架构 支持容器编排（Kubernetes） "},{"title":"PDF内容检测","date":"2025-02-27T16:00:00.000Z","url":"/2025/02/28/2025/02/281451/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":"在给pdf进行签名时，需要检测pdf内容是否为空，如果为空，则不允许签名。 但是存在一种情况,pdf的所有内容都为批注或者图章,这种文件会导致签名失败。 目标检测pdf文件内容类型,是否有内容,还是纯批注或者纯图章。 解题思路 方案设计 创建一个类，用于检测pdf文件内容类型。 如果文件检测未通过,则需要将该文件转换为图片之后再合成一个新的pdf 在新pdf文件中添加水印或者批注 实现细节 文件在预览时已经转换为图片,所以不需要再次转换。 但是此时的图片为jpg格式,有一定的精度丢失。 所以需要将图片转换为png格式,以减少精度丢失。 核心代码 参考附录 加密检测 "},{"title":"将字体复制到Docker镜像中","date":"2025-02-27T06:19:09.000Z","url":"/2025/02/27/2025/02/271419/","tags":[["font","/tags/font/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Java服务需要中文字体，因为生成图片、PDF，或者处理文字渲染时遇到了缺少字体的问题。常见的比如使用iText生成PDF，或者使用Apache POI处理Excel时，如果系统没有安装中文字体，可能会出现乱码或者方块的问题。 在Java服务的Docker镜像中处理中文字体，推荐以下分步方案： 方案选择：将字体复制到Docker镜像中此方法系统级支持更可靠，尤其适合依赖系统字体的第三方库。 步骤说明 准备字体文件将所需中文字体（如SimSun.ttf）放入项目目录的fonts/文件夹。 编写Dockerfile在镜像中安装字体工具，复制字体文件，并更新字体缓存。 构建镜像执行构建命令，确保字体被正确包含： 验证字体安装进入容器检查字体是否存在： 关键考虑因素 兼容性：确保第三方库（如PDF生成工具）能正确识别系统字体。 镜像体积：若使用Alpine镜像，需调整安装步骤（如apk add fontconfig）。 字体授权：确认字体允许嵌入，避免法律风险。 替代方案对比 打包到JAR中：需代码显式加载字体，适合纯应用级控制，但可能无法覆盖所有库。 运行时挂载字体：灵活但依赖部署配置，适合动态环境： 结论推荐将字体复制到镜像，确保系统级支持且兼容性最佳。若对镜像体积敏感或需动态更换字体，可考虑运行时挂载方案。"},{"title":"Java Mail报错：javax 迁移到 jakarta","date":"2025-02-24T06:06:09.000Z","url":"/2025/02/24/2025/02/241406/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":" Java Mail报错：javax 迁移到 jakarta 前言报错：java.lang.IllegalStateException: No provider of jakarta.mail.util.StreamProvider was found NoClassDefFoundError: jakarta&#x2F;activation&#x2F;MimetypesFileTypeMap 解决方案 参考附录 jakarta.activation-api "},{"title":"文档加密检测","date":"2025-02-13T06:06:09.000Z","url":"/2025/02/13/2025/02/051813/","tags":[["tika","/tags/tika/"]],"categories":[["Java","/categories/Java/"]],"content":"检测文件是否加密 前言在对文档类型的文件进行编辑或者格式转换时,如果存在加密, 则操作无法进行,为了避免堵塞进程,则在操作前进行一次加密检测. 背景需要将docx类型的文档转换成pdf,或者在word,pdf之类的文档上添加水印等. 目标在操作前可以正确获取到当前文档是否加密 解题思路 解析文件元数据,判断是否有加密相关的属性 尝试对文件内容进行解析,是否报与加密相关的错误 方案设计 下载临时文件 对文件进行检测 技术架构 jdk tika 实现细节 自定义检测器,仅包含目标文件类型以加快检测速度 配置PDF解析器仅处理元数据 writeLimit设置为0,不处理文件内容new BodyContentHandler(0) 单独处理PDF的加密元数据 捕获明确加密异常 处理POI加密异常 确保流可重复使用（需支持mark&#x2F;reset） 核心代码 效果展示 示例展示 参考附录 tika "},{"title":"使用 RestTemplate 下载文件","date":"2025-01-13T16:00:00.000Z","url":"/2025/01/14/2025/01/141123/","tags":[["spring","/tags/spring/"],["file","/tags/file/"]],"categories":[["Java","/categories/Java/"]],"content":" RestTemplate 下载 RetryTemplate 重试使用 RetryTemplate 包装重试逻辑，可以在网络不稳定或服务器临时故障时自动重试下载操作。以下是使用 RetryTemplate 实现重试逻辑的完整代码示例： 1. 添加依赖确保你的项目中已经引入了 spring-retry 依赖。如果使用 Maven，可以在 pom.xml 中添加以下依赖： 2. 配置 RetryTemplate在 Spring 配置类中定义 RetryTemplate Bean： 3. 使用 RetryTemplate 包装下载逻辑在你的 downloadFile 方法中，使用 RetryTemplate 包装下载逻辑： 4. 关键点说明 重试策略： SimpleRetryPolicy 设置了最大重试次数为 3 次。 如果下载失败，会最多重试 3 次。 重试间隔： FixedBackOffPolicy 设置了每次重试的间隔时间为 2 秒。 异常处理： 如果重试次数用尽仍然失败，会抛出 RuntimeException，并记录错误日志。 RetryTemplate.execute： 第一个参数是重试逻辑（RetryCallback）。 第二个参数是重试失败后的回调（RecoveryCallback）。 5. 测试调用 downloadFile 方法时，如果下载失败，会自动重试 3 次，每次间隔 2 秒。如果重试次数用尽仍然失败，会抛出异常。 6. 日志输出示例 下载成功： 下载失败并重试： 通过以上方法，你可以轻松地为文件下载逻辑添加重试机制，提高系统的健壮性。"},{"title":"azure推送docker镜像","date":"2025-01-08T16:00:00.000Z","url":"/2025/01/09/2025/01/091126/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 先决条件 Azure 容器注册表-在 Azure 订阅中创建容器注册表 docker cli-在本地安装 Docker 登录 安装azure-cli命令行 登录由世纪互联运营的 Microsoft Azure docker login 方式 拉取推送 参考文档 推送和拉取镜像 "},{"title":"使用 Nginx 作为反向代理设置 Navidrome","date":"2025-01-08T16:00:00.000Z","url":"/2025/01/09/2025/01/091537/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用 Nginx 代理 Navidrome 是一个常见的部署场景，可以实现通过自定义域名或路径来访问 Navidrome 服务。以下是具体的步骤： 1. 安装 Navidrome确保 Navidrome 已经安装并运行。如果尚未安装，可以参考使用Navidrome构建个人音乐服务或者官方文档 Navidrome Installation。假设 Navidrome 在服务器的 localhost:4533 运行。 2. 安装并配置 Nginx如果尚未安装 Nginx，可以通过以下方式安装： 安装后，确保 Nginx 服务已启动： 3. 配置 Nginx 作为反向代理配置文件路径 Nginx 的配置文件通常位于 &#x2F;etc&#x2F;nginx&#x2F;sites-available&#x2F; 或 &#x2F;etc&#x2F;nginx&#x2F;nginx.conf。推荐在 sites-available 中创建一个单独的文件以便管理。 创建一个新的配置文件，例如 navidrome.conf： 配置内容 假设你希望通过 music.example.com 访问 Navidrome，以下是一个基本的配置模板： 激活配置 将配置链接到 sites-enabled 目录并重载 Nginx： 4. 配置 HTTPS（可选但强烈推荐） 使用 Let’s Encrypt 获取免费的 SSL 证书： 安装 Certbot： 获取证书并自动配置 Nginx： 证书续期可以通过以下命令测试： 5. 访问 Navidrome现在，您应该能够通过  或  访问您的 Navidrome 实例。 6. 其他配置（可选）• Basic Authentication 如果需要额外保护，可以在 Nginx 配置中添加 Basic Authentication。 • 路径子目录配置 如果希望通过子路径（如 &#x2F;music）访问 Navidrome，可将 location &#x2F; 替换为 location &#x2F;music&#x2F; 并设置 proxy_pass "},{"title":"在 Arch Linux 中切换到 LTS 内核","date":"2025-01-02T16:00:00.000Z","url":"/2025/01/03/2025/01/031227/","tags":[["linux","/tags/linux/"],["ArchLinux","/tags/ArchLinux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在 Arch Linux 中切换到 LTS 内核 1. 安装 LTS 内核Arch Linux 的 LTS 内核包名为 linux-lts，可以直接通过 Pacman 包管理器安装： 注意： 如果你使用的是 NVIDIA 专有驱动或其他与内核相关的模块，也需要同时安装与 LTS 内核匹配的模块，例如 nvidia-lts。 2. 检查是否需要 LTS 内核头文件如果你需要编译第三方模块（如 VirtualBox 或 VMware 驱动），还需要安装 LTS 内核的头文件： 3. 更新启动引导（GRUB）安装 LTS 内核后，更新 GRUB 配置文件以添加 LTS 内核选项： 4. 设置默认启动的内核（可选）如果希望默认使用 LTS 内核，需要调整 GRUB 的启动顺序： 查看当前的 GRUB 启动项： 重启系统，并观察 LTS 内核的启动项编号（从 0 开始）。 编辑 GRUB 配置文件： sudo nano &#x2F;etc&#x2F;default&#x2F;grub 找到以下行，并将 GRUB_DEFAULT 设置为对应的启动项编号： GRUB_DEFAULT&#x3D;0 保存后，重新生成 GRUB 配置： sudo grub-mkconfig -o &#x2F;boot&#x2F;grub&#x2F;grub.cfg 5. 重启系统完成后，重启系统： 启动时，你可以选择 Advanced options for Arch Linux，然后选择 LTS 内核进行启动。 6. 验证运行的内核重启后，使用以下命令确认系统是否运行的是 LTS 内核： 输出类似于以下内容表明你已经切换到 LTS 内核： 6.6.68-1-lts 7. 移除旧内核（可选）如果确认 LTS 内核工作正常，可以选择移除旧的 stable 内核： 注意： 不要在未确认 LTS 内核工作正常的情况下移除旧内核。"},{"title":"按照时间清理文件夹","date":"2025-01-02T16:00:00.000Z","url":"/2025/01/03/2025/01/031228/","tags":[["linux","/tags/linux/"],["file","/tags/file/"]],"categories":[["work","/categories/work/"]],"content":" gist-clean python清理脚本"},{"title":"Linux 文件名超限处理","date":"2025-01-02T16:00:00.000Z","url":"/2025/01/03/2025/01/031229/","tags":[["file","/tags/file/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在大多数常见的 Linux 文件系统（如 ext4、xfs 等）上，单个文件名（不含路径）通常限制在 255 个字节（并非字符）之内。如果尝试创建超过 255 字节长度的文件名，就会遇到“File name too long”等错误。要解决或规避这个问题，可以从以下几个方面入手： 1. 文件名本身缩短&#x2F;重命名 • 最简单直接的方式就是在应用层面或脚本中，提前避免产生过长文件名。 • 如果文件名是自动生成的，可以在生成逻辑里进行截断或编码（如 MD5&#x2F;SHA 等哈希）来保证文件名长度不会过长。 2. 利用目录结构减少文件名长度 • 如果是因为路径 + 文件名整体过长，可以通过将文件放到更深或更浅层级的目录，或者将路径层级拆分&#x2F;简化，从而缩短实际文件名在该目录下的长度。 • 某些情况下，也可以使用**软链接（Symbolic Link）**来将长路径“分割”，从而让实际操作的路径变短。 3. 更换文件系统（不常用，且有兼容性风险） 大多数主流文件系统（ext4、XFS、Btrfs 等）依然使用 255 字节的文件名限制。如果确实有非常特殊的需求，需要更长的文件名，可以考虑以下思路： 网络文件系统：例如一些分布式&#x2F;网络文件系统（NFS、CIFS 等）可能在协议层面对文件名长度有不同要求，但这通常依赖服务端&#x2F;客户端的实现及协议版本，不一定能无限增加长度。 特定的 FUSE 文件系统或特殊文件系统：例如部分 FUSE 文件系统可能实现了更复杂的文件名映射机制。但是这类方案往往不够通用，且可能对性能、兼容性有影响。 4. 使用压缩包&#x2F;归档等方式进行存储 如果只是需要以某种形式保存数据，而不是直接以超长文件名形式对外暴露，可以考虑： • 将超长文件名作为一种“元数据”，附加在文件内部或另存为记录； • 打包&#x2F;压缩文件，不再直接以超长文件名形式放在文件系统中； • 使用数据库或对象存储（S3、OSS 等），将文件内容以对象方式保存，把原本的“超长文件名”当作对象关键字或数据库字段存储。 5. 软件层面对文件名做特殊映射 有时需要兼容某些应用程序内部对超长文件名的处理，可以考虑： • 在应用层，自动将文件名与一个唯一标识（UUID&#x2F;哈希）做映射； • 在文件系统中落地时只存储短文件名（如哈希），再在数据库&#x2F;元数据中保存实际的完整名字。 • 访问时，先根据短文件名或者哈希查找映射关系，再显示或恢复出完整名字给用户或程序使用。 小结 • Linux 上常见的本地文件系统（ext4 等）本身对单个文件名的限制是 255 字节，无法直接突破。 • 最简单的做法是减少文件名长度，或通过软链接、变更目录层级、改用数据库&#x2F;对象存储等方式来绕过限制。 • 如果对超长文件名有非常特殊的、不可避免的需求，需考虑更换或开发特殊的文件系统或存储方案，但这往往会带来兼容性和管理的复杂度。 综上，除非有极为特殊的场景，大多数情况下都会通过限制或优化文件命名的方式来解决超长文件名的问题。"},{"title":"2024总结","date":"2024-12-30T16:00:00.000Z","url":"/2024/12/31/2024/12/311020/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":"2024年即将结束，这一年对我而言充满了挑战与收获，也让我对生活和工作的平衡有了更深刻的体会。 一、工作总结 加入新项目组 年初加入了新的项目组，项目进展紧张，工期压力大，基本每周都处于加班的状态。在这种高强度的工作环境下，我逐渐提升了对任务优先级的判断能力和解决问题的效率。尽管加班较多，但我尽力保证了工作的质量和进度，为项目的推进贡献了自己的力量。 面对公司的变动 在这一年里，公司经历了搬迁、上市、裁员等多重变化，整个环境充满了不确定性。同事的离职对团队也产生了一定影响，但我始终保持着乐观和适应的心态，努力稳定团队士气，并积极承担更多的责任。 在变化中学会了拥抱挑战，提升了自己对环境快速适应的能力。 专业技能提升 尽管工作忙碌，我依然抓住一些零碎时间，学习了与项目相关的新技术，为团队引入了一些解决方案，优化了开发流程。虽然自我感觉学习的时间还不够多，但这些提升让我更从容地应对工作中的挑战。 二、生活总结 家庭陪伴 因为加班较多，陪伴家人的时间较为有限。这一点让我深感遗憾，尤其是宝宝的成长快一年了，我错过了许多值得铭记的瞬间。 尽管如此，在节假日和周末，我依然尽量抽出时间，陪伴家人，参与宝宝的日常生活，见证了他从牙牙学语到蹒跚学步的点滴成长，这也是我今年最大的幸福来源。 个人生活 由于工作繁忙和频繁的变化，今年没能坚持健身，体重持续增加，身体状态不如以往，这也让我更加意识到健康管理的重要性。6月份回家探望了家人，也和朋友一起去了崇明岛，这些片段让我感受到短暂的放松与生活的美好。 三、个人成长与反思 关于自我提升 忙碌的工作让我一度陷入机械的状态，思考和自我提升的时间被大幅压缩。缺乏锻炼也让我精力不足，效率下降。这些问题值得我在来年认真反思并改善。 关于生活的平衡 这一年，我深刻体会到生活和工作的平衡有多重要。再忙的工作也不能忽视陪伴家人，忽视自己的身体健康。宝宝的成长只有一次，我需要用心去参与，而不仅仅是旁观。 四、2025年的展望 重新拾起健身 健康是工作和生活的基石，2025年计划重新制定健身目标，合理规划时间，每周至少锻炼三次，逐步恢复体能和健康。 学习与成长 工作中将继续学习新技术，并尝试参与更具有挑战性的项目；同时，每月阅读一本书或参加线上课程，补充自己的知识体系。 陪伴家人 在忙碌的工作中抽出更多时间陪伴家人，记录宝宝的成长，与家人一起创造更多美好的回忆。 生活质量提升 尝试用更有效的时间管理方式，提高工作效率，为自己的兴趣爱好腾出时间，增加生活的丰富性和幸福感。 结语2024年是充满变化和挑战的一年，同时也让我更加明确了自己生活中的优先级。面对过去忙碌而略显遗憾的一年，我将在2025年做出调整，不仅要在工作中继续精进，还要让生活更有温度、让自己更加从容地面对每一天。 新的年轮即将开启，愿未来更加精彩！"},{"title":"docker使用rclone挂载S3","date":"2024-12-23T16:00:00.000Z","url":"/2024/12/24/2024/12/241018/","tags":[["docker","/tags/docker/"],["s3","/tags/s3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"docker-compose 注意其他需要挂载当前目录的容器,需要同样的配置 "},{"title":"挂载对象存储为本地磁盘","date":"2024-12-22T16:00:00.000Z","url":"/2024/12/23/2024/12/231427/","tags":[["AWS","/tags/AWS/"],["s3","/tags/s3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"s3fs挂载 安装 设置 AWS 凭据 修改权限 挂载 S3 存储桶 rclone挂载(Linux) 安装 配置 S3 存储桶 • 选择 n 创建新配置。 • 提供一个名称（如 my-s3）。 • 选择 4 (Amazon S3)。 • 填写 AWS Access Key ID 和 AWS Secret Access Key。 • 根据需要选择存储桶区域和其他设置。 rclone 常用操作 rclone syncrclone sync 命令显示进度条。默认情况下，rclone 在终端运行时会显示实时的同步进度信息，包括文件传输的进度条。 确保输出中有进度条： • 添加 -P 或 –progress 参数可以确保强制显示进度条，即使某些环境中默认没有进度。 • 输出示例： 静默模式（无进度条）： 如果不想看到任何进度条，可以使用 –quiet（或 -q）参数。 设置自动挂载（可选）为确保系统重启后挂载自动生效，可以设置 rclone 挂载服务。 使用 Systemd 创建服务： 创建 Systemd 服务文件： 添加以下内容 启用并启动服务 Win下挂载 安装winfsp 安装时需要开启dev模式github 安装rcloneZ daemon moderclone 在 Windows 平台上不支持守护进程模式（daemon mode），因此当你尝试在 Windows 上使用 rclone mount 命令时，会报错： 这是因为 rclone mount 在 Windows 上默认会尝试以守护进程模式运行，而 Windows 不支持这种模式。 解决方法1. 使用 --no-console 参数在 Windows 上运行 rclone mount 时，可以通过添加 --no-console 参数来避免守护进程模式。这个参数会阻止 rclone 尝试以守护进程模式运行。 remote:path：你的远程存储配置和路径。 X:：你想要挂载到的本地驱动器字母（例如 X:）。 --no-console：避免守护进程模式。 2. 使用 --vfs-cache-mode 参数如果你需要缓存功能，可以添加 --vfs-cache-mode 参数。例如： --vfs-cache-mode full：启用完整的缓存模式，适合频繁读写的场景。 3. 使用 --volname 参数（可选）你可以为挂载的驱动器指定一个名称，方便识别： --volname &quot;MyCloudDrive&quot;：将挂载的驱动器命名为 MyCloudDrive。 4. 以普通模式运行如果你不需要守护进程模式，可以直接运行 rclone mount，但需要保持命令行窗口打开，否则挂载会终止。 示例命令假设你的远程存储配置名为 mydrive，路径为 Documents，你想将其挂载到本地驱动器 X:，可以使用以下命令： 注意事项 保持命令行窗口打开： 在 Windows 上运行 rclone mount 时，必须保持命令行窗口打开。如果关闭窗口，挂载会终止。 使用任务计划程序（Task Scheduler）： 如果你希望 rclone mount 在后台运行，可以使用 Windows 的任务计划程序来启动挂载命令。 使用第三方工具： 你可以使用第三方工具（如 nssm）将 rclone mount 包装为 Windows 服务，从而实现后台运行。 使用 nssm 将 rclone mount 包装为服务如果你希望 rclone mount 在后台运行，可以按照以下步骤操作： 1. 下载 nssm从 nssm 官网 下载并解压 nssm。 2. 创建服务打开命令提示符（以管理员身份运行），然后执行以下命令： 管理员身份 配置文件需要复制到 C:\\Windows\\System32\\config\\systemprofile\\AppData\\Roaming\\rclone\\rclone.conf 在弹出的窗口中配置服务： Path：rclone.exe 的完整路径（例如 C:\\rclone\\rclone.exe）。 Startup directory：rclone.exe 所在的目录（例如 C:\\rclone）。 Arguments：mount mydrive:Documents X: --no-console --vfs-cache-mode full。 点击 Install service 完成服务创建。 3. 启动服务在命令提示符中运行以下命令启动服务： 总结 在 Windows 上使用 rclone mount 时，添加 --no-console 参数以避免守护进程模式。 如果需要后台运行，可以使用任务计划程序或 nssm 将 rclone mount 包装为服务。 保持命令行窗口打开，否则挂载会终止。 通过以上方法，你可以在 Windows 上成功挂载远程存储。"},{"title":"docker: 'buildx' is not a docker command","date":"2024-12-20T16:00:00.000Z","url":"/2024/12/21/2024/12/211314/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":"背景idea连接docker进行构建x构建镜像时，报了错： ERROR: BuildKit is enabled but the buildx component is missing or broken. Install the buildx component to build images with BuildKit:  然后执行 报错:docker: ‘buildx’ is not a docker command 当 docker 报错 buildx is not a docker command 时，通常意味着你的 Docker CLI 没有安装或未正确配置 buildx 插件。以下是解决该问题的步骤： 检查 Docker 版本 docker-buildx 是 Docker 的扩展功能，要求 Docker 的版本是 19.03 或更高。 运行以下命令检查版本： docker version 如果 Docker 版本过低，请升级到最新版本。 升级 Docker（Mac） 1. 打开 Docker 官方网站。 2. 下载并安装最新版本的 Docker Desktop。 3. 安装完成后，重启终端并运行： docker version 检查 Buildx 是否已安装 docker-buildx 是一个 CLI 插件，通常在 Docker Desktop（Mac）中自带。如果没有安装或配置错误，可以通过以下方式安装。 验证 Buildx 是否已存在 运行以下命令： docker buildx version 安装 Buildx github-buildx 手动安装 Buildx 插件 1. 创建插件目录： mkdir -p ~&#x2F;.docker&#x2F;cli-plugins 配置 Buildx 如果 Buildx 已安装，但还未配置 builder，需要初始化它。 1. 创建 Builder 如果看到类似以下内容，说明 Builder 配置成功： 启用 BuildKit（可选） Buildx 是基于 Docker BuildKit 的，需要确保 BuildKit 已启用。 启用 BuildKit 运行以下命令： 为了永久生效，可以将该环境变量写入 ~&#x2F;.zshrc 或 ~&#x2F;.bashrc： 测试 Buildx 运行以下命令测试： 如果 Buildx 工作正常，你将能够构建多平台镜像。 常见问题及解决方法 问题 1：docker buildx 命令仍然不可用 • 确保插件安装在正确的位置：~&#x2F;.docker&#x2F;cli-plugins&#x2F;。 • 确保文件权限正确： 问题 2：Docker CLI 没有识别插件 • 如果 CLI 未正确加载插件，请重启终端或重启 Docker Desktop。 • 检查环境变量是否设置： 确保路径中包含 Docker 的安装目录（如 &#x2F;usr&#x2F;local&#x2F;bin）。 问题 3：Builder 状态为 Inactive • 运行以下命令启动 Builder： 参考资料 docker-buildx "},{"title":"使用 FFmpeg 从视频中截图","date":"2024-12-19T16:00:00.000Z","url":"/2024/12/20/2024/12/201432/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用 FFmpeg 从视频中截图，可以通过简单的命令实现。这些步骤假设你已经安装了 FFmpeg。 基本命令 以下是一个从视频中截图的基本命令： 参数说明： • -i input.mp4：输入的视频文件。 • -ss 00:00:05：截图的时间点（这里为 5 秒）。 • -vframes 1：提取一帧。 • output.jpg：输出的截图文件名。 高质量截图 如果需要更高质量的截图，可以添加以下参数： • -q:v 2：控制输出质量，值越低质量越高（范围 2-31）。 批量截图 如果需要从视频中每隔一段时间截取一张图片： 参数说明： • -vf “fps&#x3D;1”：每秒提取 1 帧。 • output_%03d.jpg：输出文件名，%03d 表示编号，例如 output_001.jpg，output_002.jpg。 你可以根据需求调整 fps 的值。 指定分辨率截图 如果需要调整截图分辨率，可以添加 -s 参数： 参数说明： • -s 1920x1080：指定输出图片的分辨率为 1920x1080。 带水印截图 如果你需要在截图中加入水印： 参数说明： • -i watermark.png：水印图片。 • overlay&#x3D;10:10：水印的位置，左上角距边缘 10px。"},{"title":"使用证书登陆docker","date":"2024-12-15T16:00:00.000Z","url":"/2024/12/16/2024/12/161725/","tags":[["docker","/tags/docker/"],["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在 Docker 中使用证书进行身份验证和加密通信，可以显著提高 Docker 环境的安全性。通过配置 TLS（传输层安全性）证书，您可以确保 Docker 客户端与 Docker 守护进程（Daemon）之间的通信是加密的，并且只有经过授权的客户端才能访问 Docker 守护进程。 一、概述Docker 使用 TLS 证书来实现以下安全功能： 加密通信：确保 Docker 客户端与 Docker 守护进程之间的通信是加密的，防止中间人攻击。 身份验证：确保只有拥有有效证书的客户端可以访问 Docker 守护进程。 授权：基于证书的身份验证，可以进一步细化权限控制。 二、准备工作1. 安装必要工具确保您的系统已安装以下工具： OpenSSL：用于生成和管理 TLS 证书。 Docker：确保已安装并运行最新版本的 Docker。 2. 创建目录结构为管理证书和密钥，建议创建一个专用的目录： 三、生成 TLS 证书1. 生成 CA（证书颁发机构）证书CA 证书用于签发和验证其他证书。 在生成 CA 证书时，系统会提示您输入信息，如国家、组织等。 2. 生成 Docker 守护进程（服务器）证书 注意： 将 your-server-domain 替换为您的服务器域名（例如 docker.example.com）。 将 your-server-ip 替换为服务器的实际 IP 地址。 3. 生成 Docker 客户端证书 4. 设置权限确保私钥文件的权限正确，以防止未经授权的访问： 四、配置 Docker 守护进程使用 TLS1. 移动证书文件将生成的证书和密钥文件移动到 Docker 守护进程可以访问的位置，例如 /etc/docker/certs： 2. 配置 Docker 守护进程编辑 Docker 守护进程的启动配置文件，通常位于 /etc/docker/daemon.json。如果该文件不存在，可以创建它。 添加以下内容： 说明： tlsverify: 启用 TLS 验证。 tlscacert: CA 证书路径。 tlscert: 服务器证书路径。 tlskey: 服务器私钥路径。 hosts: Docker 守护进程监听的地址。这里设置为监听所有网络接口的 2376 端口和本地的 Unix 套接字。 3. 重启 Docker 服务应用配置更改，重启 Docker 服务： 五、配置 Docker 客户端使用 TLS 证书1. 移动客户端证书文件将客户端证书和密钥文件（ca.pem、cert.pem、key.pem）移动到客户端机器的某个目录，例如 ~/.docker： 2. 设置环境变量在客户端机器上，设置以下环境变量以使用 TLS 连接到 Docker 守护进程： 说明： DOCKER_HOST: Docker 守护进程的地址，使用 tcp 协议和 2376 端口。 DOCKER_TLS_VERIFY: 启用 TLS 验证。 DOCKER_CERT_PATH: 客户端证书和密钥所在的路径。 3. 验证连接使用以下命令验证客户端是否能成功连接到 Docker 守护进程： 如果配置正确，您应该能看到客户端和服务器的 Docker 版本信息，而不会出现连接错误。 六、使用 Docker 配置文件自动加载证书为了简化每次使用 Docker 命令时设置环境变量的过程，您可以在客户端机器上配置 Docker 配置文件，使其自动加载 TLS 证书。 1. 创建或编辑 ~/.docker/config.json 添加以下内容： 注意： 将 /home/your-username/.docker/ 替换为实际的路径。 将 your-server-domain 替换为 Docker 守护进程所在服务器的域名或 IP 地址。 2. 使用 Docker 命令现在，您可以直接使用 Docker 命令，而无需每次手动设置环境变量： 七、常见问题及解决方法1. 证书无效或被拒绝症状： 错误信息类似于 x509: certificate signed by unknown authority 或 certificate is not valid for host. 解决方法： 确保 Docker 守护进程和客户端使用的是同一个 CA 签发的证书。 检查 subjectAltName 是否正确配置，确保服务器域名和 IP 地址包含在证书的 subjectAltName 中。 2. Docker 守护进程未启动解决方法： 检查 Docker 服务状态： 查看 Docker 日志以获取更多信息： 3. 权限问题症状： 无法访问证书文件，提示权限不足。 解决方法： 确保证书文件的权限正确： 确保当前用户有权访问证书文件所在的目录。 4. 防火墙阻止端口症状： 客户端无法连接到 Docker 守护进程，可能提示超时或连接被拒绝。 解决方法： 检查服务器防火墙设置，确保 2376 端口开放： 如果使用其他防火墙工具，如 iptables，相应地添加规则。 5. 使用不同主机名导致证书不匹配症状： 证书的主机名与连接的主机名不匹配，导致验证失败。 解决方法： 确保在生成服务器证书时，CN 和 subjectAltName 包含正确的主机名和 IP 地址。 八、示例：完整流程以下是一个完整的示例流程，假设服务器域名为 docker.example.com(没有域名都使用IP)，IP 地址为 192.168.1.100。 1. 在服务器上生成 CA、服务器和客户端证书 2. 配置 Docker 守护进程 添加以下内容： 3. 重启 Docker 服务 4. 在客户端配置证书将 ca.pem、cert.pem 和 key.pem 复制到客户端机器的 ~/.docker 目录： 5. 在客户端设置环境变量 6. 验证连接 您应该能够看到 Docker 客户端和服务器的版本信息，表明连接成功。 idea远程连接docker 配置url(注意https)以及证书所在文件夹 buildx参考:buildx 通过 curl 验证 Docker 连接 九、总结通过上述步骤，您可以为 Docker 环境配置 TLS 证书，实现安全的客户端和服务器通信。以下是关键要点： 生成和管理证书：使用 OpenSSL 生成 CA、服务器和客户端证书，确保证书的安全性和有效性。 配置 Docker 守护进程：设置 Docker 守护进程使用 TLS 证书进行加密通信和身份验证。 配置 Docker 客户端：确保客户端正确配置环境变量，以使用生成的证书安全连接到 Docker 守护进程。 安全实践：妥善管理和保护证书和密钥文件，限制访问权限，定期更新和轮换证书。 "},{"title":"使用Navidrome构建个人音乐服务","date":"2024-12-12T01:09:09.000Z","url":"/2024/12/12/2024/12/121212/","tags":[["musix","/tags/musix/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用Navidrome构建个人音乐服务 前言Navidrome是一个现代化的音乐服务器和流媒体解决方案，它可以让你在任何地方访问和播放你的音乐收藏。本文将详细介绍如何使用Docker部署Navidrome，并结合music-tag和dufs工具构建一个完整的个人音乐服务。 系统要求硬件要求 CPU：双核及以上 内存：最低1GB，推荐2GB以上 存储：取决于音乐库大小，建议预留足够空间 软件要求 操作系统：Linux&#x2F;Windows&#x2F;macOS Docker版本 &gt;&#x3D; 20.10 Docker Compose版本 &gt;&#x3D; 2.0 网络：开放相应端口（4533&#x2F;8001&#x2F;8002） 安装前准备 首先，在服务器上安装docker： 安装docker-compose： 创建项目目录： 配置docker-compose.yaml文件 启动容器：docker-compose up -d 背景随着音乐收藏的增加，需要一个功能完善的音乐服务器来统一管理和播放音乐，主要解决以下问题： 音乐文件的集中管理和备份 支持多设备访问和播放 音乐元数据（标签、封面等）的管理 支持多种音频格式 目标搭建一个功能完善的音乐服务器，实现： 集中化音乐管理 支持多设备访问 完善的音乐标签管理 便捷的音乐上传功能 高质量音频播放体验 解题思路 使用docker部署，确保环境一致性 实现音乐文件的统一管理 提供音乐标签编辑功能 支持便捷的音乐上传 支持各平台客户端访问 方案设计整体架构 Navidrome作为核心音乐服务器 Music-tag负责音乐元数据管理 Dufs提供文件上传服务 共享音乐文件夹实现统一管理 数据流设计 通过Dufs上传音乐文件 使用Music-tag处理音乐元数据 Navidrome自动扫描并提供播放服务 技术架构 Docker容器化部署 Docker Compose实现服务编排 Navidrome提供音乐服务 Music-tag处理音乐标签、歌词和封面 Dufs提供Web文件上传下载，也可选择SFTP或Rclone 核心代码Docker Compose配置 配置说明Navidrome配置项 ND_SCANSCHEDULE：音乐库扫描间隔 ND_LOGLEVEL：日志级别设置 ND_SESSIONTIMEOUT：用户会话超时时间 ND_BASEURL：反向代理基础URL 文件权限设置 效果展示Navidrome界面 Music-tag标签编辑 Dufs文件管理 Feishin客户端 使用步骤 访问Navidrome 打开浏览器，访问 首次访问需要创建管理员账号 配置Navidrome 登录后进入设置页面 配置音乐库扫描选项 设置转码参数（可选） 上传音乐 访问打开Dufs 上传音乐文件到music目录 管理音乐标签 访问打开Music-tag 默认账号密码：admin/admin 编辑音乐文件的标签信息 使用Navidrome 等待音乐库扫描完成 开始享受音乐！ 进阶配置反向代理设置使用Nginx配置示例： 性能优化 音乐库优化 使用SSD存储 定期清理缓存 优化文件结构 系统优化 调整系统打开文件数限制 配置合适的Java内存 常见问题 音乐文件不显示 检查文件权限 确认文件格式支持 手动触发扫描 播放出现卡顿 检查网络带宽 调整转码设置 优化缓存配置 标签编辑失败 验证文件权限 检查磁盘空间 确认文件未被锁定 参考附录 Navidrome官方文档 Feishin客户端 Music-tag工具 Dufs文件服务 更新日志 2024-12-12：初始版本发布 2024-12-13：补充安装细节和常见问题解决方案 "},{"title":"构建一个基于Amazon S3存储的音乐播放器","date":"2024-12-05T16:00:00.000Z","url":"/2024/12/06/2024/12/061408/","tags":[["music","/tags/music/"]],"categories":[["Linux","/categories/Linux/"]],"content":"将分别介绍如何使用 Vue.js 和 Spring Boot 构建一个基于Amazon S3存储的音乐播放器，以及如何使用 Rust 构建一个命令行式的播放程序。您可以根据项目需求和个人偏好选择合适的技术栈。 选项一：使用 Vue.js 和 Spring Boot 构建音乐播放器一、项目总体结构一个典型的 Vue.js 前端和 Spring Boot 后端的项目结构如下： 二、版本控制1. 初始化Git仓库在项目根目录下初始化Git仓库： 2. 创建 .gitignore在根目录下创建一个 .gitignore 文件，添加以下内容以忽略不必要的文件： 三、前端开发框架搭建（Vue.js）假设选择 Vue.js 作为前端框架，并使用 Vuetify 作为UI组件库。 1. 初始化Vue项目使用 Vue CLI 初始化项目： 选择默认配置或根据需要自定义配置。 2. 安装必要的依赖进入 frontend 目录并安装Vuetify和AWS SDK： 3. 设置项目结构在 frontend/src 下创建以下文件夹： 4. 配置环境变量在 frontend 目录下创建 .env 文件，添加AWS相关配置： 注意：不要在前端直接暴露AWS密钥。推荐使用后端代理来处理S3访问，或者使用预签名URL。 5. 创建上传组件在 frontend/src/components/Upload.vue 中创建上传组件： 6. 创建音乐列表组件在 frontend/src/components/MusicList.vue 中创建音乐列表组件： 7. 配置路由在 frontend/src/router/index.js 中配置路由： 8. 集成到主应用在 frontend/src/App.vue 中集成导航： 在 frontend/src/main.js 中引入路由： 9. 配置代理解决跨域问题在 frontend/vue.config.js 中配置代理： 四、后端开发框架搭建（Spring Boot）1. 初始化Spring Boot项目您可以使用 Spring Initializr 初始化项目，选择以下依赖： Spring Web Spring Security Spring Data JPA AWS SDK (需要手动添加) PostgreSQL Driver (或其他数据库驱动) Lombok (可选) 下载并解压项目，导入到您的IDE（如 IntelliJ IDEA）。 2. 配置 pom.xml在 pom.xml 中添加AWS SDK依赖： 3. 配置应用属性在 src/main/resources/application.properties 中配置数据库和AWS信息： 注意：为了安全，建议使用环境变量或AWS IAM角色管理凭证，避免在配置文件中明文存储密钥。 4. 创建S3服务创建一个服务类来处理与S3的交互。 5. 创建控制器创建控制器来处理上传和获取音乐列表的请求。 6. 配置CORS确保Spring Boot允许来自前端的跨域请求。在 src/main/java/com/example/musicplayer/config/WebConfig.java 中配置： 7. 数据库配置（可选）如果需要存储用户信息、播放列表等数据，可以配置数据库并使用JPA进行管理。这里假设使用 PostgreSQL。 a. 配置数据库连接在 application.properties 中已经配置了数据库连接参数。 b. 创建实体类例如，创建 User 实体： c. 创建仓库接口 d. 同步数据库确保在应用启动时，JPA自动创建或更新表结构： 8. 用户认证与授权可以使用 Spring Security 实现用户认证和授权，或者集成 Amazon Cognito。这里以 Spring Security + JWT 为例。 a. 添加依赖在 pom.xml 中添加Spring Security和JWT相关依赖： b. 配置Spring Security创建一个配置类 SecurityConfig： c. 创建认证控制器创建一个简单的认证控制器进行用户注册和登录。 d. 配置密码编码器在 SecurityConfig 中添加密码编码器 Bean： e. 实现JWT生成与验证需要实现JWT生成和验证逻辑，包括过滤器、JWT工具类等。具体实现可以参考Spring Security与JWT的集成教程，这里略过详细代码。 五、部署与运维1. 使用Docker容器化应用a. 创建Dockerfile后端Dockerfile (backend/Dockerfile): 前端Dockerfile (frontend/Dockerfile): b. 创建 docker-compose.yml在项目根目录下创建 docker-compose.yml 文件： c. 构建和启动容器在项目根目录下运行： 2. 部署到AWS可以使用 AWS Elastic Beanstalk、Amazon ECS 或 AWS Fargate 部署容器化应用。以下以 Elastic Beanstalk 为例： a. 安装AWS CLI和Elastic Beanstalk CLI b. 配置AWS CLI 输入您的AWS访问密钥、秘密密钥、区域和输出格式。 c. 初始化Elastic Beanstalk进入 backend 目录并初始化： d. 创建环境并部署 类似地，可以为前端创建Elastic Beanstalk环境，或将前后端整合到同一个应用中。 六、安全与合规1. 数据加密确保S3存储桶启用了服务器端加密（SSE）： 在S3存储桶设置中，启用默认的SSE（如AES-256或使用KMS）。 配置Spring Boot中的S3客户端以使用加密选项。 2. 网络安全 使用 VPC 和 安全组 控制数据库和后端的访问权限。 配置 HTTPS 以加密传输数据。可以使用 AWS Certificate Manager 申请SSL证书，并在Elastic Beanstalk或CloudFront中配置。 3. 监控与日志使用 AWS CloudWatch 监控应用性能和安全事件。 配置CloudWatch Logs来收集后端和前端的日志。 设置CloudWatch Alarms以监控关键指标，如CPU使用率、内存使用等。 七、总结使用 Vue.js 和 Spring Boot 构建一个基于Amazon S3存储的音乐播放器，可以充分利用这两种技术的优势。Vue.js 提供了灵活的前端开发体验，Spring Boot 则提供了强大的后端服务能力。通过合理的项目结构、容器化部署和安全配置，您可以构建一个高效、可扩展且安全的音乐播放器。 如果在开发过程中遇到具体问题，欢迎随时提问！ 选项二：使用 Rust 构建命令行式的音乐播放程序如果您希望构建一个命令行式的音乐播放器，可以使用 Rust 语言来实现。Rust 拥有高性能和安全性，非常适合构建高效的CLI应用。以下是详细的步骤和技术选型建议。 一、项目初始化1. 安装Rust确保您已经安装了Rust工具链。如果还没有安装，可以使用 rustup 安装： 安装完成后，重新启动终端并验证安装： 2. 创建新项目使用Cargo（Rust的包管理工具）创建新项目： 二、技术选型 HTTP 客户端：reqwest 用于与S3进行通信。 JSON 解析：serde 和 serde_json 用于处理JSON数据。 CLI 框架：clap 或 structopt 用于构建命令行界面。 异步编程：tokio 提供异步运行时。 音频播放：rodio 用于播放音频。 AWS SDK：aws-sdk-s3（目前Rust的AWS SDK在发展中，可考虑使用其他库或通过HTTP API进行S3交互）。 三、添加依赖编辑 Cargo.toml 文件，添加所需依赖： 四、配置环境变量使用 .env 文件存储AWS凭证和配置。 在项目根目录下创建 .env 文件： 安装 dotenv 以加载环境变量。 五、实现功能1. 构建命令行接口使用 clap 定义CLI命令和选项。 2. 实现上传功能创建一个模块来处理S3上传。 在 main.rs 中引入 s3 模块： 3. 实现列出音乐文件 在 main.rs 中引入 list_music 函数。 4. 实现播放功能使用 rodio 播放音频。 在 main.rs 中引入 play_music 函数。 四、运行程序确保后端服务已启动并可访问。 在项目根目录下运行： 五、优化与增强1. 错误处理为所有异步操作添加更完善的错误处理，避免程序崩溃。 2. 配置管理使用 dotenv 加载环境变量，增强安全性和灵活性。 3. 并发与性能使用 tokio 提高网络请求和音频处理的并发性能。 4. 音频格式支持确保 rodio 支持您需要播放的音频格式，必要时引入额外的编解码器。 六、部署与分发1. 构建二进制文件使用Cargo构建适用于目标平台的二进制文件。 生成的可执行文件位于 target/release/ 目录下。 2. 打包与分发根据目标平台，打包二进制文件或创建安装脚本，方便用户下载和使用。 七、安全与合规1. 凭证管理避免在代码中硬编码AWS凭证，使用环境变量或AWS IAM角色管理凭证。 2. 数据加密确保上传到S3的文件启用了服务器端加密（SSE）。 3. 网络安全确保与后端通信时使用HTTPS，保护数据传输的安全性。 八、总结使用 Rust 构建命令行式的音乐播放器，可以实现高效、可靠的音乐播放功能。通过与Spring Boot后端的协作，CLI程序可以安全地与Amazon S3交互，上传、列出和播放音乐文件。Rust的性能和安全性使其成为构建此类应用的理想选择。 如果您需要更详细的实现细节或在开发过程中遇到问题，欢迎随时提问！"},{"title":"使用 Steam 的双重验证","date":"2024-12-04T16:00:00.000Z","url":"/2024/12/05/2024/12/051438/","tags":[["game","/tags/game/"]],"categories":[["congco","/categories/congco/"]],"content":"要在第三方双重验证器中使用 Steam 的双重验证（如 Google Authenticator 等），您需要获取 Steam 帐号的共享密钥（shared secret）和身份密钥（identity secret）。以下是获取这些密钥的常见方法： 方法一：使用 WinAuth WinAuth 是一款适用于 Windows 的开源双重验证工具，支持 Steam 等多种服务。通过 WinAuth，您可以在 PC 上管理 Steam 的双重验证。具体步骤如下： 下载并安装 WinAuth：前往 WinAuth 的 GitHub 页面 下载最新版本的 WinAuth。 添加 Steam 帐号： 打开 WinAuth，点击“Add”，选择“Steam”。 输入您的 Steam 帐号名称和密码，然后点击“Verify Authenticator”。 按照提示输入从 Steam 收到的验证码。 获取共享密钥和身份密钥： 在添加帐号的过程中，WinAuth 会生成并显示您的共享密钥（shared secret）和身份密钥（identity secret）。 请妥善保存这些密钥，以便在其他双重验证器中使用。 方法二：使用 Steam Desktop Authenticator（SDA） Steam Desktop Authenticator 是另一款用于管理 Steam 双重验证的工具。通过 SDA，您可以提取共享密钥和身份密钥。具体步骤如下： 下载并安装 SDA：前往 Steam Desktop Authenticator 的 GitHub 页面 下载最新版本。 设置您的 Steam 帐号： 打开 SDA，点击“Setup new account”。 输入您的 Steam 帐号信息，并按照提示完成设置。 导出密钥： 在 SDA 中，选择您的帐号，点击“Manage Encryption”，然后选择“Remove Encryption”。 在 SDA 的主界面，右键点击您的帐号，选择“Export Account”。 保存导出的 maFile（JSON 格式），其中包含 shared_secret 和 identity_secret。 注意事项： 安全性：提取并使用这些密钥存在一定风险，可能影响帐号安全。请确保从官方或可信来源下载工具，并妥善保管您的密钥。 官方支持：Steam 官方并不支持在第三方验证器中使用其双重验证。使用非官方工具可能违反 Steam 的服务条款，导致帐号风险。 备份：在进行任何操作前，务必备份您的密钥和相关数据，以防丢失。 请谨慎操作，确保您的帐号安全。"},{"title":"Using GraalVM and Native Image on Windows","date":"2024-12-04T16:00:00.000Z","url":"/2024/12/05/2024/12/051439/","tags":[["java","/tags/java/"],["GraalVM","/tags/GraalVM/"],["windows","/tags/windows/"]],"categories":[["Java","/categories/Java/"]],"content":"合著者：Oracle 的 Scott Seighman 注意：从适用于 JDK 17 &#x2F; 20 的 GraalVM 开始，该工具会查找 Visual Studio 安装，并（如果找到）自动设置您的构建环境。因此，您可以在命令提示符或 PowerShell 中运行，而无需任何其他安装。*native-image*``*native-image* 此博客文章已更新为与适用于 JDK 21 及更高版本的 GraalVM 兼容。网站上提供了旧版 GraalVM 的安装说明。 在 Windows 上使用 GraalVM 和 Native Image 本文将帮助任何使用 Windows 的人安装高性能 GraalVM JDK 及其 Native Image 工具。 我们已经看到越来越多的 Windows 用户表示对使用 GraalVM Native Image 感兴趣，GraalVM Native Image 是一种预先 （AOT） 编译工具，可将 Java 字节码打包到独立的可执行文件中。此可执行文件是本机应用程序，与用户系统上的任何其他应用程序一样，通常可以实现更快的启动时间，同时占用的空间更小。 在 Windows 上安装 GraalVM 既快速又简单。使用 Native Image 只有几个先决条件。让我们一步一步来。 第 1 部分：安装 GraalVM 转到 graalvm.org&#x2F;downloads，选择 Java 版本并下载。 将 directory 更改为要安装 GraalVM 的位置，然后将 .zip 文件移动到该位置。 解压缩文件。 接下来，您应该配置运行时环境，请注意计算机上可能安装了多个 JDK。打开 Windows 命令提示符并执行以下操作： - 将环境变量的值设置为 GraalVM bin 目录： 将环境变量的值设置为安装目录：PATH``setx /M PATH “C:\\Progra~1\\Java\\&lt;graalvm install dir&gt;\\bin;%PATH%”``JAVA_HOME setx /M JAVA_HOME “C:\\Progra~1\\Java\\&lt;graalvm install dir&gt;” 要验证 GraalVM 是否安装成功，请重新启动命令提示符并执行 。java -version 原生映像随 GraalVM 一起提供，但在 Windows 上，它需要 **Visual Studio 2022 版本 17.1.0 或更高版本，**以及 **Microsoft Visual C++ （MSVC）。**有两种安装选项： 使用 Windows 11 SDK 安装 Visual Studio 生成工具 使用 Windows 11 SDK 安装 Visual Studio 第 2 部分：安装 Visual Studio 构建工具和 Windows 11 SDKVisual Studio的 从 visualstudio.microsoft.com 下载 Visual Studio Build Tools 2022（C 开发环境）版本 17.1.0 或更高版本。 通过单击_.exe_文件开始 Visual Studio Build Tools 安装，然后单击 Continue： 开始安装 Visual Studio Build Tools 在主窗口中选中使用 C++ 进行桌面开发复选框。此外，在右侧的 安装详细信息 下，选择 **Windows 11 SDK，**然后单击 安装. 使用 C++ 和 Windows 11 SDK 进行桌面开发 安装完成后，重新启动系统。 Windows 11 开发工具包：接下来，如果您安装了 Visual Studio 2022，则需要确保 Windows 11 SDK 也可用： 打开 Visual Studio 安装程序： Visual Studio 安装程序 在已安装选项卡下，单击修改，然后选择单个组件： Visual Studio 安装程序 然后滚动到底部，检查是否安装了 Windows 11 SDK，并确认已安装构建工具。 现在，您可以开始使用 GraalVM Native Image。 第 3 部分。开始在 Windows 上使用 Native Image如果 Native Image 可以在已知位置找到合适的 Visual Studio 安装，它会自动在 Windows 上设置构建环境。 由于您已安装 Windows 11 SDK 和 Visual Studio 工具，因此您现在可以在命令提示符 （） 或 PowerShell （） 中运行该实用程序。例如，检查版本：native-image``cmd``pwsh``native-image C:&gt; native-image.cmd - version 或者，您可以从 Visual Studio 2022 界面启动 Dev 命令提示符。 现在让我们尝试一下 Native Image，看看它能做什么！ 将此简单代码保存到 HelloWorld.java 文件中： public class HelloWorld { public static void main(String[] args) { System.out.println(“Hello, GraalVM Native Image!”); } } 在 JVM 上编译并运行： C:&gt; javac HelloWorld C:&gt; java HelloWorld 现在将类编译为本机可执行文件： C:&gt; native-image HelloWorld 这将在工作目录中生成一个可执行文件 。helloworld 现在执行它： C:&gt; helloworld Hello, GraalVM Native Image! 比较在 JVM 上运行时（步骤 2）和执行 生成的二进制文件时的启动时间（步骤 4）。或者在 PowerShell 中测量时间：native-image C:&gt; Measure-Command {“.\\helloworld”} Days : 0 Hours : 0 Minutes : 0 Seconds : 0 Milliseconds : 3 Ticks : 38000 TotalDays : 4.39814814814815E-08 TotalHours : 1.05555555555556E-06 TotalMinutes : 6.33333333333333E-05 TotalSeconds : 0.0038 TotalMilliseconds : 3.8 总结 在 Windows 上安装 GraalVM 非常简单！请记住，使用 Native Image 有一定的要求。借助 Native Image，您可以充分利用大大增加的启动时间和更小的占用空间来提前编译 Java 应用程序。本指南适用于 Windows 11，但也应适用于 Windows 8 和 10。 原文地址"},{"title":"使用 GraalVM 将 Java 代码编译为本地可执行文件","date":"2024-11-29T08:16:00.000Z","url":"/2024/11/29/2024/11/301856/","tags":[["GraalVM","/tags/GraalVM/"]],"categories":[["Java","/categories/Java/"]],"content":"使用 GraalVM 将 Java 代码编译为本地可执行文件（native executable）可以显著提高启动速度和降低内存消耗。以下是将您提供的从二维码中提取 secret 密钥的 Java 代码使用 GraalVM 编译为可执行命令的详细步骤。 前提条件 安装 GraalVM： 访问 GraalVM 官方下载页面 下载适用于您操作系统的 GraalVM 版本。 DownloadLiberica Native Image Kit ( Mac：java.library.path 中没有 awt 的问题) 解压缩并设置环境变量 GRAALVM_HOME 指向 GraalVM 安装目录。 将 GraalVM 的 bin 目录添加到 PATH 环境变量中。 安装 native-image 工具： GraalVM 默认不包含 native-image，需要通过 gu（GraalVM 的组件管理器）进行安装。 执行以下命令安装 native-image： 说明： -jar：指定要转换的 JAR 文件。 QRCodeSecretExtractor：指定生成的本地可执行文件的名称。 注意事项： 静态初始化：GraalVM 的 native-image 对反射、动态类加载等有一些限制。确保您的代码不依赖于这些特性，或者为它们提供相应的配置。 资源文件：如果您的应用使用了资源文件，可能需要在构建命令中指定资源目录。 类路径：确保所有依赖库都包含在 JAR 文件中（通过 maven-shade-plugin 已处理）。 4. 运行本地可执行文件生成的可执行文件将在当前目录下，命名为 QRCodeSecretExtractor（在 Linux 和 macOS 上）或 QRCodeSecretExtractor.exe（在 Windows 上）。 完整示例以下是整个过程的总结： 项目结构： pom.xml 配置： 编写 Java 代码：如上所述。 构建和生成本地可执行文件： 运行可执行文件： 可能遇到的问题及解决方案 缺少依赖或资源： 确保所有依赖库已正确打包在可执行 JAR 中。使用 maven-shade-plugin 可以帮助解决这个问题。 反射和动态加载问题： 如果您的应用使用了反射，可能需要为 native-image 提供配置文件。例如，使用 --initialize-at-build-time 选项或创建一个 reflect-config.json 文件。 性能问题： 本地可执行文件在启动速度和运行性能上通常优于 JVM 运行，但编译时间较长。根据需要调整 native-image 的选项以优化性能。 跨平台兼容性： 本地可执行文件是针对特定操作系统和架构生成的。确保在目标平台上生成和运行可执行文件。 额外优化建议 减少镜像大小： 使用 -H:IncludeResources 和 -H:ExcludeResources 选项来控制包含在镜像中的资源。 优化启动时间： 使用 --no-fallback 选项来移除 JVM 备份，以减少镜像大小和启动时间。 配置日志和输出： 根据需要调整日志输出，避免在生产环境中输出敏感信息。 参考资料 GraalVM 官方文档 GraalVM Native Image User Guide ZXing 项目主页 通过以上步骤，您可以将 Java 代码使用 GraalVM 编译为高效的本地可执行文件，并在无需 JVM 环境的情况下运行该程序。"},{"title":"ffmpeg转换多种视频流","date":"2024-11-27T16:00:00.000Z","url":"/2024/11/28/2024/11/281121/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"ffmpeg转换多种视频流 GPU Hardware Acceleration CPU Hardware Acceleration"},{"title":"ffmpeg批量转换音频到mp3格式","date":"2024-11-05T16:00:00.000Z","url":"/2024/11/06/2024/11/060958/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用 ffmpeg 批量转换音频文件到 MP3 格式是一个常见的任务，可以通过编写一个简单的脚本来实现。以下是在不同操作系统中实现这一任务的方法。 在 Linux 或 macOS 上 创建一个 Bash 脚本：创建一个名为 convert_to_mp3.sh 的文件，并在其中编写以下内容： 赋予脚本执行权限： 运行脚本：将你的音频文件放在一个目录中，然后运行脚本： 在 Windows 上 创建一个批处理文件：创建一个名为 convert_to_mp3.bat 的文件，并在其中编写以下内容： 运行批处理文件：将你的音频文件放在一个目录中，然后运行批处理文件： 解释 ffmpeg -i &quot;$file&quot; -vn -ar 44100 -ac 2 -ab 192k -f mp3 &quot;$DIR/$filename.mp3&quot;： -i &quot;$file&quot;：指定输入文件。 -vn：不包含视频流。 -ar 44100：设置音频采样率为 44.1 kHz。 -ac 2：设置音频通道数为 2（立体声）。 -ab 192k：设置音频比特率为 192 kbps。 -f mp3：指定输出格式为 MP3。 &quot;$DIR/$filename.mp3&quot;：指定输出文件名。 "},{"title":"批量重命名","date":"2024-11-05T16:00:00.000Z","url":"/2024/11/06/2024/11/061330/","tags":[["shell","/tags/shell/"]],"categories":[["Linux","/categories/Linux/"]],"content":"批量重命名文件是一个常见的任务，可以通过编写脚本来实现。以下是在不同操作系统中实现批量重命名的方法。 在 Linux 或 macOS 上 创建一个 Bash 脚本：创建一个名为 rename_files.sh 的文件，并在其中编写以下内容： 赋予脚本执行权限： 运行脚本：将你的文件放在一个目录中，然后运行脚本： 在 Windows 上 创建一个批处理文件：创建一个名为 rename_files.bat 的文件，并在其中编写以下内容： 运行批处理文件：将你的文件放在一个目录中，然后运行批处理文件： 解释 for file in &quot;$DIR&quot;/* 和 for %%f in (&quot;%DIR%\\*&quot;)：遍历指定目录中的所有文件。 if [ -f &quot;$file&quot; ] 和 if exist &quot;%%f&quot;：检查文件是否存在并且是普通文件。 ext=&quot;$&#123;file##*.&#125;&quot; 和 set &quot;ext=%%~xf&quot;：获取文件扩展名。 new_file=&quot;$DIR/$&#123;NEW_NAME_PREFIX&#125;_$COUNT.$ext&quot; 和 set &quot;new_file=%DIR%\\%NEW_NAME_PREFIX%_!COUNT!!ext!&quot;：构建新文件名。 mv &quot;$file&quot; &quot;$new_file&quot; 和 ren &quot;%%f&quot; &quot;!new_file!&quot;：重命名文件。 ((COUNT++)) 和 set /a COUNT+=1：增加计数器。 "},{"title":"在MySQL中insert时生成UUID","date":"2024-10-30T16:00:00.000Z","url":"/2024/10/31/2024/10/311458/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["Linux","/categories/Linux/"]],"content":"1. 生成UUID"},{"title":"使用ffmpeg裁剪图片","date":"2024-10-30T16:00:00.000Z","url":"/2024/10/31/2024/10/311755/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用 ffmpeg 裁剪图片尺寸为 1920x660，您可以使用 scale 滤镜来调整图片大小。但是，如果您想从原始图片中裁剪出一个特定区域而不是缩放整个图片，应该使用 crop 滤镜。以下是两个示例，分别展示了如何使用 ffmpeg 缩放和裁剪图片。 缩放图片到 1920x660如果您只是想简单地将图片缩放到指定的宽度和高度，可以使用以下命令： 这里的 -i input.jpg 是您的输入文件名，-vf &quot;scale=1920:660&quot; 表示视频滤镜（Video Filter）用来调整输出图片的大小，最后 output.jpg 是输出文件名。 裁剪图片到 1920x660如果您想从原始图片中裁剪出一个 1920x660 大小的区域，需要确定裁剪的起始点（左上角坐标）。假设您想从图片的中心位置开始裁剪，首先需要计算出正确的 x 和 y 坐标。这里有一个简单的例子，假设原始图片的尺寸大于 1920x660： 在这个命令中： crop=1920:660 指定了裁剪后的宽度和高度。 (in_w-1920)/2 计算了水平方向上的起始点，使得裁剪区域位于原始图片的中心。 (in_h-660)/2 同样是为了确保裁剪区域在垂直方向上也是居中的。 "},{"title":"Elasticsearch 7.9.3 和 Spring Boot 2.4.13 的索引别名管理","date":"2024-10-27T16:00:00.000Z","url":"/2024/10/28/2024/10/281527/","tags":[["elasticsearch","/tags/elasticsearch/"]],"categories":[["Java","/categories/Java/"]],"content":" Elasticsearch 7.9.3 和 Spring Boot 2.4.13 的索引别名管理 1. 理解索引别名索引别名（Alias） 是 Elasticsearch 提供的一种机制，允许你为一个或多个索引创建一个逻辑名称。通过别名，你可以在不影响应用程序的情况下管理底层索引。这对于以下场景尤为重要： 零停机部署：通过切换别名指向新索引，实现无缝迁移。 读写分离：将读操作和写操作指向不同的索引。 多租户支持：为不同租户创建独立索引，并通过别名进行访问。 2. 选择索引别名策略鉴于你需要处理大量文件的索引，以下几种策略可能适用： a. 时间分片索引（Time-Based Indices）适用于时间敏感的数据，如日志、文件等。 按时间创建索引：例如，file-index-2024-04、file-index-2024-05。 使用别名进行读写操作： 写入别名：指向当前活跃的索引，用于写入新数据。 读取别名：指向一个或多个索引，用于读操作。 b. Rollover 索引（Rollover Indices）Rollover API 允许根据条件（如文档数量、索引大小、时间）自动切换到新的索引。这对于动态增长的数据非常有用。 初始索引：命名为 file-index-000001，并设置别名。 条件触发 Rollover：如达到 30 天、100 万文档或 50GB 大小时，自动创建新索引并更新别名。 3. 在 Spring Boot 2.4.13 中集成 Elasticsearch 7.9.3 并管理索引别名a. 添加依赖确保你的 pom.xml 包含以下依赖，以集成 Elasticsearch 7.9.3： 注意：elasticsearch-rest-high-level-client 是管理索引和别名的关键依赖。 b. 配置 Elasticsearch 连接在 application.yml 或 application.properties 中配置 Elasticsearch 连接信息： c. 使用 IndicesAliasesRequest 管理索引别名由于 PutAliasRequest 在 Elasticsearch 7.9.3 中不可用，我们将使用 IndicesAliasesRequest 来添加、删除和切换别名。 示例代码：ElasticsearchAliasService d. 实现索引别名策略基于上述服务，你可以实现时间分片或 Rollover 策略。以下是一个基于 Rollover 的示例。 示例代码：ElasticsearchIndexService 示例代码：ElasticsearchSetup你可以在应用启动时或通过定时任务来初始化索引和执行 Rollover。 4. 最佳实践和注意事项 监控索引状态：定期监控索引的健康状态和别名的指向，确保系统正常运行。可以使用 Elasticsearch 的 _cat/aliases 和 _cat/indices API 进行监控。 自动化管理：使用定时任务或自动化工具（如 Spring Scheduler）管理索引的创建和别名的切换，减少手动操作的出错风险。 映射和设置一致性：在创建新索引时，确保索引映射和设置的一致性，以避免查询和写入问题。可以使用索引模板（Index Templates）来预定义索引的映射和设置。 清理旧索引：根据业务需求，定期删除不再需要的旧索引，节省存储空间。可以使用 Elasticsearch 的 DeleteIndexRequest 或通过定时任务实现自动清理。 权限管理：确保应用程序具有足够的权限进行索引和别名的管理操作。配置 Elasticsearch 的安全性（如 X-Pack）时，注意为应用程序分配适当的角色和权限。 错误处理：在实际操作中，确保捕获和处理可能的异常，保证系统的稳定性。例如，处理索引创建失败、别名更新失败等情况。 版本兼容性：确保 Elasticsearch 客户端版本与 Elasticsearch 服务器版本兼容。Elasticsearch 7.9.3 的 REST High-Level Client 应与相同版本的 Elasticsearch 服务器配合使用。 5. 完整示例代码以下是一个完整的示例，展示如何在 Spring Boot 2.4.13 中使用 Elasticsearch 7.9.3 管理索引别名，并实现基于 Rollover 的索引管理。 a. pom.xml b. application.yml c. ElasticsearchAliasService.java d. ElasticsearchIndexService.java e. ElasticsearchSetup.java f. FileDocument.java定义一个实体类，映射到 Elasticsearch 的文档。 g. FileRepository.java定义一个 Repository 接口，使用别名进行操作。 h. FileService.java编写一个服务，用于处理文件的索引和查询。 6. 总结在 Spring Boot 2.4.13 中集成 Elasticsearch 7.9.3 并使用索引别名，可以通过以下步骤实现： 配置依赖和连接：确保项目中包含适用于 Elasticsearch 7.9.3 的 REST High-Level Client，并正确配置连接信息。 管理索引别名：使用 IndicesAliasesRequest 添加、删除和切换索引别名。 实现索引策略：根据业务需求选择时间分片或 Rollover 策略，确保索引的高效管理和扩展。 集成到 Spring Boot：通过服务层封装 Elasticsearch 操作，并在应用启动时或通过定时任务初始化和管理索引。 遵循最佳实践：监控索引状态、自动化管理、确保映射一致性、定期清理旧索引等，提升系统的稳定性和可维护性。 通过以上步骤，你可以在 Spring Boot 应用中高效地集成 Elasticsearch，并利用索引别名策略管理大量文件的索引。如果在实施过程中遇到进一步的问题，欢迎提供更多细节，以便获得更具体的帮助。"},{"title":"高德地图JS API 安全密钥使用","date":"2024-10-20T16:00:00.000Z","url":"/2024/10/21/2024/10/211547/","tags":[["map","/tags/map/"]],"categories":[["Linux","/categories/Linux/"]],"content":"高德地图在2021年12月02日以后申请的 key 需要配合安全密钥一起使用。 通过代理服务器转发 通过命令nginx -s reload命令重新加载nginx配置文件 JS API 脚本同步加载示例 通过明文方式设置不建议在生产环境使用（不安全） 这个设置必须是在 JS API 脚本加载之前进行设置，否则设置无效 参考文件高德地图JS API 安全密钥使用"},{"title":"hexo 添加 RSS 订阅功能","date":"2024-10-13T16:00:00.000Z","url":"/2024/10/14/2024/11/091806/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在 Hexo 框架中，添加 RSS 订阅功能是一个相对简单的过程，Hexo 本身已经内建了 RSS 功能支持。你只需要做以下几个步骤即可启用它。 步骤 1: 安装 Hexo 插件（如果未安装）如果你使用的 Hexo 版本没有自动启用 RSS 功能，可以通过安装一个 Hexo 插件来支持它。一般来说，默认情况下，Hexo 会通过 hexo-generator-feed 插件生成 RSS feed 文件。 首先，你需要确认是否已经安装了这个插件。如果没有，使用以下命令安装： 步骤 2: 配置 RSS 生成器在 Hexo 的配置文件 _config.yml 中，通常会有一部分配置涉及到 RSS 订阅。打开 Hexo 项目的根目录下的 _config.yml 配置文件，找到类似下面的配置部分： type：可以是 rss 或 atom，这里选择 rss。 path：生成的 RSS 文件的路径（通常是 rss.xml）。 limit：控制 RSS feed 中文章的数量。如果你希望每次生成的 RSS 文件包含 20 篇文章，可以设置为 20。 你可以根据自己的需要调整这些配置项。 步骤 3: 修改主题中的配置有些 Hexo 主题可能会有自己的配置项来控制是否启用 RSS 功能。如果你的主题支持 RSS 功能，通常你会在主题的 _config.yml 配置文件中找到类似如下的配置： 确保将其设置为 true，这样主题会自动在页面中添加一个指向 RSS 文件的链接。 如果你的主题没有这个配置选项，可以手动添加 RSS 订阅链接到主题的布局模板文件。 步骤 4: 自定义 RSS 订阅页面（可选）如果你希望自定义生成的 RSS 订阅内容，可以通过编辑 _config.yml 中的 feed 配置项，或者通过自定义 Hexo 插件来扩展。 例如，添加文章摘要、修改 RSS 标题、设置作者信息等，都可以通过编辑或扩展插件实现。 步骤 5: 生成和查看 RSS 文件配置完成后，运行以下命令来生成站点： 这会生成你的博客站点和 rss.xml 文件，通常会在 public 文件夹下。 你可以通过访问以下 URL 来查看你的 RSS 订阅链接： 确保将 your-hexo-site.com 替换成你的 Hexo 网站域名。 步骤 6: 在页面上添加 RSS 订阅链接（可选）如果希望在你的站点页面上添加一个明显的 RSS 订阅按钮或链接，可以将其添加到你的主题模板中。通常，这涉及到修改主题的 header.ejs 或 footer.ejs 文件，添加如下代码： 将这段代码放到你想显示 RSS 订阅链接的位置，通常是页脚或导航栏。 完成完成以上步骤后，你就为你的 Hexo 博客添加了 RSS 订阅功能。用户可以通过访问 rss.xml 获取你网站的更新内容，也可以订阅 RSS 链接，在他们的 RSS 阅读器中查看你的博客更新。 如果你有其他需求，如生成 Atom Feed 或对 RSS 输出进行更深入的自定义，可以参考 hexo-generator-feed 插件的文档。"},{"title":"CompletableFuture中优雅地处理异常","date":"2024-09-29T16:00:00.000Z","url":"/2024/09/30/2024/09/301554/","tags":[["CompletableFuture","/tags/CompletableFuture/"]],"categories":[["Java","/categories/Java/"]],"content":"在 CompletableFuture.runAsync 中优雅地处理异常，可以通过使用 handle() 或 exceptionally() 方法来处理异步任务中的异常。这种方式比在任务内部直接捕获异常更符合异步编程的思路，且更清晰易读。 使用 handle() 处理异常handle() 方法允许你处理任务的结果或异常，并返回新的结果。 使用 exceptionally() 处理异常exceptionally() 方法只在任务出现异常时被调用，处理完异常后可以返回默认的结果。 总结 handle() 可以同时处理成功和失败的情况。 exceptionally() 只处理异常情况。 这两种方法都比在异步任务内部直接捕获异常更优雅，可以根据你的具体需求选择合适的方式。"},{"title":"用python获取文件名称和后缀","date":"2024-09-25T16:00:00.000Z","url":"/2024/09/26/2024/09/260942/","tags":[["python","/tags/python/"]],"categories":[["Python","/categories/Python/"]],"content":"下载文件时,将文件保存为链接中的名称与类型 获取文件后缀 获取名称与后缀"},{"title":"Onlyoffice升级至8.1","date":"2024-09-25T16:00:00.000Z","url":"/2024/09/26/2024/09/261107/","tags":[["onlyoffice","/tags/onlyoffice/"]],"categories":[["Linux","/categories/Linux/"]],"content":"目前使用的是6.4版本,存在一些问题 背景 光标错乱 编辑后行间距变大等问题 一些bug需要自行修改原服务才可以正常运行 比如: 请求Editor.bin 协议变成websocket原因: 这是onlyoffice内部请求，这个url是有内部js拼接的，拼接url时会根据当前请求上下文拿到获取协议，但是不同的部署环境，可能造成获取到协议有问题 解决方案: 通过修改配置/etc/onlyoffice/documentserver/local.json https被重定向成http，请求被block问题打开onlyoffice编辑器，请求经过多层转发，到达onlyoffice的ng时，被重定向 解决办法 升级的dockerfile docker-compose.yml 参考文档 Installing ONLYOFFICE "},{"title":"docx水印透明度","date":"2024-09-02T16:00:00.000Z","url":"/2024/09/03/2024/09/031405/","tags":[["docx","/tags/docx/"]],"categories":[["Java","/categories/Java/"]],"content":"docx水印透明度 查看添加水印后的文件样式 将docx改为zip 解压文件 找到header.xml 对应的代码 gist链接"},{"title":"idea使用ZGC","date":"2024-08-25T16:00:00.000Z","url":"/2024/08/26/2024/08/261446/","tags":[["idea","/tags/idea/"]],"categories":[["Java","/categories/Java/"]],"content":"idea使用ZGC 设置Jre,确认使用的为21版本 设置JVM参数 "},{"title":"MySQL合并多列数据","date":"2024-08-16T16:00:00.000Z","url":"/2024/08/17/2024/08/172130/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["Linux","/categories/Linux/"]],"content":"MySQL合并多列数据 CONCAT(s1,s2…sn) 字符串 s1,s2 等多个字符串合并为一个字符串 CONCAT_WS(x, s1,s2…sn) 同 CONCAT(s1,s2,…) 函数，但是每个字符串之间要加上 x，x 可以是分隔符 合并多个字符串 合并多个字符串，并添加分隔符 "},{"title":"autoindex","date":"2024-08-12T16:00:00.000Z","url":"/2024/08/13/2024/08/131556/","tags":[["autoindex","/tags/autoindex/"]],"categories":[["Linux","/categories/Linux/"]],"content":"搭建文件共享服务 nginx autoindexNginx 是一个高性能的 HTTP 和反向代理服务器，它也可以作为 IMAP&#x2F;POP3 代理服务器。Nginx 以其稳定性、丰富的功能集、简单的配置和低资源消耗而闻名。 autoindex 是 Nginx 模块中的一个特性，它允许 Nginx 服务器自动生成目录索引页面。这意味着当用户访问一个目录而不是一个具体的文件时，Nginx 可以返回一个包含该目录下所有文件和子目录列表的页面。这对于网站管理员来说非常有用，因为它可以方便地展示文件结构，而无需手动创建索引页面。 以下是 Nginx 使用 autoindex 模块的一些基本配置示例： 启用 autoindex 模块：在 Nginx 配置文件中，你需要启用 autoindex 模块。通常，这可以通过在 server 块中添加 autoindex on; 指令来实现。 自定义索引页面：你可以使用 autoindex_exact_size 和 autoindex_localtime 指令来自定义索引页面的显示。autoindex_exact_size 允许显示文件的确切大小，而 autoindex_localtime 则使用本地时间而不是GMT时间。 设置索引页面的格式：你还可以使用 autoindex_format 指令来设置索引页面的格式，它可以是 html 或 json。 添加自定义样式：如果你想对自动生成的索引页面进行样式定制，你可以使用 autoindex_style 指令来指定一个 CSS 文件。 限制访问：如果你只想让特定的用户或IP地址访问索引页面，你可以使用 allow 和 deny 指令来设置访问控制。 示例配置 配置文件 docker-compose 样式自定义.autoindex.html放在www目录下 flask autoindex 已不再更新github:flask-autoindex Apache(httpd) (可选)修改配置文件/usr/local/apache2/conf/httpd.conf 反注释掉 python docker-compose Simple Web Serverjdk18+ "},{"title":"ArchLinux下安装Pytorch","date":"2024-08-10T16:00:00.000Z","url":"/2024/08/11/2024/08/111054/","tags":[["pytorch","/tags/pytorch/"]],"categories":[["Linux","/categories/Linux/"]],"content":"安装Pytorch 在 Arch Linux 下安装 PyTorch 的步骤如下： 1. 安装 Python确保你的系统上已经安装了 Python。你可以使用以下命令来安装 Python 和 pip（Python 包管理工具）： 2. 安装 PyTorch在 Arch Linux 上，可以通过 pip 安装 PyTorch。 condapytorch mamba 上面的命令将安装 PyTorch 及其相关的库 torchvision（计算机视觉相关工具）和 torchaudio（音频处理工具）。 3. 使用 AUR 安装（可选）如果你更喜欢使用 Arch User Repository (AUR) 安装 PyTorch，可以使用以下步骤： 首先，你需要安装一个 AUR 助手，如 yay 或 paru。如果你还没有安装 AUR 助手，可以使用以下命令安装 yay： 然后使用 yay 安装 PyTorch： 4. 验证安装安装完成后，你可以通过以下 Python 命令来验证 PyTorch 是否安装成功： 如果正确安装，你将看到 PyTorch 的版本号输出。 5. 安装 CUDA（如果需要）如果你计划在 GPU 上使用 PyTorch 并且你的 GPU 支持 CUDA，你需要确保 CUDA 已经安装并正确配置。你可以通过安装 cuda 包来完成： 完成后，确保 CUDA 工具链路径已添加到你的 PATH 环境变量中： 完成以上步骤后，你的 Arch Linux 系统应该已经成功安装了 PyTorch 并准备就绪。"},{"title":"SDB模版导出","date":"2024-08-04T16:00:00.000Z","url":"/2024/08/05/2024/08/051210/","tags":[["word","/tags/word/"]],"categories":[["Java","/categories/Java/"]],"content":"模版导出 模版导出 配置视图SQL 在代码中设置变量，变量名称为视图SQL中所需要的参数名称 视图配置中选择参数与模版之中的变量一一匹配 如果所需的视图函数有多个，且需要的参数也为多个，则需要配置不同的视图变量 问题处理模版导出缺失部分数据 原因 大数据服务在执行视图SQL时会报错,需要对应的同学去排查 同一个模版中,如果取值不一样,不能配置成相同的标签"},{"title":"在Kubernetes中设置容器环境变量","date":"2024-07-16T16:00:00.000Z","url":"/2024/07/17/2024/07/171605/","tags":[["k8s","/tags/k8s/"]],"categories":[["Java","/categories/Java/"]],"content":"在Kubernetes中设置容器环境变量 在Kubernetes中设置容器环境变量有几种方法： 通过Dockerfile设置：在Dockerfile中使用ENV指令设置环境变量，这些变量会在构建镜像时设置，并且可以在容器运行时使用。 通过Kubernetes配置文件设置： 在Kubernetes的部署配置文件中，你可以在spec.containers.env部分设置环境变量。 通过命令行参数传递：使用kubectl命令行工具时，可以使用--env或-e参数来设置环境变量。 使用ConfigMap或Secret：如果环境变量需要来自外部配置或敏感信息，可以使用ConfigMap或Secret来设置。 ConfigMap： 然后在Pod的配置中引用ConfigMap： Secret：对于敏感信息，使用Secret： 然后在Pod的配置中引用Secret： 使用downwardAPI：如果你需要根据Pod的字段设置环境变量，可以使用downwardAPI。 选择哪种方法取决于你的具体需求和偏好。通常情况下，对于静态环境变量，直接在配置文件中设置是最简单的方法。对于动态或敏感的环境变量，使用ConfigMap或Secret更为合适。"},{"title":"podman","date":"2024-07-14T16:00:00.000Z","url":"/2024/07/15/2024/07/151400/","tags":[["podman","/tags/podman/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Podman 是一个无需守护进程的容器引擎，由 Red Hat 开发。它允许你运行 Linux 容器，无需运行守护进程。Podman 旨在提供一种更安全、更灵活的方式来管理容器，同时减少系统资源的消耗。 Podman 的一些主要特点包括： 无需守护进程：Podman 直接在前台运行，不需要后台守护进程。 用户空间：Podman 运行在用户空间，不需要 root 权限（除了一些特定的操作，如创建网络或挂载文件系统）。 兼容性：Podman 支持 Docker 命令行界面，因此可以无缝替换 Docker。 安全性：Podman 提供了更好的隔离性和安全性，因为它不需要 root 权限来运行大多数操作。 多架构支持：Podman 支持多种架构，包括 x86_64、ARM、s390x 等。 使用 Podman 的基本命令包括： podman run：运行一个容器。 podman pull：从镜像仓库拉取镜像。 podman push：将镜像推送到镜像仓库。 podman images：列出本地镜像。 podman ps：列出正在运行的容器。 podman exec：在运行的容器中执行命令。 安装如果安装了docker需要先卸载 在 Ubuntu 上安装 Podman： 在 CentOS 上安装 Podman： 在Arch linux上安装podman： 在MacOS上安装podman： 参考资料 Podman "},{"title":"Caffeine","date":"2024-07-10T16:00:00.000Z","url":"/2024/07/11/2024/07/111544/","tags":[["Caffeine","/tags/Caffeine/"]],"categories":[["Java","/categories/Java/"]],"content":"缓存框架 Caffeine依赖 配置Caffeine缓存创建一个专门的Caffeine缓存配置。使用本地缓存选择淘汰策略很重要，业务场景是根据实现来查询，所以Caffeine将按照最近最少使用（LRU）的策略来淘汰旧数据成符合业务。 使用"},{"title":"MySQL视图","date":"2024-07-09T16:00:00.000Z","url":"/2024/07/10/2024/07/102149/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["Linux","/categories/Linux/"]],"content":"MySQL视图 MySQL视图（View）是基于 SQL 查询的结果集创建的虚拟表。视图本身不存储数据，而是存储查询。在使用视图时，MySQL 会动态执行该视图所对应的查询，将结果集返回给用户。视图可以简化复杂查询，提高代码的可读性，并增强数据的安全性。 创建视图创建视图的基本语法如下： 例如，假设有一个名为 employees 的表，想创建一个只包含 name 和 salary 列的视图，视图名为 employee_salaries，可以这样做： 使用视图视图可以像表一样使用。可以对视图进行查询、连接等操作。例如，查询 employee_salaries 视图： 更新视图视图的定义可以更新，使用 CREATE OR REPLACE VIEW 语句。例如，假设需要在 employee_salaries 视图中增加一个 department 列，可以这样做： 删除视图删除视图使用 DROP VIEW 语句。例如，要删除 employee_salaries 视图，可以这样做： 视图的优点 简化复杂查询：可以将复杂查询逻辑封装在视图中，提高查询的可读性和可维护性。 数据安全性：可以限制用户访问表中的特定列或行，通过视图只暴露需要的数据。 数据独立性：应用程序可以通过视图访问数据，数据库结构发生变化时只需要更新视图定义，而不需要修改应用程序。 注意事项 性能：视图本身不存储数据，每次查询视图时都会执行视图定义中的查询。如果视图定义的查询比较复杂，可能会影响性能。 更新限制：某些情况下，通过视图更新数据可能会受到限制，特别是视图包含聚合函数、连接操作或子查询时。 "},{"title":"2024下半年目标","date":"2024-07-07T16:00:00.000Z","url":"/2024/07/08/2024/07/081348/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"下半年目标 目标 每月阅读一本 每两个星期减掉1kg "},{"title":"kindle","date":"2024-07-04T16:00:00.000Z","url":"/2024/07/05/2024/07/051356/","tags":[["kindle","/tags/kindle/"]],"categories":[["wuw","/categories/wuw/"]],"content":"2024年6月30日，也是亚马逊中国宣布关闭电子书店、关闭云端的最后一天。 是的，这一天终于来了。‍ 从今天开始，如果你的Kindle地址是@kindle.cn结尾，将不能使用邮箱推送书籍。 因此，建议大家：去注册美国亚马逊，使用美国亚马逊的云端推送服务。‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍ kindle官网 kindle 推送到Kindle 管理推送 推送设置 管理设备 Calibre calibre 推送到kindle首选项 —&gt; 通过邮件分享 —&gt; 添加邮箱（邮箱为kindle接收的邮箱） 发件人地址（邮箱需要添加到信任列表，否则无法收到推送） 建议使用outlook邮箱,可以快速设置，其他邮箱需要自行设置smtp 工具网站 书伴 "},{"title":"Git命令用于合并分支/变基","date":"2024-07-04T01:06:01.000Z","url":"/2024/07/04/2024/07/041522/","tags":[["git","/tags/git/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Git命令用于合并分支可以通过以下步骤完成： Git 合并分支1. 确保分支最新首先，确保你所在的分支是最新的。通常情况下，你想要将另一分支合并到当前分支。 2. 合并分支使用 git merge 命令来合并分支。 假设你想将 feature-branch 合并到当前的 main 分支： 3. 处理冲突如果合并过程中出现冲突，需要手动解决冲突。Git 会标记冲突的文件，你需要编辑这些文件来解决冲突，然后标记冲突已解决： 4. 推送更新最后，将合并后的分支推送到远程仓库： 这样就完成了分支的合并。 Git 变基（rebase）Git 变基（rebase）命令用于将一组提交移动到一个新的基础提交之上。变基通常用于清理提交历史或在合并时保持历史记录的线性。以下是变基的基本用法： 1. 基本变基假设你在 feature-branch 分支上工作，你想将其变基到 main 分支上： 2. 解决冲突如果在变基过程中出现冲突，Git 会暂停变基并让你解决冲突。解决冲突后，继续变基过程： 如果你想中止变基过程，可以使用： 3. 交互式变基交互式变基允许你在变基过程中编辑、合并或删除提交。使用 -i 参数进行交互式变基： 此命令会打开一个文本编辑器，其中列出了即将变基的所有提交。你可以通过修改该文件来选择如何处理每个提交，例如修改提交信息、合并提交等。 4. 推送变基后的分支变基会修改提交历史，因此在变基后需要强制推送分支到远程仓库： 使用 --force-with-lease 是为了防止覆盖其他人的变更，它会确保你推送的分支自你上次拉取后没有被修改。 示例假设你在 feature-branch 上工作，你想将其变基到 main 分支上并解决所有冲突： 这样，你就完成了对分支的变基。"},{"title":"工作使我快乐","date":"2024-06-29T16:00:00.000Z","url":"/2024/06/30/2024/06/301704/","tags":[["daily","/tags/daily/"]],"categories":[["work","/categories/work/"]],"content":"格子间的生与死：当我们厌恶工作 工作耗尽了一个人绝大部分的神经力量，而这些力量原本是可以用来沉思、冥想、做梦、担忧、去爱、去恨的。– 尼采我是 975 工作制，其实工作时间并不比前一份工作要长，但是感觉累坏了。每天到家晚上啥也干不了，感觉整个人又寂寞又孤独，感觉人生下来就是为了上班。而且我回家就天黑了，感觉感官停留在公司，我中午找个河边走走才感觉好点。我觉得好难受，像零件。人是社会关系的总和，过长的工作时间挤占了人类社交空间的同时，也削减了人类定义自我的能力。除了金钱上的问题、社交上的孤独，没有成就感或者成就感太迟，也是工作生活中另一个常见又容易被忽视的痛苦。我喜欢工作，因为有意义的工作能让我找到自己的价值。当然我说的是有意义的工作我们不讨厌工作，我们都是 985 出来的，过去接受的教育和责任感决定了我们并不是那种对工作敷衍塞责的人，我们讨厌的只是不信任和压榨。大卫·格雷伯在《毫无意义的工作》一书中称，“人类之所以觉得自己是自主存在，是与世界和他人分离的独立体，很大程度上是因为人们觉得自己可以按照可预测的方式对世界和他人产生影响。一旦被剥夺这种拥有力量的感觉，人类就什么也不是了。”为什么金钱的激励作用越来越弱，不能压倒重复性工作的痛苦，因为大部分金钱只能被锁在柜子里，或者被锁在银行账户里，根本不会对世界产生什么影响，不能增强自己的力量。为什么人会因为在格子间里缺乏社交而感到痛苦，因为这样让我们无法影响他人，如果某一天突然死去，远方的亲朋或许在很长一段时间里都浑然无知，而同事或许只会觉得你想安静一阵子。为什么会因为觉得工作毫无价值而感到痛苦，因为你很容易意识到，这样的工作只是一种自欺欺人的游戏，既无法影响他人，也无法完善自我。我需要工作对社会有帮助，需要从中找到自己的价值，但或许工作就是没有正向的社会价值的。或许正确的应对手段，是放弃把工作作为目的本身，而是把它作为一种达成目的的手段，获得资金和人脉的积累，获得经验，为了某一天去做自己真正想做的事。 "},{"title":"使视频背景自适应屏幕分辨率","date":"2024-06-28T16:00:00.000Z","url":"/2024/06/29/2024/06/291644/","tags":[["css","/tags/css/"]],"categories":[["work","/categories/work/"]],"content":"使视频背景自适应屏幕分辨率 要使视频背景自适应屏幕分辨率，你可以使用CSS来调整视频的大小和位置。以下是一个示例代码，你可以将其添加到你的CSS文件中： 确保你的HTML结构保持不变： 这种方式可以确保视频背景在不同分辨率和设备上都能自适应屏幕大小。"},{"title":"pywpsrpc转换excel为pdf时设置横向打印","date":"2024-06-28T08:00:00.000Z","url":"/2024/06/28/2024/06/281600/","tags":[["wps","/tags/wps/"]],"categories":[["Linux","/categories/Linux/"]],"content":"wps转换excel为pdf时设置横向打印，当列多的时候设置横向，能更多的使其显示在页面上。 示例转换代码 更多参考：容器中使用wps转pdf 更多用法: learn.microsoft.com"},{"title":"Cannot load libnvcuvid.so.1","date":"2024-06-21T16:00:00.000Z","url":"/2024/06/22/2024/06/221759/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"[h264_cuvid @ 0x5579a46f5700] Cannot load libnvcuvid.so.1[h264_cuvid @ 0x5579a46f5700] Failed loading nvcuvid. 这个错误信息表明在使用FFmpeg进行视频转换时，库文件libnvcuvid.so.1未能加载成功。libnvcuvid.so.1是NVIDIA提供的用于硬件加速的视频解码库。要解决这个问题，您可以尝试以下步骤： 确认系统中已安装NVIDIA驱动和CUDA Toolkit：确保系统中安装了NVIDIA的驱动和CUDA Toolkit，并且版本兼容。您可以通过以下命令检查CUDA的安装情况： 如果没有安装，可以参考NVIDIA的官方文档进行安装。 安装NVIDIA Video Codec SDK：libnvcuvid.so.1是NVIDIA Video Codec SDK的一部分。您需要下载并安装该SDK。可以从NVIDIA开发者网站下载。 设置环境变量：确保LD_LIBRARY_PATH环境变量包含NVIDIA库的路径。例如，如果库文件位于/usr/local/cuda/lib64，您可以通过以下命令添加路径： 验证库文件是否存在：确保系统中确实存在libnvcuvid.so.1文件。您可以使用以下命令进行检查： 如果文件不存在，您可能需要重新安装相关的CUDA工具包。 重建FFmpeg：如果以上步骤都未能解决问题，您可以尝试重新编译FFmpeg，确保在配置时正确包含了NVIDIA支持。您可以使用以下配置选项： 通过这些步骤，应该能够解决libnvcuvid.so.1未能加载的问题。"},{"title":"Github Packages","date":"2024-06-12T16:00:00.000Z","url":"/2024/06/13/2024/06/131456/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Github Packages 是 Github 提供的容器仓库，可以用来存放镜像。 dockerhub已经无法正常使用。。。 创建token首先需要创建token，在github的个人设置中，找到Personal access tokens，点击Generate new token。 至少勾选 read:packages 和 write:packages 权限 Developer Settings 注意选择token (classic) 登录 推送镜像"},{"title":"端午快乐","date":"2024-06-09T16:00:00.000Z","url":"/2024/06/10/2024/06/101042/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"端午快乐 "},{"title":"GraphicsMagick","date":"2024-05-27T16:00:00.000Z","url":"/2024/05/28/2024/05/281112/","tags":[["image","/tags/image/"]],"categories":[["Linux","/categories/Linux/"]],"content":"GraphicsMagick提供了一个强大的命令行实用程序gm，它 可用于访问所有 GraphicsMagick 函数 官方网站 下载地址 命令示例 将多张图片转成pdfthreads限制资源使用的线程数量 其他可以使用GraphicsMagick+im4java来处理图片"},{"title":"ClashX","date":"2024-05-21T16:00:00.000Z","url":"/2024/05/22/2024/05/221342/","tags":[["clashx","/tags/clashx/"]],"categories":[["work","/categories/work/"]],"content":"Clasx的安装与使用 下载安装地址 clashx 配置 系统更新后没有网速问题卸载重新安装"},{"title":"TransactionalEventListener的使用和实现原理","date":"2024-05-21T16:00:00.000Z","url":"/2024/05/22/2024/05/221536/","tags":[["Spring","/tags/Spring/"]],"categories":[["Java","/categories/Java/"]],"content":"@TransactionalEventListener 是 Spring Framework 中的一个注解，用于在事务成功提交后异步处理事件。它是 ApplicationListener 的一种特殊实现，允许在事务成功提交后才触发事件处理方法。这个功能主要用于确保某些事件处理逻辑只有在事务成功提交后才执行，从而保证数据的一致性。 使用方法以下是一个示例，展示了如何使用 @TransactionalEventListener： 定义事件类： 发布事件： 在一个事务性方法中发布事件： 事件监听器： 使用 @TransactionalEventListener 注解来监听事件： 实现原理@TransactionalEventListener 的工作原理依赖于 Spring 的事务和事件机制。 事件发布： 当调用 eventPublisher.publishEvent(event) 时，事件会发布到 Spring 的事件机制中。 事件监听： 使用 @TransactionalEventListener 注解的方法会被 Spring 识别为事务性事件监听器。 Spring 会将这些事件监听器注册到事务管理器中，以便在事务完成时调用。 事务完成时触发事件： 当一个事务成功提交后，Spring 的事务管理器会通知所有注册的 TransactionalEventListener，并触发对应的事件处理方法。 保证一致性： 通过这种方式，事件处理逻辑只有在事务成功提交后才会执行，确保了数据的一致性。例如，如果事务回滚了，事件处理逻辑不会被触发。 配置选项@TransactionalEventListener 提供了一些配置选项，可以进一步定制其行为： phase：指定监听器的调用阶段，默认是 TransactionPhase.AFTER_COMMIT。其他选项包括 BEFORE_COMMIT, AFTER_ROLLBACK, 和 AFTER_COMPLETION。 总结： @TransactionalEventListener 提供了一种在事务完成后处理事件的机制，确保事件处理逻辑与事务的状态保持一致。 它依赖于 Spring 的事务管理和事件发布机制，通过注解和配置选项可以灵活地定制其行为。 "},{"title":"PowerShell无法运行：running scripts is disabled on this system","date":"2024-05-21T16:00:00.000Z","url":"/2024/05/22/2024/05/221754/","tags":[["PowerShell","/tags/PowerShell/"]],"categories":[["Linux","/categories/Linux/"]],"content":"运行PowerShell脚本提示：running scripts is disabled on this system 权限策略的问题，修改执行策略: 以管理员运行PowerShell。 "},{"title":"Java中Future的get方法超时处理","date":"2024-05-20T16:00:00.000Z","url":"/2024/05/21/2024/05/211805/","tags":[["Future","/tags/Future/"]],"categories":[["Java","/categories/Java/"]],"content":"Java 中 Future 的 get 方法超时会怎样？ 工作中，很多人会使用线程池的 submit 方法 获取 Future 类型的返回值，然后使用 java.util.concurrent.Future#get(long, java.util.concurrent.TimeUnit) 实现“最多等多久”的效果。 在 Java 中，Future 接口的 get 方法有两个重载版本：一个是不带参数的 get()，另一个是带有超时参数的 get(long timeout, TimeUnit unit)。当使用带超时参数的 get 方法时，如果在指定的时间内任务没有完成，该方法会抛出 TimeoutException。 具体来说，超时的行为如下： 不带参数的 get() 方法： 该方法会一直阻塞，直到任务完成或线程被中断。如果任务完成，它会返回任务的结果；如果线程被中断，它会抛出 InterruptedException。 带超时参数的 get(long timeout, TimeUnit unit) 方法： 该方法会阻塞指定的时间 timeout（由 unit 指定时间单位）。如果在超时时间内任务完成，它会返回任务的结果；如果超时，它会抛出 TimeoutException；如果线程在等待期间被中断，它会抛出 InterruptedException。 示例代码： 在这个例子中，任务模拟了一个需要 5 秒完成的任务。由于我们在 get 方法中只等待 2 秒，所以在超时后会抛出 TimeoutException 并输出 “任务超时”。 总的来说，使用带超时参数的 get 方法可以防止程序长时间阻塞在未完成的任务上，通过处理 TimeoutException 来做相应的超时处理。 测试demo 结论： 当前线程会因为收到 TimeoutException 而被中断，线程池里对应的线程“却”继续执行完毕。 取消 看源码注释我们可以知道： 当设置为 true 时，正在执行的任务将被中断（interrupted）； 当设置为 false 时，如果任务正在执行中，那么仍然允许任务执行完成。 但是 不同的实现类对参数的“效果”也有差异。 java.util.concurrent.CompletableFuture#cancel java.util.concurrent.FutureTask#cancel 使用demo 在 Java 中，Future 接口的 cancel(boolean mayInterruptIfRunning) 方法可以用于取消正在进行的任务。这个方法有一个布尔参数 mayInterruptIfRunning，决定是否中断正在运行的任务。 具体行为如下： cancel(false)：尝试取消任务，但不会中断正在运行的任务。如果任务尚未启动或已经完成，则任务将被取消。 cancel(true)：尝试取消任务，并且如果任务正在运行，则会中断它。 需要注意的是，任务是否可以被中断取决于任务的实现。如果任务中有对线程中断状态的检查（例如通过 Thread.interrupted() 或 Thread.isInterrupted() 检查中断状态，或抛出 InterruptedException），那么任务可以响应中断请求并停止执行。 下面是一个示例代码，展示了如何使用 cancel 方法来中断一个正在运行的任务： 在这个例子中，任务每秒打印一次 “任务正在运行…”。主线程等待 3 秒后，尝试取消任务并中断它。如果任务正在运行，它会被中断，并输出 “任务被中断”。 总结： cancel(false) 只尝试取消尚未启动或未完成的任务，不中断正在运行的任务。 cancel(true) 不仅尝试取消任务，还会中断正在运行的任务（如果任务支持中断）。 任务能否被中断取决于任务的实现，必须在任务中适当处理中断状态。 "},{"title":"powershell统计以某个单词开头文件个数","date":"2024-05-20T16:00:00.000Z","url":"/2024/05/21/2024/05/211804/","tags":[["powershell","/tags/powershell/"]],"categories":[["work","/categories/work/"]],"content":"由于aws s3海外环境不允许上传以hcc开头的文件（403错误），所以需要统计文件的个数。 powershell统计以HCC单词开头文件个数 文件夹"},{"title":"nano编辑保存","date":"2024-04-23T16:00:00.000Z","url":"/2024/04/24/2024/04/241601/","tags":[["nano","/tags/nano/"]],"categories":[["Linux","/categories/Linux/"]],"content":"键入 nano 可让你进入Nano 编辑器 CTRL + O 保存一个 Nano 文件 CTRL + X 退出 Nano "},{"title":"在 CentOS 7|RHEL 7|Oracle Linux 7 上安装 Temurin OpenJDK 21","date":"2024-04-23T16:00:00.000Z","url":"/2024/04/24/2024/04/242046/","tags":[["jdk","/tags/jdk/"]],"categories":[["Linux","/categories/Linux/"]],"content":"AdoptOpenJDK，也称为 Eclipse Temurin 是一个开源 Java，成立于 2017 &gt; 经过对 OpenJDK 缺乏开源和测试系统的长时间讨论。 Eclipse Adoptium 小组承担这项任务的主要目标是生产高质量、经过 TCK 认证的技术和运行时，以便在 Java 生态系统中使用。自发布以来，AdoptOpenJDK 已经取得了巨大的发展，现在是基于 OpenJDK 的二进制文件的领先提供商，这些二进制文件可用于桌面、现代云平台、传统服务器、企业嵌入式系统，甚至大型机。这一成功是通过多个项目以及与外部项目（例如 OpenJDK）的密切合作来实现的，以提供所需的 Java SE 运行时实现。 AdoptOpenJDK 可以安装在各种平台上，例如 Windows、macOS、Linux 等。 方式一，下载二进制文件清华镜像源-Adoptium 下载对应的版本 解压 复制到对应的位置 设置环境变量 sudo vim /etc/profile 使配置生效并验证 设置默认 Java 版本如果系统上安装了多个 Java 版本，则需要设置运行 Java 应用程序时使用的默认 Java 版本。 首先，将 Temurin OpenJDK 21 添加到 &#x2F;usr&#x2F;bin&#x2F;java 路径。 列出可用的 Java 安装。 添加源，通过yum命令安装 将下列内容添加到 sudo vim /etc/yum.repos.d/adoptium.repo 再执行 安装软件包 "},{"title":"springboot容器镜像","date":"2024-04-20T16:00:00.000Z","url":"/2024/04/21/2024/04/212227/","tags":[["docker","/tags/docker/"],["springboot","/tags/springboot/"]],"categories":[["Java","/categories/Java/"]],"content":"Spring Boot 应用程序可以使用 Dockerfile 进行容器化，也可以使用 Cloud Native Buildpack 创建优化的 docker 兼容容器映像，这些映像可以在任何地方运行。 高效的容器镜像可以很容易地将 Spring Boot uber jar 打包为 docker 映像。 但是，像在 docker 映像中一样复制和运行 jar 有各种缺点。 在不拆包的情况下运行 uber jar 时，总会有一定的开销，在容器化环境中，这可能很明显。 另一个问题是，将应用程序的代码及其所有依赖项放在 Docker 映像中的一层中是次优的。 由于重新编译代码的频率可能比升级使用的 Spring Boot 版本的频率更高，因此通常最好将内容分开一点。 如果将 jar 文件放在应用程序类之前的层中，Docker 通常只需要更改最底层，就可以从缓存中获取其他文件。 对 Docker 镜像进行分层为了更轻松地创建优化的 Docker 映像，Spring Boot 支持向 jar 添加层索引文件。 它提供了层和应包含在其中的罐子部分的列表。 索引中的层列表根据层添加到 Docker&#x2F;OCI 映像的顺序进行排序。 开箱即用，支持以下层： dependencies（对于定期发布的依赖项） spring-boot-loader（对于下面的所有内容org&#x2F;springframework&#x2F;boot&#x2F;loader) snapshot-dependencies（对于快照依赖项） application（用于应用程序类和资源） 下面显示了一个文件示例：BOOT-INFO/layers.idx 此分层旨在根据代码在应用程序生成之间更改的可能性来分离代码。 库代码不太可能在构建之间更改，因此它被放置在自己的层中，以允许工具重用缓存中的层。 应用程序代码更有可能在生成之间更改，因此它被隔离在单独的层中。 Spring Boot 还支持借助 .layers.idx Maven打包分层jar 自定义图层配置 XML 格式分为三个部分：layers 该块定义了应用程序类和资源的分层方式。 该块定义了依赖项的分层方式。 该块定义了层的写入顺序。 Dockerfile使用分层功能来创建优化的 docker 映像 构建 docker 映像，也可以选择性地指定应用程序 jar 的路径，如以下示例所示：docker build . 这是一个多阶段 dockerfile。 构建器阶段提取稍后需要的目录。 每个COPY命令都与 jarmode 提取的层相关。 springboot3.*org.springframework.boot.loader.JarLauncher已经被弃用， 需要修改对应的配置 "},{"title":"Java虚拟线程","date":"2024-04-14T16:00:00.000Z","url":"/2024/04/15/2024/04/151653/","tags":[["Thread","/tags/Thread/"]],"categories":[["Java","/categories/Java/"]],"content":"Java的虚拟线程（Virtual Threads）是从Java 19开始引入的一项实验性功能，它们也被称作轻量级线程（Lightweight Threads）。虚拟线程旨在提高Java程序在处理大量并发任务时的性能，尤其是在IO密集型应用中。这种线程模型可以让开发者编写出高并发的应用，同时使用较少的操作系统资源。 核心特性 轻量级：虚拟线程相比传统的操作系统线程占用更少的内存，启动更快。 简化并发编程：虚拟线程可以大幅减少使用显式同步和复杂的并发控制结构的需要，使并发编程更接近传统的顺序编程。 提高性能：尤其是在IO密集型和多任务应用中，可以开启成千上万的虚拟线程，而不会像操作系统线程那样资源耗费巨大。 与现有API兼容：虚拟线程设计时考虑到与现有的java.lang.Thread API兼容，使得迁移现有代码变得简单。 使用场景虚拟线程特别适用于需要处理大量短暂任务的应用，例如服务器处理大量的客户端请求，每个请求都需要独立处理，但处理时间很短。 代码示例使用Executors.newVirtualThreadPerTaskExecutor() 使用Thread.startVirtualThread(Runnable) 使用Thread Builder 注意事项 虚拟线程的主要优势在于能够处理大量并发任务，对于计算密集型任务，传统的操作系统线程可能更适合。 虚拟线程的调度是由Java运行时管理的，不同于操作系统线程直接由操作系统调度。 虚拟线程的引入是Java在现代并发编程领域的一大进步，它让Java在微服务和云应用等场景下的表现更加强大和高效。 "},{"title":"MySQL连接错误次数过多，导致IP被blocked","date":"2024-04-10T16:00:00.000Z","url":"/2024/04/11/2024/04/111845/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["Linux","/categories/Linux/"]],"content":"message from server: “Host ‘10.10.14.129’ is blocked because of many connection errors; unblock with ‘mysqladmin flush-hosts’” 登录一下mysql控制台 执行一下 "},{"title":"win11开启ssh远程登录","date":"2024-04-08T16:00:00.000Z","url":"/2024/04/09/2024/04/091732/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 要在Windows 11上开启SSH远程登录，可以按照以下步骤进行操作： 安装OpenSSH服务器： 打开“设置”。 选择“应用”。 选择“可选功能”。 向下滚动并找到“添加功能”，然后点击“查看功能”。 在搜索框中输入“OpenSSH”，然后选中“OpenSSH服务器”，点击“下一步”，然后点击“安装”。 启动并设置OpenSSH服务器服务： 按Win + R，输入services.msc并按回车。 在服务列表中找到“OpenSSH SSH Server”。 右键点击它并选择“启动”。 再次右键点击它并选择“属性”。 将“启动类型”设置为“自动”，然后点击“确定”。 打开防火墙端口： 打开“控制面板”。 选择“系统和安全”。 选择“Windows Defender 防火墙”。 选择“高级设置”。 选择“入站规则”。 点击“新建规则”。 选择“端口”，然后点击“下一步”。 选择“TCP”，并在“特定本地端口”中输入“22”，然后点击“下一步”。 选择“允许连接”，然后点击“下一步”。 选择适用的网络类型（通常为“域”、“专用”和“公用”），然后点击“下一步”。 为规则命名，例如“SSH”，然后点击“完成”。 测试SSH连接： 从另一台计算机上，打开终端或命令提示符。 使用以下命令尝试连接到Windows 11计算机： 输入密码进行验证。 这样，你就应该能够在Windows 11上启用和配置SSH远程登录了。"},{"title":"sevenzipjbinding进行文件解压","date":"2024-04-06T16:00:00.000Z","url":"/2024/04/07/2024/04/071021/","tags":[["unzip","/tags/unzip/"]],"categories":[["Java","/categories/Java/"]],"content":"使用sevenzipjbinding文件解压 "},{"title":"s3fs挂载s3存储","date":"2024-04-02T16:00:00.000Z","url":"/2024/04/03/2024/04/031529/","tags":[["s3","/tags/s3/"],["ArchLinux","/tags/ArchLinux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"s3fs挂载s3存储 安装 配置 挂载 取消挂载 文件权限问题 参考文档 s3fs-fuse "},{"title":"WPS for Linux字体加粗问题","date":"2024-03-30T16:00:00.000Z","url":"/2024/03/31/2024/03/311455/","tags":[["wps","/tags/wps/"]],"categories":[["Linux","/categories/Linux/"]],"content":" WPS for Linux字体加粗问题,导致问题 1.文件显示黑块，2.文件导出pdf乱码 Arch方式解决版本 11.1.0.11704-1 与 freetype2 版本 2.13.1 和 2.13.2 兼容性不佳。通常，如果所选字体没有粗体版本，freetype2 会通过算法生成伪粗体（fakebold），但在这种情况下，wps-office 也自行将字体设为粗体，结果是文本看起来特别粗，标题、粗体文本几乎不可用。解决方案如下： 可以尝试安装otf-noto-sans-cjk来获得一个粗体字体，如果不生效， 降级 freetype2 软件包至 2.13.0 版本，你可以从 archlinuxcn 仓库或者 AUR 安装 downgrade，然后使用 downgrade 去降级 freetype2 软件包，示例如下： 这样就成功降级该软件包，重启 WPS 即可生效。 或者安装freetype2-wps，重启WPS即可生效。 注意： 另一种办法是在wps的desktop文件中添加旧的freetype2库文件位置的环境变量，这样可以无需降级系统的freetype2 有爆字体问题的，官方修复之前 可以用老版本 freetype2 而无需降级系统的 freetype2 下载  解包后使用 备用下载地址 wps 启动程序即即可。可以把环境变量加入到desktop file文件里面去 解决方案 其他发型版解决方式 如上 设置环境变量 下载“”，解压deb，将x86_64-linux-gnu里面的都拷贝到/opt/kingsoft/wps-office/office6 参考文档 wiki.archlinuxcn.org WPS for Linux字体加粗问题 "},{"title":"pdf添加权限","date":"2024-03-28T16:00:00.000Z","url":"/2024/03/29/2024/03/291049/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":"使用itext或openpdf为pdf添加权限 itext5 openpdf"},{"title":"visualvm分析内存溢出(OOM)问题","date":"2024-03-27T16:00:00.000Z","url":"/2024/03/28/2024/03/281827/","tags":[["tika","/tags/tika/"],["docx","/tags/docx/"]],"categories":[["Java","/categories/Java/"]],"content":"使用visual VM 分析内存溢出 下载安装visualvmvisualvm 设置java_home编辑 ./visualvm_218/etc/visualvm.conf,设置jdk的位置，注意替换成自己的地址 启动并加载.hprof文件File-&gt;Load,找到文件位置加载 问题线程 进入线程内部 找到对应的本地变量，可以将参数复制出来做进一步排查 问题原因使用tika解析一个5M左右的文件出现了内存溢出，但是这个文件有2600多页的文字。"},{"title":"sevenzipjbinding解析压缩包","date":"2024-03-25T16:00:00.000Z","url":"/2024/03/26/2024/03/262038/","tags":[["zip","/tags/zip/"],["rar","/tags/rar/"]],"categories":[["Java","/categories/Java/"]],"content":"解析压缩包目录结构 依赖 代码示例 TreeBuilder另一种树形结构展示 "},{"title":"用 GPG 对文件进行加密和解密","date":"2024-03-19T16:00:00.000Z","url":"/2024/03/20/2024/03/201005/","tags":[["gpg","/tags/gpg/"]],"categories":[["Linux","/categories/Linux/"]],"content":"用 GPG 对文件进行加密和解密 安装GPG 在基于 Debian 和 Ubuntu 的系统中，安装 gpg 包： 如果你使用 基于 Arch 的发行版，用 pacman 命令 安装 gnupg 软件包： 生成一个GPG密钥 系统会提示您输入密钥对的某些规格。当系统提示“请选择您想要的密钥类型”时，按“1”，然后按“Enter”，选择选项 1 RSA。提醒一下，RSA 是一种公钥加密系统，它使用非对称加密对数据进行加密。 对于这篇文章，我们接受默认的密钥大小为 3072 位。当提示“您想要什么密钥大小？”时，点击“Enter”，这会导致选择 3072 位。 接下来，系统会问您“密钥是否有效？在这个例子中，我们点击“Enter”，这意味着密钥永远不会过期。 现在，系统会提示您构建一个用户 ID 来标识您的密钥对。您必须提供： “实名”，我们使用 testuser 作为这个例子。 “电子邮件”，输入您希望与此密钥对关联的电子邮件。我们稍后在加密文件时会使用此电子邮件。在此示例中，我们使用 testuser@example.com。 验证您输入的信息，并接受键入O表示“确定”。 “密码”，确保你写下你的密码，这样你就不会忘记它。我们以后需要它。 创建密钥对后，gpg 会输出 “pub”、“uid” 和 “sub”。 导出私钥、公钥 导出私钥 gpg –output &lt;file name here&gt; –armor –export-secret-key &lt;email here&gt; 查看密钥 导出公钥 导入 删除 加密&#x2F;解密文件 加密文件夹下所有文件 "},{"title":"使用miniforge替代miniconda/anaconda","date":"2024-03-13T16:00:00.000Z","url":"/2024/03/14/2024/03/181739/","tags":[["anaconda","/tags/anaconda/"],["miniconda","/tags/miniconda/"],["miniforge","/tags/miniforge/"]],"categories":[["Python","/categories/Python/"]],"content":"鉴于Anaconda的版权问题，将使用miniforge替代 包含了conda和mamba等常用命令 安装miniforgeminiforge or Uninstallation Homebrew 配置.condarc 配置环境变量"},{"title":"k8s应用新的yaml报错：v1.LabelSelectorRequirement(nil)}:field is immutable","date":"2024-03-06T16:00:00.000Z","url":"/2024/03/07/2024/03/071017/","tags":[["k8s","/tags/k8s/"]],"categories":[["Java","/categories/Java/"]],"content":"k8s应用新的yaml报错：v1.LabelSelectorRequirement(nil)&#125;: field is immutable 需要对前端项目添加app.kubernetes.io/prometheus: enabled用来监控，于是在代码中添加了相关的label 发布报错 问题原因： 在 API 版本 apps&#x2F;v1 中，Deployment 标签选择算符在创建后是不可变的。 改变了spec.selector导致 解决方案修改deployment.yml模版只在需要的地方添加label 参考资料 deployment "},{"title":"特殊的日子","date":"2024-02-28T16:00:00.000Z","url":"/2024/02/29/2024/02/292333/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"今天快过完了，才想起是四年一遇的闰年二月二十九。 难得，如果今天出生，不想四年一过的话，最好过农历的生日。 因为服务器配置更改验证，导致现在还没睡觉。 实际上，因为茱丽叶的出生，我的陪产假结束，晚上应该是早睡的，不然中间醒来几次，第二天上班，肯定撑不住。 但是，也没得办法。 希望新的一月，顺顺利利。茱丽叶健康成长。"},{"title":"定时任务技术调研","date":"2024-02-17T16:00:00.000Z","url":"/2024/02/18/2024/02/0418462/","tags":[["job","/tags/job/"]],"categories":[["Java","/categories/Java/"]],"content":"定时任务技术调研 背景目前job-service因为使用的quartz调度，存在以下问题： 1、每次只能单副本运行，无法发挥集群分布式的优势，后期定时任务多起来以后单副本压力会很大 2、每隔十分钟需要从数据库中重新捞一遍定时任务数据加载到quartz中，在重新加载过程中有几率导致本次某个任务丢失不执行 需要考虑引入新的分布式调度组件（PowerJob、xxl-job等） NICE TO HAVE（重要）： 1、可以提高稳定性 2、可以提供服务性能 Quartz(集群) Scheduler:调度器 Trigger:触发器 Job:任务 类型 优点 缺点 RAMJobStore 不要外部数据库，配置容易，运行速度快 因为调度程序信息是存储在被分配给 JVM 的内存里面，所以，当应用程序停止运行时，所有调度信息将被丢失。另外因为存储到JVM内存里面，所以可以存储多少个 Job 和 Trigger 将会受到限制 JDBC 作业存储 支持集群，因为所有的任务信息都会保存到数据库中，可以控制事物，还有就是如果应用服务器关闭或者重启，任务信息都不会丢失，并且可以恢复因服务器关闭或者重启而导致执行失败的任务 运行速度的快慢取决与连接数据库的快慢 引入依赖 示例Job Job1 job2 DisallowConcurrentExecution 保证在多个JVM中有且仅有一个节点在执行，是以 JobDetail 为依据的 而 JobDetail 的唯一标识是 JobKey ，使用 name + group 两个属性。一般情况下，我们只需要设置 name 即可，而 Quartz 会默认 group = DEFAULT 。 配置文件 Quartz表结构 每个表都有一个 SCHED_NAME 字段，Quartz Scheduler 名字。这样，实现每个 Quartz 集群，数据层面的拆分 DataSourceConfiguration 定时任务配置 Bean自动设置 在 Quartz 调度器启动的时候，会根据该配置，自动调用如下方法： ​ Scheduler#addJob(JobDetail jobDetail, boolean replace) 方法，将 JobDetail 持久化到数据库。 Scheduler手动设置 集群方式运行 集群任务调度的原理在集群模式下，Quartz通过数据库行锁来确保同一个任务不会被多个节点同时执行。当一个调度器实例尝试执行一个任务时，它会先在数据库中对该任务加锁。如果加锁成功，该实例就会执行任务；否则，意味着有其他实例已经在执行该任务，当前实例就会放弃执行。 BTW，分布式部署时需要保证各个节点的系统时间一致。 Quartz数据库核心表如下： Table Name Description QRTZ_CALENDARS 存储Quartz的Calendar信息 QRTZ_CRON_TRIGGERS 存储CronTrigger，包括Cron表达式和时区信息 QRTZ_FIRED_TRIGGERS 存储与已触发的Trigger相关的状态信息，以及相联Job的执行信息 QRTZ_PAUSED_TRIGGER_GRPS 存储已暂停的Trigger组的信息 QRTZ_SCHEDULER_STATE 存储少量的有关Scheduler的状态信息，和别的Scheduler实例 QRTZ_LOCKS 存储程序的悲观锁的信息 QRTZ_JOB_DETAILS 存储每一个已配置的Job的详细信息 QRTZ_JOB_LISTENERS 存储有关已配置的JobListener的信息 QRTZ_SIMPLE_TRIGGERS 存储简单的Trigger，包括重复次数、间隔、以及已触的次数 QRTZ_BLOG_TRIGGERS Trigger作为Blob类型存储 QRTZ_TRIGGER_LISTENERS 存储已配置的TriggerListener的信息 QRTZ_TRIGGERS 存储已配置的Trigger的信息 一个scheduler实例在集群模式下首先获取{0}LOCKS表中的行锁； 向Mysql获取杭锁的语句： {0}会替换为配置文件默认配置的**QRTZ_**。sched_name为应用集群的实例名，lock_name就是行级锁名。Quartz主要由两个行级锁。 lock_name desc STATE_ACCESS 状态访问锁 TRIGGER_ACCESS 触发器访问锁 Quartz集群争用触发器行锁，锁被占用只能等待，获取触发器行锁之后，先获取需要等待触发的其他触发器信息。数据库更新触发器状态信息，及时是否触发器行锁，供其他调度实例获取，然后在进行触发器任务调度操作，对数据库操作就要先获取行锁。 同时执行的最大任务数在Quartz任务调度框架中，并没有直接定义每个执行器（Executor）的容量上限这一概念。Quartz主要关注的是任务调度和管理，而非线程池大小或并发执行任务的数量这样的执行细节。 不过，如果指的是在Quartz中执行作业时的并发处理能力，那么可以通过配置调度器（Scheduler）使用的线程池来间接控制并发执行任务的数量。Quartz内部可以配置ThreadPool来管理执行作业的线程资源，默认实现是org.quartz.simpl.SimpleThreadPool。 在SimpleThreadPool中，你可以设置如下的参数来控制线程池的行为： threadCount：线程池中的线程数量，这可视为“容量上限”之一。 threadPriority：线程的优先级。 makeThreadsDaemons：是否将线程设为守护线程。 threadKeepAliveTime：空闲线程等待新任务的时间，在此时间过后若无新任务则会终止线程。 配置示例： #java 通过调整这些参数，可以控制调度器能够同时执行的最大任务数，但这并不是对单个执行器的容量上限进行限制，而是整个调度器层面的并发控制。 Quartz处理耗时较长的任务会出现Handling 2 trigger(s) that missed their scheduled fire-time.任务会丢失，需要手动设置处理。 Quartz Scheduler 可能会面临一些挑战和问题，具体取决于任务的性质、Quartz Scheduler 的配置以及环境条件等因素。以下是一些可能出现的问题和解决方法： 任务执行超时： 如果任务的执行时间超过了预期，Quartz Scheduler 可能会超出预定的调度间隔，导致调度的不稳定性。可以通过调整任务的执行时间、调度间隔或者使用并行执行来解决这个问题。 资源竞争： 长时间运行的任务可能会导致资源（如线程、内存）的竞争和耗尽，影响其他任务的执行。可以通过限制任务的并发数量、合理分配资源或者使用异步执行等方式来解决。 数据库连接问题： 如果任务涉及到数据库操作，长时间的任务可能会导致数据库连接的泄漏或者超时，影响其他任务的执行。可以通过使用连接池、合理管理数据库连接、优化数据库操作等方式来解决。 任务重复执行： 如果任务在执行期间发生异常或者执行时间超过了预期，Quartz Scheduler 可能会重新调度该任务，导致任务的重复执行。可以通过合理捕获异常、设置任务的状态、使用分布式锁等方式来避免任务的重复执行。 任务状态管理： 长时间运行的任务可能需要额外的状态管理，以便在执行期间进行监控、中断或者重新调度。可以通过设置任务的状态、使用监控工具、合理设计任务的执行逻辑等方式来管理任务的状态。 集群故障转移每个服务器会定时（org.quartz.jobStore.clusterCheckinInterval这个时间）更新SCHEDULER_STATE表中的LAST_CHECK_TIME（将服务器的当前时刻更新为最后更新时刻）字段，遍历集群各兄弟节点的实例状态，检测集群各个兄弟节点的健康状态。 Quartz使用了一个随机的负载均衡算法，Job以随机的方式由不同的实例执行。 处理集群环境下的任务冲突和容错为了有效地在集群环境下管理任务调度，需要考虑到任务冲突和容错的问题。任务冲突通常发生在两个或两个以上的调度器实例尝试同时执行同一个任务的情况。如前所述，Quartz通过数据库锁机制来避免这种情况的发生。 容错机制是指当一个调度器实例失败时，集群中的其他实例能够接管未完成的任务。Quartz通过持续检查数据库中的锁和任务状态来实现这一点。如果一个实例在执行任务时失败，数据库中的锁将被释放，其他实例就可以接管并执行该任务。 通过部署Quartz到集群环境，能够提高任务调度的可靠性和可用性，确保即使在部分节点出现故障的情况下，任务调度也能正常进行。这对于构建高可用的大规模应用至关重要。 启动多个实例，本地启动 注意端口不能重复 XXL-JOB XXL-JOB 是一个轻量级分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。 初始化数据库 - xxl_job_lock：任务调度锁表； - xxl_job_group：执行器信息表，维护任务执行器信息； - xxl_job_info：调度扩展信息表： 用于保存XXL-JOB调度任务的扩展信息，如任务分组、任务名、机器地址、执行器、执行入参和报警邮件等等； - xxl_job_log：调度日志表： 用于保存XXL-JOB任务调度的历史信息，如调度结果、执行结果、调度入参、调度机器和执行器等等； - xxl_job_log_report：调度日志报表：用户存储XXL-JOB任务调度日志的报表，调度中心报表功能页面会用到； - xxl_job_logglue：任务GLUE日志：用于保存GLUE更新历史，用于支持GLUE的版本回溯功能； - xxl_job_registry：执行器注册表，维护在线的执行器和调度中心机器地址信息； - xxl_job_user：系统用户表 配置部署“调度中心”xxl-job-admin 统一管理任务调度平台上调度任务，负责触发调度执行，并且提供任务管理平台。 修改配置文件/xxl-job/xxl-job-admin/src/main/resources/application.properties,修改数据连接地址 调度中心访问地址： (该地址执行器将会使用到，作为回调地址) 默认登录账号 admin/123456, 登录后运行界面如下图所示。 调度中心集群调度中心支持集群部署，提升调度系统容灾和可用性。 调度中心集群部署时，几点要求和建议： DB配置保持一致； 集群机器时钟保持一致（单机集群忽视）； 建议：推荐通过nginx为调度中心集群做负载均衡，分配域名。调度中心访问、执行器回调配置、调用API服务等操作均通过该域名进行。 执行器项目引入依赖确认pom文件中引入了 “xxl-job-core” 的maven依赖； 配置文件 执行器组件，配置文件 执行器集群 执行器回调地址（xxl.job.admin.addresses）需要保持一致；执行器根据该配置进行执行器自动注册等操作。 同一个执行器集群内AppName（xxl.job.executor.appname）需要保持一致；调度中心根据该配置动态发现不同集群的在线执行器列表。 Job(BEAN) 架构设计思想将调度行为抽象形成“调度中心”公共平台，而平台自身并不承担业务逻辑，“调度中心”负责发起调度请求。 将任务抽象成分散的JobHandler，交由“执行器”统一管理，“执行器”负责接收调度请求并执行对应的JobHandler中业务逻辑。 因此，“调度”和“任务”两部分可以相互解耦，提高系统整体稳定性和扩展性； 系统组成 调度模块（调度中心）：负责管理调度信息，按照调度配置发出调度请求，自身不承担业务代码。调度系统与任务解耦，提高了系统可用性和稳定性，同时调度系统性能不再受限于任务模块；支持可视化、简单且动态的管理调度信息，包括任务新建，更新，删除，GLUE开发和任务报警等，所有上述操作都会实时生效，同时支持监控调度结果以及执行日志，支持执行器Failover。 执行模块（执行器）：负责接收调度请求并执行任务逻辑。任务模块专注于任务的执行等操作，开发和维护更加简单和高效；接收“调度中心”的执行请求、终止请求和日志请求等。 架构图 自研调度模块XXL-JOB最终选择自研调度组件（早期调度组件基于Quartz）；一方面是为了精简系统降低冗余依赖，另一方面是为了提供系统的可控度与稳定性； XXL-JOB中“调度模块”和“任务模块”完全解耦，调度模块进行任务调度时，将会解析不同的任务参数发起远程调用，调用各自的远程执行器服务。这种调用模型类似RPC调用，调度中心提供调用代理的功能，而执行器提供远程服务的功能。 当调度中心启动后，会启动以下两个线程： schedulerThread scheudlerThread主要做如下两件事情： 从数据中心（db），也就是 表中扫描出符合 条件 1的任务， 条件1 限制如下： 任务执行时间 小于（当前时间 + 5 s） 限制扫描个数， 这个值是动态的，会根据后面的提到的 快慢线程池 中线程数量有关系。 ringThread ringThread的作用就是不断从 容器 中读取 当前时间点需要执行 的任务， 读取出来的任务会交给一个叫 快慢线程池 的东西去将任务传递给调度器去执行。 快慢线程池 调度中心HA（集群）基于数据库的集群方案，数据库选用Mysql；集群分布式并发环境中进行定时任务调度时，会在各个节点会上报任务，存到数据库中，执行时会从数据库中取出触发器来执行，如果触发器的名称和执行时间相同，则只有一个节点去执行此任务。 调度线程池调度采用线程池方式实现，避免单线程因阻塞而引起任务调度延迟。 并行调度XXL-JOB调度模块默认采用并行机制，在多线程调度的情况下，调度模块被阻塞的几率很低，大大提高了调度系统的承载量。 XXL-JOB的不同任务之间并行调度、并行执行。 XXL-JOB的单个任务，针对多个执行器是并行运行的，针对单个执行器是串行执行的。同时支持任务终止。 过期处理策略任务调度错过触发时间时的处理策略： 可能原因：服务重启；调度线程被阻塞，线程被耗尽；上次调度持续阻塞，下次调度被错过； 处理策略： 过期超5s：本次忽略，当前时间开始计算下次触发时间 过期5s内：立即触发一次，当前时间开始计算下次触发时间 任务HA（Failover）执行器如若集群部署，调度中心将会感知到在线的所有执行器，如“127.0.0.1:9997, 127.0.0.1:9998, 127.0.0.1:9999”。 当任务”路由策略”选择”故障转移(FAILOVER)”时，当调度中心每次发起调度请求时，会按照顺序对执行器发出心跳检测请求，第一个检测为存活状态的执行器将会被选定并发送调度请求。 均衡调度调度中心在集群部署时会自动进行任务平均分配，触发组件每次获取与线程池数量（调度中心支持自定义调度线程池大小）相关数量的任务，避免大量任务集中在单个调度中心集群节点 故障转移 &amp; 失败重试一次完整任务流程包括”调度（调度中心） + 执行（执行器）”两个阶段。 “故障转移”发生在调度阶段，在执行器集群部署时，如果某一台执行器发生故障，该策略支持自动进行Failover切换到一台正常的执行器机器并且完成调度请求流程。 “失败重试”发生在”调度 + 执行”两个阶段，支持通过自定义任务失败重试次数，当任务失败时将会按照预设的失败重试次数主动进行重试； 执行器灰度上线调度中心与业务解耦，只需部署一次后常年不需要维护。但是，执行器中托管运行着业务作业，作业上线和变更需要重启执行器，尤其是Bean模式任务。执行器重启可能会中断运行中的任务。但是，XXL-JOB得益于自建执行器与自建注册中心，可以通过灰度上线的方式，避免因重启导致的任务中断的问题。 步骤如下： 1、执行器改为手动注册，下线一半机器列表（A组），线上运行另一半机器列表（B组）； 2、等待A组机器任务运行结束并编译上线；执行器注册地址替换为A组； 3、等待B组机器任务运行结束并编译上线；执行器注册地址替换为A组+B组；操作结束； 避免任务重复执行调度密集或者耗时任务可能会导致任务阻塞，集群情况下调度组件小概率情况下会重复触发；针对上述情况，可以通过结合 “单机路由策略（如：第一台、一致性哈希）” + “阻塞策略（如：单机串行、丢弃后续调度）” 来规避，最终避免任务重复执行。 PowerJob部署调度中心（docker版） 注册应用 登录Username:应用名称 password：应用密码 官方处理器PowerJob 支持 Python、Shell、HTTP、SQL 等众多通用任务的处理，只需要引入依赖，在控制台配置好相关参数即可 处理器（Processor）开发 引入依赖，最新稳定版本为4.3.8，对应的spring boot版本为2.7.18 启动服务 观察日志 管理界面 配置任务 演示地址 单机处理器：BasicProcessor 广播处理器：BroadcastProcessor 架构 Server 启动流程分析 集群 无锁化调度的实现经典调度原理 重复调度为了解决重复调度，当前开源调度框架的解决方案：锁 select for update 分布式锁 本质上只有一台 server 在真正提供服务！集群部署只获得了 高可用能力，并没有获取性能的提升，不具备水平扩展性 无锁化调度 引入分组依据 AppName，以应用集群作为 server 调度的单位。 每一个 worker 集群在运行时只会连接到某一台 server。 每一个 server 实例只会调度当前与自己保持心跳的 worker 关联的 AppName 下的所有任务。 时区next_trigger_time worker 因为没办法获取 server 的准确状态，所以不能由 worker 来决定连接哪一台 server。因此，worker 需要做的，只是服务发现。即定时使用 HTTP 请求任意一台 server，请求获取当前该分组（appName）对应的 server。 而 server 收到来自 worker 的服务发现请求后，其实就是进行了一场小型的分布式选主：server 依赖的数据库中存在着 server_info 表，其中记录了每一个分组（appName）所对应的 server 信息。如果该 server 发现表中存在记录，那就说明该 worker 集群中已经有别的 worker 事先请求 server 进行选举，那么此时只需要发送 PING 请求检测该 server 是否存活。如果发现该 server 存活，那么直接返回该 server 的信息作为该分组的 server。否则就完成篡位，将自己的信息写入数据库表中，成为该分组的 server。 细心的小伙伴可能又要问了？发送 PING 请求检测该 server 是否存活，不还是有和刚才一样的问题吗？请求不同，发送方和接收方都有可能出问题，凭什么认为是原先的 server 挂了呢？ 确实，在这个方案下，依旧没办法解决 server 到底挂没挂这个堪比“真假美猴王”的玄学问题。但是，这还重要吗？我们的目标是某个分组下所有的 worker 都连接到同一台 server，因此，即便产生那种误打误撞篡位的情况，在服务发现机制的加持下，整个集群最终还是会连接到同一台 server，完美实现我们的需求。 任务配置 可靠调度——WAL WAL（Write-Ahead Logging，预写式日志），这是主流关系型数据库（MS SQLServer、MySQL、Oracle）用来确保了事务原子性和持久性的关键技术。WAL 的核心思想是：**在数据写入到数据库之前，先写入到日志中。**这样，在硬盘数据不损坏的情况下，预写式日志允许存储系统在崩溃后能够在日志的指导下恢复到崩溃前的状态，避免数据丢失。 PowerJob 为了实现任务的可靠调度，也借鉴了该思想。每一个任务被调度执行时，系统都会为其生成一条记录，这条记录包含了该任务实例（任务的一次运行叫任务实例）的预期调度时间。之后，PowerJob 会首先将该记录持久化到数据库中，只有持久化成功后，该任务才会被正式推入时间轮进行调度。 一旦这一台 server 宕机，任务没有被准时执行。其他 server 就能根据已经写入数据库中的任务实例记录将其恢复，做到可靠调度～ 也就是说，只要你的系统中还有一台 powerjob-server 活着，就不会有缺失调度的情况。 秒级任务 每一个秒级任务，都会直接被投递到集群中的某一台 powerjob-worker 上，由 powerjob-worker 全权负责执行。而 powerjob-server 此时只需要负责故障恢复即可。 这样一来，server 的压力进一步减轻，同时，由于秒级任务的调度与执行全部落在了 worker 身上，调度的精度也会上升（至少能省下通讯的网络延迟） Apache DolphinScheduler主要面向大数据处理领域，其核心需求是按照规定流程（ DAG ）跑一堆脚本去完成一些数据任务。 试用地址 容错设计服务容错设计依赖于ZooKeeper的Watcher机制，实现原理如图 其中Master监控其他Master和Worker的目录，如果监听到remove事件，则会根据具体的业务逻辑进行流程实例容错或者任务实例容错。 Master容错流程 容错范围：从host的维度来看，Master的容错范围包括：自身host+注册中心上不存在的节点host，容错的整个过程会加锁； 容错内容：Master的容错内容包括：容错工作流实例和任务实例，在容错前会比较实例的开始时间和服务节点的启动时间，在服务启动时间之后的则跳过容错； 容错后处理：ZooKeeper Master容错完成之后则重新由DolphinScheduler中Scheduler线程调度，遍历 DAG 找到”正在运行”和“提交成功”的任务，对”正在运行”的任务监控其任务实例的状态，对”提交成功”的任务需要判断Task Queue中是否已经存在，如果存在则同样监控任务实例的状态，如果不存在则重新提交任务实例。 Worker容错流程 容错范围：从工作流实例的维度看，每个Worker只负责容错自己的工作流实例；只有在handleDeadServer时会加锁； 容错内容：当发送Worker节点的remove事件时，Master只容错任务实例，在容错前会比较实例的开始时间和服务节点的启动时间，在服务启动时间之后的则跳过容错； 容错后处理：Master Scheduler线程一旦发现任务实例为” 需要容错”状态，则接管任务并进行重新提交。 注意：由于” 网络抖动”可能会使得节点短时间内失去和ZooKeeper的心跳，从而发生节点的remove事件。对于这种情况，我们使用最简单的方式，那就是节点一旦和ZooKeeper发生超时连接，则直接将Master或Worker服务停掉。 负载均衡 预热 考虑到 JIT 优化，会让 worker 在启动后低功率的运行一段时间，使其逐渐达到最佳状态，这段过程称之为预热。 随机（加权） 在符合的 worker 中随机选取一台（权重会影响他的比重） 平滑轮询（加权） 加权轮询算法一个明显的缺陷。即在某些特殊的权重下，加权轮询调度会生成不均匀的实例序列，这种不平滑的负载可能会使某些实例出现瞬时高负载的现象，导致系统存在宕机的风险。为了解决这个调度缺陷，我们提供了平滑加权轮询算法。 每台 worker 都有两个权重，即 weight（预热完成后保持不变），current_weight（动态变化），每次路由。都会遍历所有的 worker，使其 current_weight+weight，同时累加所有 worker 的 weight，计为 total_weight，然后挑选 current_weight 最大的作为本次执行任务的 worker，与此同时，将这台 worker 的 current_weight-total_weight。 线性加权(默认算法) 该算法每隔一段时间会向注册中心上报自己的负载信息。我们主要根据两个信息来进行判断 load 平均值（默认是 CPU 核数 *2） 可用物理内存（默认是 0.3，单位是 G） 如果两者任何一个低于配置项，那么这台 worker 将不参与负载。（即不分配流量） 可以在 worker.properties 修改下面的属性来自定义配置 worker.max.cpu.load.avg&#x3D;-1 (worker最大cpu load均值，只有高于系统cpu load均值时，worker服务才能被派发任务. 默认值为-1: cpu cores * 2) worker.reserved.memory&#x3D;0.3 (worker预留内存，只有低于系统可用内存时，worker服务才能被派发任务，单位为百分比) Elastic-Job 支持分布式部署；不同节点上执行的是不一样的任务(代码是同一套)；对于一个大任务，可以用分片策略，让他在多节点上执行； 能够保证高可用； 利用zk实现分布式环境管理； 水平扩展（核心） 快速开始 分片ElasticJob 中任务分片项的概念，使得任务可以在分布式的环境下运行，每台任务服务器只运行分配给该服务器的分片。 随着服务器的增加或宕机，ElasticJob 会近乎实时的感知服务器数量的变更，从而重新为分布式的任务服务器分配更加合理的任务分片项，使得任务可以随着资源的增加而提升效率。 任务的分布式执行，需要将一个任务拆分为多个独立的任务项，然后由分布式的服务器分别执行某一个或几个分片项。 当新增加作业服务器时，ElasticJob 会通过注册中心的临时节点的变化感知到新服务器的存在，并在下次任务调度的时候重新分片，新的服 务器会承载一部分作业分片 高可用当作业服务器在运行中宕机时，注册中心同样会通过临时节点感知，并将在下次运行时将分片转移至仍存活的服务器，以达到作业高可用的效果。 本次由于服务器宕机而未执行完的作业，则可以通过失效转移的方式继续执行。 将分片总数设置为 1，并使用多于 1 台的服务器执行作业，作业将会以 1 主 n 从的方式执行。 一旦执行作业的服务器宕机，等待执行的服务器将会在下次作业启动时替补执行。开启失效转移功能效果更好，如果本次作业在执行过程中宕机，备机会立即替补执行。 实现原理 ElasticJob 并无作业调度中心节点，而是基于部署作业框架的程序在到达相应时间点时各自触发调度。注 册中心仅用于作业注册和监控信息存储。而主作业节点仅用于处理分片和清理等功能。 弹性分布式实现 • 第一台服务器上线触发主服务器选举。主服务器一旦下线，则重新触发选举，选举过程中阻塞，只 有主服务器选举完成，才会执行其他任务。 • 某作业服务器上线时会自动将服务器信息注册到注册中心，下线时会自动更新服务器状态。 • 主节点选举，服务器上下线，分片总数变更均更新重新分片标记。 • 定时任务触发时，如需重新分片，则通过主服务器分片，分片过程中阻塞，分片结束后才可执行任 务。如分片过程中主服务器下线，则先选举主服务器，再分片。 • 通过上一项说明可知，为了维持作业运行时的稳定性，运行过程中只会标记分片状态，不会重新分 片。分片仅可能发生在下次任务触发前。 • 每次分片都会按服务器 IP 排序，保证分片结果不会产生较大波动。 • 实现失效转移功能，在某台服务器执行完毕后主动抓取未分配的分片，并且在某台服务器下线后主 动寻找可用的服务器执行任务。 错过任务重执行ElasticJob 不允许作业在同一时间内叠加执行。 当作业的执行时长超过其运行间隔，错过任务重执行能够保证作业在完成上次的任务后继续执行逾期的作业。 作业启动 作业执行 ej失效转移ElasticJob 不会在本次执行过程中进行重新分片，而是等待下次调度之前才开启重新分片流程。 当作业执行过程中服务器宕机，失效转移允许将该次未完成的任务在另一作业节点上补偿执行。 线程池策略线程池策略，用于执行作业的线程池创建。 SPI 名称 详细说明 JobExecutorThreadPoolSizeProvider 作业执行线程数量提供策略 已知实现类 详细说明 CPUUsageJobExecutorThreadPoolSizeProvider 根据 CPU 核数 * 2 创建作业处理线程池 SingleThreadJobExecutorThreadPoolSizeProvider 使用单线程处理作业 设置合理的超时时间 运维平台配置 启动 浏览器打开 ，用户名/密码：root/root 连接zookeeper 作业开发ElasticJob 目前提供 Simple、Dataflow 这两种基于 class 的作业类型，并提供 Script、HTTP 这两种基于 type 的作业类型，用户可通过实现 SPI 接口自行扩展作业类型。 http zookeeper数据 对比 特性 quartz elastic-job-lite xxl-job PowerJob 依赖 MySQL、jdk jdk、zookeeper mysql、jdk MySQL、JDK、MongoDB(可选) 高可用 多节点部署，通过竞争数据库锁来保证只有一个节点执行任务 通过zookeeper的注册与发现，可以动态的添加服务器 基于竞争数据库锁保证只有一个节点执行任务，支持水平扩容。可以手动增加定时任务，启动和暂停任务，有监控 任务分片 × √ √ MapReduce动态分片 管理界面 × √ √ √ 难易程度 简单 简单 简单 简单 高级功能 - 弹性扩容，多种作业模式，失效转移，运行状态收集，多线程处理数据，幂等性，容错处理，spring命名空间支持 弹性扩容，分片广播，故障转移，Rolling实时日志，GLUE（支持在线编辑代码，免发布）,任务进度监控，任务依赖，数据加密，邮件报警，运行报表，国际化 DAG工作流 版本更新 半年没更新 2023-10-14 3.0.4 最近有更新 最近有更新 任务分配细节调研比较 项目\\方案 qz Xxl-job p-j e-j d-s 任务分配的方式如何保证公平 集群任务调度的原理 均衡调度 任务配置 分片 负载均衡 每个执行器的容量上限 同时执行的最大任务数 线程池 线程池 线程池策略 负载状况决定,和硬件相关 当pod重启时被分配的任务会怎么处理（如果有任务正在执行时会怎么样） 处理集群环境下的任务冲突和容错 过期处理策略,灰度上线 可靠调度WAL 高可用 容错设计 长事务的任务执行会不会有问题 Quartz处理耗时较长的任务 避免任务重复执行 设置合适的间隔时间和运行时间限制 超时时间 设置超时时间 参考资料 PowerJob elastic-job xxl-job DolphinScheduler PowerJob源码分析-分组隔离设计 "},{"title":"当爸爸了","date":"2024-02-02T16:00:00.000Z","url":"/2024/02/03/2024/02/02031500/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":"闺女出生，当爸爸了。"},{"title":"大文件流式下载","date":"2024-01-24T16:00:00.000Z","url":"/2024/01/25/2024/01/251331/","tags":[["nginx","/tags/nginx/"]],"categories":[["Java","/categories/Java/"]],"content":"流下载大文件 背景持续连接的问题：对于非持续连接，浏览器可以通过连接是否关闭来界定请求或响应实体的边界；而对于持续连接，这种方法显然不奏效。有时，尽管我已经发送完所有数据，但浏览器并不知道这一点，它无法得知这个打开的连接上是否还会有新数据进来，只能傻傻地等了。 用Content-length解决：计算实体长度，并通过头部告诉对方。浏览器可以通过 Content-Length 的长度信息，判断出响应实体已结束 Content-length引入的新问题：由于 Content-Length 字段必须真实反映实体长度，但是对于动态生成的内容来说，在内容创建完之前，长度是不可知的。这时候要想准确获取长度，只能开一个足够大的 buffer，等内容全部生成好再计算。但这样做一方面需要更大的内存开销，另一方面也会让客户端等更久。 我们需要一个新的机制：不依赖头部的长度信息，也能知道实体的边界——分块编码（Transfer-Encoding: chunked） 分块编码（Transfer-Encoding: chunked）Transfer-Encoding，是一个 HTTP 头部字段（响应头域），字面意思是「传输编码」。最新的 HTTP 规范里，只定义了一种编码传输：分块编码(chunked)。 分块传输编码（Chunked transfer encoding）是超文本传输协议（HTTP）中的一种数据传输机制，允许HTTP由网页服务器发送给客户端的数据可以分成多个部分。分块传输编码只在HTTP协议1.1版本（HTTP&#x2F;1.1）中提供。 数据分解成一系列数据块，并以一个或多个块发送，这样服务器可以发送数据而不需要预先知道发送内容的总大小。 具体方法 在头部加入 Transfer-Encoding: chunked 之后，就代表这个报文采用了分块编码。这时，报文中的实体需要改为用一系列分块来传输。 每个分块包含十六进制的长度值和数据，长度值独占一行，长度不包括它结尾的 CRLF(\\r\\n)，也不包括分块数据结尾的 CRLF。 最后一个分块长度值必须为 0，对应的分块数据没有内容，表示实体结束。 代码片段 nginx配置 参考资料 分块编码 Transfer-Encoding "},{"title":"PDF签名证书","date":"2024-01-21T16:00:00.000Z","url":"/2024/01/22/2024/01/221326/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":"pdf签名证书 生成签名证书，保存为keystore.p12 签名代码片段-itext实现方式 OpenPDF实现pdf签名方式 "},{"title":"Helm模版配置多个端口映射","date":"2024-01-13T16:00:00.000Z","url":"/2024/01/14/2024/03/081541/","tags":[["k8s","/tags/k8s/"]],"categories":[["Java","/categories/Java/"]],"content":"通过将端口映射的配置定义为列表，并在values.yaml中指定该列表，然后在模板中遍历这个列表。以下是一个示例 values.yaml service.yaml"},{"title":"GraalVM","date":"2024-01-13T16:00:00.000Z","url":"/2024/01/14/2024/04/102109/","tags":[["GraalVM","/tags/GraalVM/"]],"categories":[["Java","/categories/Java/"]],"content":"GraalVM 会提前将 Java 应用程序编译为独立的二进制文件，这些二进制文件可立即启动，无需预热即可提供最佳性能，并且使用更少的资源。 安装步骤下载地址 idea方式安装 Linux 选择对应的版本和平台，下载GraalVM 然后将.tar.gz文件移动到要安装的目录 解压缩 配置JAVA_HOME 将环境变量的值设置为 GraalVM bin 目录：PATH 验证 macOS 下载 移除隔离属性（macOS Catalina 及更高版本需要） 解压 移动到jvm路径 设置环境变量 验证 Windows 下载 移动到你想要安装的目录，并解压 设置环境变量 命令方式设置 通过 Windows GUI 设置环境变量： 转到 Windows“开始”菜单，然后转到“设置”，然后转到“高级”。 单击环境变量。在“系统变量”部分，找到该变量并选择它。JAVA_HOME 单击编辑。 单击“新建”。 单击“浏览”以查找要添加的目录。单击“确定”进行确认。 重新启动命令提示符以重新加载环境变量。 重新启动终端，并执行java -version验证 Docker在此处查看 GraalVM Community Edition 容器映像的完整列表。 拉取镜像 或者，若要将容器映像用作 Dockerfile 中的基础映像，请使用: 要使用特定 JDK 功能版本（例如 22）的实用程序拉取容器映像，请运行：native-image 验证 在不指定处理器体系结构的情况下进行调用会拉取与 Docker 客户端匹配的处理器体系结构的容器映像。若要为其他平台体系结构拉取容器映像，请使用选项指定所需的平台体系结构，如下所示：docker pull--platformlinux/amd64linux/aarch64 开始运行应用程序 使用 Spring Boot 和 GraalVM 构建原生镜像如何使用 Spring Boot 和 GraalVM 构建原生镜像应用。 原生镜像原生（本地）镜像是一种将 Java 代码构建为独立可执行文件的技术。该可执行文件包括应用程序类、其依赖项的类、运行时库类以及来自 JDK 的静态链接本地代码。JVM 被打包到原生镜像中，因此在目标系统上不需要任何 Java 运行环境，但构建产物依赖于平台。因此，需要为每个支持的目标系统进行一次构建，在使用 Docker 等容器技术时会更加简单，将容器构建为一个目标系统，可以部署到任何 Docker 运行时。 原生镜像具有各种优势，如即时启动和减少内存消耗。它们可以打包成轻量级的容器镜像，以实现更快、更高效的部署，并减少攻击面。 限制由于采用了 “闭环优化”，在编写应用代码和使用框架时必须注意一些 限制。简而言之： 类初始化器可以在构建时执行，以实现更快的启动和更好的性能峰值。但必须意识到，这可能会破坏代码中的一些假设，例如，在加载文件时，该文件必须在构建时可用。 反射和动态代理在运行时成本很高，因此在 “闭环优化” 假设下，在构建时进行了优化。在构建时执行时，可以不受限制地在类初始化器中使用。任何其他用途都必须向 AOT 编译器公布，Native Image builder 会尝试通过执行静态代码分析来达到这一目的。如果分析失败，就必须通过 配置文件 等方式提供相关信息。 这同样适用于所有基于反射的技术，如 JNI 和序列化（Serialization） 此外，Native Image builder 还提供了自己的原生接口，比 JNI 简单得多，开销也更低。对于原生镜像构建，字节码在运行时不再可用，因此无法使用针对 JVMTI 的工具进行调试和监控。因此，必须使用本地调试器和监控工具。 对于 Spring Boot，运行时不再完全支持 配置文件、条件 Bean 和 .enable 属性等功能。如果使用配置文件，则必须在构建时指定。 使用 GraalVM native-image"},{"title":"spring.config.location多个文件","date":"2024-01-13T16:00:00.000Z","url":"/2024/01/14/2024/04/191342/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"需要指定多个配置文件时，可以使用逗号分隔它们的路径，无论是通过命令行参数还是在应用程序的配置文件中使用属性来指定。 JVM参数指定 配置文件 在应用程序的配置文件中指定了一个主配置文件 config1.properties 和两个额外的配置文件 config2.properties 和 config3.properties。"},{"title":"powershell","date":"2024-01-13T16:00:00.000Z","url":"/2024/01/14/2024/05/051545/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"生成自签名证书 "},{"title":"XWPFTemplate渲染特殊字符","date":"2024-01-13T16:00:00.000Z","url":"/2024/01/14/2024/05/311007/","tags":[["deepoove","/tags/deepoove/"],["poi","/tags/poi/"]],"categories":[["Java","/categories/Java/"]],"content":"poi-tl（poi template language）是Word模板引擎，使用模板和数据创建很棒的Word文档。 下是一些常见的支持特殊字符的字体： Cambria Math：专门用于数学和科学文本，支持大量的数学符号。 Arial Unicode MS：一个包含大量 Unicode 字符的字体，适用于多种语言和符号。 Segoe UI Symbol：支持广泛的符号和特殊字符，包括数学符号。 更新模板文件以使用合适的字体 确保你的模板文件中使用这些字体。例如，您可以在 Word 模板文件中为占位符设置所需的字体。 参考文档 poi-tl "},{"title":"Pdf添加页眉页脚","date":"2024-01-11T16:00:00.000Z","url":"/2024/01/12/2024/01/121012/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":" 代码片段 "},{"title":"新年快乐","date":"2023-12-30T16:00:00.000Z","url":"/2023/12/31/2023/12/312051/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"2023即将结束 总结 新房验收，准备装修 疫情结束，但与之共存 升职但没有加薪 开始健身，小有成绩 买了新电脑 结婚、买车、装修 没有存到钱&#x2F;(ㄒoㄒ)&#x2F;~~ 学会了路冲板 展望&amp;&amp;计划 身体健康 照顾好家人 认识新盆友 升职不重要，加薪多来点 培养新的兴趣爱好 学会一种新的语言 "},{"title":"使用yagmail发送邮件","date":"2023-12-24T16:00:00.000Z","url":"/2023/12/25/2023/12/251741/","tags":[["email","/tags/email/"]],"categories":[["Python","/categories/Python/"]],"content":"python使用yagmail发送邮件 创建邮箱授权秘钥163邮箱客户端授权码 发送邮件 参考文档 yagmail "},{"title":"Python压缩文件夹为zip","date":"2023-12-24T16:00:00.000Z","url":"/2023/12/25/2023/12/251758/","tags":[["zip","/tags/zip/"]],"categories":[["Python","/categories/Python/"]],"content":"对文件夹进行压缩，以便更方便地传输或存储。在Python中，我们可以使用zipfile模块来压缩文件夹为zip格式。 导入zipfile模块 压缩文件夹 参考文档 python-zipfile "},{"title":"spring boot读取配置文件中的中文值乱码","date":"2023-11-30T16:00:00.000Z","url":"/2023/12/01/2023/12/011500/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":"spring boot在读取properties类型的配置文件时，如果值中包含中文，则会出现乱码 创建配置文件 读取测试 启动测试 原因编码有问题 实际上字符的编码还是iso-8859-1 解码方式是：用ISO-8859-1（又叫Latin-1）编码保存，然后以UTF-8编码读取,读取的配置是不正确的 早期版本的spring boot 读取配置的类是org.springframework.boot.context.config.ConfigFileApplicationListener,已过时， 已弃用since 2.4.0 for removal in 2.6.0 in favor of ConfigDataEnvironmentPostProcessor 所以这里我们看下org.springframework.boot.context.config.ConfigDataEnvironmentPostProcessor配置类中的读取方式 查看org.springframework.core.env.ConfigurableEnvironment,其中 getPropertySources() 获取配置文件org.springframework.core.env.MutablePropertySources是PropertySources的默认实现,由此可以找到PropertiesPropertySourceLoader 而org.springframework.boot.env.PropertiesPropertySourceLoader是Strategy to load &#39;.properties&#39; files into a PropertySource. loadProperties方法 org.springframework.boot.env.OriginTrackedPropertiesLoader 其中的load()方法中使用了静态内部类的CharacterReader，而这个类的实现是使用ISO_8859_1编码读取文件的。 解决方案 将中文进行编码 使用yaml格式的配置文件 参考文档 SpringBoot使用@Value读取.properties中文乱码及解决方法 "},{"title":"Java获取字体宽高","date":"2023-11-20T16:00:00.000Z","url":"/2023/11/21/2023/11/211737/","tags":[["font","/tags/font/"]],"categories":[["Java","/categories/Java/"]],"content":"Java获取字体宽高 获取字体宽高"},{"title":"conda的清理","date":"2023-11-16T16:00:00.000Z","url":"/2023/11/17/2023/11/171756/","tags":[["conda","/tags/conda/"]],"categories":[["Python","/categories/Python/"]],"content":"conda清理命令 "},{"title":"使用openpdf给PDF添加水印或文本","date":"2023-11-09T16:00:00.000Z","url":"/2023/11/10/2023/11/101553/","tags":[["openpdf","/tags/openpdf/"]],"categories":[["Java","/categories/Java/"]],"content":"使用openpdf替换itext给PDF添加水印和文本填充 依赖 添加水印 添加文本内容 参考资料 OpenPDF "},{"title":"勾号、叉号、圈号等unicode编码","date":"2023-11-08T16:00:00.000Z","url":"/2023/11/09/2023/11/091053/","tags":[["pdf","/tags/pdf/"],["watermark","/tags/watermark/"]],"categories":[["Java","/categories/Java/"]],"content":" 支持特殊字符的字体Unicode中包含了许多不同样式的符号，包括勾号、叉号和圈号。不同的字体和操作系统可能对这些符号的显示效果有所不同，但下面是一些常见的字体，它们通常支持这些符号： Arial Unicode MS 这是一种广泛使用的字体，包含了大量的Unicode字符，包括勾号（✓）、叉号（✗）和圈号（⦿）。 Segoe UI Symbol 这是一种由微软开发的字体，支持许多Unicode符号，包括勾号（✓）、叉号（✗）和圈号（⦿）。 Noto Sans Symbols 这是一套由Google开发的字体，旨在支持Unicode中的各种符号，包括勾号（✓）、叉号（✗）和圈号（⦿）。 请注意，这些字体的可用性可能因不同的操作系统和应用程序而有所不同。如果您在特定的环境中无法找到上述字体，您可以尝试在网上搜索其他支持Unicode符号的字体。 Unicode包含有多种与其相关的符号,需要符号字体支持可以使用FontCreator.exe将segoeuisl.ttf和simfang.ttf合并获取到的&#x2F;&#x2F;☑☐☒●◯◉ 符号 Unicode码(Hex) 说明 ✓ U+2713 CHECK MARK（勾号） ✔ U+2714 HEAVY CHECK MARK（粗勾号） ☐ U+2610 BALLOT BOX (方格) ☑ U+2611 BALLOT BOX WITH CHECK（打勾方格） ☒ U+2612 BALLOT BOX WITH X (带交叉方格) ✗ U+2717 BALLOT X (交叉) ✘ U+2718 HEAVY BALLOT X (粗体交叉) × U+00D7 乘法符号 ⨯ U+2A2F 向量积 ◉ U+25c9 圆圈带点（单选选中按钮） ○ U+25CB WHITE CIRCLE ● U+25CF BLACK CIRCLE ◯ U+25EF LARGE CIRCLE "},{"title":"wsl2中安装MySQL8","date":"2023-11-02T16:00:00.000Z","url":"/2023/11/03/2023/11/031825/","tags":[["wsl","/tags/wsl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"wsl2安装MySQL8并开启远程连接 wls2（Ubuntu）安装MySQL 修改配置 注释掉本机绑定 赋权 重启 连接查看wsl的ip "},{"title":"数据建模 – 表继承","date":"2023-11-01T16:00:00.000Z","url":"/2023/11/02/2023/10/021638/","tags":[["hibernate","/tags/hibernate/"]],"categories":[["Java","/categories/Java/"]],"content":" 类型 MappedSuperclass 继承仅在域模型中实现，而不反映在数据库架构中。 SingleTable 域模型类层次结构具体化为单个表，该表包含属于不同类类型的实体。 Joined table 基类和所有子类都有自己的数据库表，获取子类实体也需要与父表联接。 Table per class 每个子类都有自己的表，其中包含子类和基类属性 MappedSuperclass继承仅在域模型中可见，并且每个数据库表都包含基类和子类属性。 SQL 由于继承模型未在数据库级别进行镜像， @MappedSuperclass在通过其基类获取持久性对象时，不能使用引用@MappedSuperclass 的多态查询。 SingleTable单表继承策略将所有子类映射到一个数据库表。 每个子类都声明自己的持久属性。 SQL 层次结构中的每个子类都必须定义一个唯一的鉴别器值，该值用于区分属于不同子类类型的行。 如果未指定，则DTYPE该列将用作鉴别器，存储关联的子类名称。 保存 SQL 使用多态查询时，只需扫描单个表即可获取所有关联的子类实例。 查询 鉴别器 @DiscriminatorColumn Example @DiscriminatorFormula Example 单表继承无法很好地扩展，因为： 根据子实体的属性数量，将有许多字段。NULL 添加一个新类会使其难以维护。 Joined table每个子类也可以映射到其自己的表。 这也称为每个子类的表映射策略。 通过与超类的表联接来检索继承的状态。 此映射策略不需要鉴别器列。 但是，每个子类都必须声明一个包含对象标识符的表列。 则假定主键&#x2F;外键列与超类的主表的主键列具有相同的名称。@PrimaryKeyJoinColumn sql 查询 使用多态查询时，必须将基类表与所有子类表联接，才能提取每个关联的子类实例。 联接表继承多态查询可以使用多个 JOIN，这可能会影响提取大量实体时的性能。 Table per class仅将继承层次结构的具体类映射到表。 这称为“每个具体类的表”策略。 每个表都定义类的所有持久状态，包括继承的状态。 sql 优点 子实体将具有单独的表，并且不会存储其他子实体的属性。 子实体的必填字段将通过约束强制执行。NOT NULL 缺点 多态查询需要多个 UNION 查询，因此请注意大型类层次结构对性能的影响。 隐式和显式多态性(已过期)默认情况下，查询基类实体时， 多态查询将获取属于基类型的所有子类。 可以针对接口进行查询， Hibernate 将只获取那些被映射的实体，或者它们根本没有被注解的实体（暗示 IMPLICIT 行为）：DomainModelEntity@Polymorphism(type &#x3D; PolymorphismType.IMPLICIT)@Polymorphism 实体被标记了注解，这指示 Hibernate 在对非映射基类执行多态查询时跳过它。"},{"title":"使用python调用高德地图接口获取公交线路信息","date":"2023-10-29T16:00:00.000Z","url":"/2023/10/30/2023/10/302245/","tags":[["amapm","/tags/amapm/"]],"categories":[["Python","/categories/Python/"]],"content":"使用python调用高德地图接口获取公交线路信息 申请高德key开放平台 新建应用 添加key,取个名称，选择Web服务 代码示例 请将 Your-API-Key 替换为你自己的高德地图 Web 服务 API 密钥。另外，请注意将 公交线路名称 替换为你要查询的实际公交线路名称。运行代码后，将会返回查询到的公交线路名称和类型。 通过以上代码，你可以根据需要自己调整添加其他参数，例如 city 参数指定城市名称，output 参数指定返回的数据格式（例如 JSON 或 XML）等。详细的参数列表和返回数据格式可以参考高德地图开放平台的公交线路查询接口文档。"},{"title":"清理dns缓存","date":"2023-09-27T16:00:00.000Z","url":"/2023/09/28/2023/09/281612/","tags":[["dns","/tags/dns/"]],"categories":[["Linux","/categories/Linux/"]],"content":"刷新dns缓存 Linux Windows 打开命令提示符（CMD）或 PowerShell。 输入以下命令后按回车键： Mac 打开终端。 输入以下命令后按回车键： Chrome&#x2F;Edge "},{"title":"Linux终端快捷键","date":"2023-09-18T16:00:00.000Z","url":"/2023/09/19/2023/09/191652/","tags":[["termanal","/tags/termanal/"]],"categories":[["Linux","/categories/Linux/"]],"content":"终端快捷键！ 概览 快捷键 功能描述 Ctrl+A 光标快速跳至行首 Ctrl + E 光标快速跳至行尾。 Ctrl + U 删除光标至行首的所有内容。 Ctrl + K 删除光标至行尾的所有内容。 Ctrl + W 删除光标前的一个单词。 Ctrl + L 清空整个终端屏幕。 Ctrl + C 停止正在执行的进程或命令。 Ctrl + D 注销或退出终端。 Ctrl + Z 暂停正在执行的进程（之后可恢复执行）。 Ctrl + R 在命令历史中进行逆向搜索。 ↑ 从命令历史中显示先前的命令。 ↓ 从命令历史中显示后续的命令。 !! 重复执行最近的命令。 !n 重复执行命令历史中的第 n 条命令。 Tab 自动补全命令，文件名或目录名。 连续按 Tab 两次 列出所有可能的补全选项。 Ctrl + Shift + C 复制所选文本或命令。 !Ctrl + Shift + V 粘贴已复制的文本或命令。 Ctrl + Shift + N 打开新的终端窗口。 Ctrl + Shift + T 在当前终端中打开新的选项卡。 CCtrl + Tab 或 Ctrl + PageDown 在终端的选项卡之间切换。 "},{"title":"PIX path validation failed","date":"2023-09-12T16:00:00.000Z","url":"/2023/09/13/2023/09/131736/","tags":[["maven","/tags/maven/"]],"categories":[["Java","/categories/Java/"]],"content":"maven打包出现以下错误： sun.security.validator.ValidatorException: PIX path validation failed: 解决方案maven仓库地址ssl证书更新"},{"title":"Mac 使用终端格式化U盘","date":"2023-08-13T16:00:00.000Z","url":"/2023/08/14/2023/08/141706/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"diskutil 查看U盘的IDENTIFIER 记住u盘的disk名称，这里显示是&#x2F;dev&#x2F;disk4 取消 U 盘 挂载 使用0覆盖所有扇区 格式化U盘"},{"title":"linux文件压缩归档和解压的命令","date":"2023-08-03T16:00:00.000Z","url":"/2023/08/04/2023/08/041420/","tags":[["tar","/tags/tar/"]],"categories":[["Linux","/categories/Linux/"]],"content":"tar.gz,zip, linux把文件压缩成.tar.gz的命令 tar格式 gz格式 bz2格式 tar.bz2格式 bz格式 tar.bz格式 Z格式 tar.Z格式 tgz格式 tar.tgz格式 zip格式 lha格式 7z多线程压缩 "},{"title":"Malformed \\uxxxx encoding","date":"2023-07-24T16:00:00.000Z","url":"/2023/07/25/2023/07/251007/","tags":[["maven","/tags/maven/"]],"categories":[["Java","/categories/Java/"]],"content":"maven3.8.1版本之后会将http的资源过滤掉,需要在配置文件中修改仓库镜像地址 更新项目依赖的jar包的时，可能由于网络问题导致下载的jar包不完整 方案一：maven打包时打印错误文件信息 方案二：resolver-status.properties）在.&#x2F;m2&#x2F;文件夹（根据自己情况修改）下，找到path-to-the-library，然后删掉（若无此文件，可直接忽略此步骤）；（2）在.&#x2F;m2&#x2F;repository （根据自己情况修改）文件夹下全局搜索:resolver-status.properties 文件，将搜索到的所有此文件全部删除，然后重新编译即可。 参考文档Malformed \\uxxxx encoding问题的多种完美解决方法总结 "},{"title":"ffmpeg转码遇到编码错误如何跳过，而不影响正常播放","date":"2023-07-24T16:00:00.000Z","url":"/2023/07/25/2023/07/251406/","tags":[["ffmpeg","/tags/ffmpeg/"],["GPT","/tags/GPT/"]],"categories":[["Linux","/categories/Linux/"]],"content":"要在FFmpeg转码时跳过编码错误，你可以使用-err_detect参数结合ignore_err选项。这样，当遇到编码错误时，FFmpeg将会忽略错误并继续处理。 示例 在上面的命令中，我们添加了-err_detect ignore_err参数来告诉FFmpeg在遇到编码错误时忽略错误。这样，即使出现编码错误，FFmpeg也会继续转码并生成输出文件。 请注意，跳过编码错误可能会导致转码结果出现问题或媒体内容损坏。因此，在使用此选项时，请务必进行测试，并确保跳过编码错误不会对最终播放产生明显的质量问题。"},{"title":"hibernate","date":"2023-07-19T16:00:00.000Z","url":"/2023/07/20/2023/07/200956/","tags":[["hibernate","/tags/hibernate/"]],"categories":[["Java","/categories/Java/"]],"content":"hibernate学习笔记 级联关系类型： CascadeType.REFRESH：级联刷新，当多个用户同时作操作一个实体，为了用户取到的数据是实时的，在用实体中的数据之前就可以调用一下refresh()方法 CascadeType.REMOVE：级联删除，当调用remove()方法删除Order实体时会先级联删除OrderItem的相关数据 CascadeType.MERGE：级联更新，当调用了Merge()方法，如果Order中的数据改变了会相应的更新OrderItem中的数据 CascadeType.ALL：包含以上所有级联属性 CascadeType.PERSIST：级联保存，当调用了Persist() 方法，会级联保存相应的数据 session管理 二级缓存数据缓存的作用：缓存位于程序和数据库之间，可减少程序访问数据库频率 什么是Hibernate的二级缓存（与一级缓存对比）Hibernate中提供了两个级别的缓存: 一级缓存 是session级别的缓存，它是属于事务范围的缓存,生命周期是session的生命周期， （一个线程 绑定一个Session， 对应一份一级缓存， 一级缓存无法实现多用户之间数据共享）,它是hibernate的内置缓存,由hibernate管理的,一般情况下无需进行干预. 二级缓存 是sessionFactory 级别的缓存，它属于进程级别的缓存 （一个项目 只会对应一个SessionFactory对象， sessionFactory缓存数据 实现多用户之间共享 ），二级缓存是可插拔的。(解耦合思想) 二级缓存的并发策略 从概念上说： read-write策略：缓存数据既能读也能写(比如”经常”更新的数据) read-only策略：缓存数据一般只用来读。(比如系统参数，地区的分类)，并发效率高！ HQL QBCNativeSQL配置 命名策略hibernate.implicit_naming_strategy（例如 （默认值）、、、、、defaultjpalegacy-jpalegacy-hbmcomponent-path)用于指定要使用的隐式命名策略类。 为此设置定义了以下短名称： default 使用 ImplicitNamingStrategyJpaCompliantImpl jpa 使用 ImplicitNamingStrategyJpaCompliantImpl legacy-jpa 使用 ImplicitNamingStrategyLegacyJpaImpl legacy-hbm 使用 ImplicitNamingStrategyLegacyHbmImpl component-path 使用 ImplicitNamingStrategyComponentPathImpl 如果此属性恰好为空，则回退是使用该策略。default hibernate.physical_naming_strategy（例如 （默认值））org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl用于指定要使用的物理命名策略类。 语句记录和统计 hibernate.show_sql（例如 或（默认值））truefalse 将所有 SQL 语句写入控制台。这是将日志类别设置为调试的替代方法。org.hibernate.SQL hibernate.format_sql（例如 或（默认值））truefalse 在日志和控制台中漂亮地打印 SQL。 hibernate.highlight_sql（例如 或（默认值））truefalse 使用 ANSI 转义代码在控制台中为 SQL 着色。 hibernate.use_sql_comments（例如 或（默认值））truefalse 如果为 true，Hibernate 会在 SQL 中生成注释，以便于调试。 状态"},{"title":"使用jsonrpc调用aria2进行文件下载","date":"2023-07-13T16:00:00.000Z","url":"/2023/07/14/2023/07/101725/","tags":[["aria2","/tags/aria2/"]],"categories":[["Java","/categories/Java/"]],"content":"Java通过jsonrpc调用aria2进行文件下载 motrix设置生成授权密钥 代码"},{"title":"common-io升级后文件复制权限问题","date":"2023-07-13T16:00:00.000Z","url":"/2023/07/14/2023/07/241027/","tags":[["file","/tags/file/"]],"categories":[["Java","/categories/Java/"]],"content":"版本发布后，发现部分文件读取出现权限问题，排查之后发现是因为common-io版本升级导致 common-io版本以及代码实现 升级前 升级后 解决方案 降级版本 系统之前需要实现的功能需要新版本，但是后来功能迁移到了新的项目中，所以这里选择降级版本。 设置文件权限 请注意，上述代码假设你的操作系统是基于类Unix的操作系统（如Linux、macOS）。如果你使用的是Windows操作系统，可以考虑使用Files.setAttribute()方法来设置文件权限。具体实现可能会有所不同，取决于你的操作系统。 复制到流 此时会使用文件默认的文件权限 "},{"title":"hibernate实现软删除","date":"2023-07-13T16:00:00.000Z","url":"/2023/07/14/2023/10/191651/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"软删除 Hibernate可以使用自定义SQL进行CRUD操作。 可以在语句级别或单个列级别重写 SQL. 对于集合，Hibernate允许定义一个自定义，用于删除与给定父实体关联的所有子记录。 为了过滤集合，注释允许自定义基础 SQL WHERE 子句。@SQLDeleteAll@Where 单表记录软删除 log 关联关系 关系表 新建关系 ToOne软删除标记生效，但是中间表的相关注释需要放到实体类上 恢复关系新建关系记录即可 参考资料 hibernate adasd "},{"title":"word合并","date":"2023-07-13T16:00:00.000Z","url":"/2023/07/14/2023/12/201754/","tags":[["word","/tags/word/"]],"categories":[["Java","/categories/Java/"]],"content":"word合并 deepoove pom 代码示例"},{"title":"pdf根据关键词获取坐标","date":"2023-07-13T16:00:00.000Z","url":"/2023/07/14/2023/11/241540/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":"根据给定的字符查询在pdf中对应的坐标 局限：只能查询单个字符 RenderListener"},{"title":"spring boot 3.*升级","date":"2023-07-13T16:00:00.000Z","url":"/2023/07/14/2023/12/281649/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"升级到 spring boot 3.*和jdk17 去除swagger-maven-plugin依赖 javax -&gt;jakartaidea-&gt;重构-&gt;迁移软件包和类-&gt;Java EE to Jakarta EE commons-fileupload替换为jakarta的实现 openfeign swagger迁移到springdoc删除 springfox 和 swagger 2 依赖项。改为添加依赖项。springdoc-openapi-starter-webmvc-ui 无需其他配置 这将自动将 swagger-ui 部署到 spring-boot 应用程序： 访问地址 request 对于 HTML 格式的 swagger 文档的自定义路径，请在 spring-boot 配置文件中添加自定义 springdoc 属性：。 将 swagger 2 注释替换为 swagger 3 注释（它已包含在依赖项中）。 swagger3注释的包是 .springdoc-openapi-starter-webmvc-uiio.swagger.v3.oas.annotations @Api→@Tag @ApiIgnore→或或@Parameter(hidden &#x3D; true)@Operation(hidden &#x3D; true)@Hidden @ApiImplicitParam→@Parameter @ApiImplicitParams→@Parameters @ApiModel→@Schema @ApiModelProperty(hidden &#x3D; true)→@Schema(accessMode &#x3D; READ_ONLY) @ApiModelProperty→@Schema @ApiOperation(value &#x3D; “foo”, notes &#x3D; “bar”)→@Operation(summary &#x3D; “foo”, description &#x3D; “bar”) @ApiParam→@Parameter @ApiResponse(code &#x3D; 404, message &#x3D; “foo”)→@ApiResponse(responseCode &#x3D; “404”, description &#x3D; “foo”) 此步骤是可选的：仅当您有多个 bean 时，才将它们替换为 bean。DocketGroupedOpenApi 以前： 现在： 如果你只有一个 - 删除它，而是将属性添加到你的 ：Docketapplication.properties 添加类型的 bean。请参阅示例：OpenAPI actuator集成 fastjson升级到fastjson2 参考资料 springdoc "},{"title":"合并多个PDF","date":"2023-06-28T16:00:00.000Z","url":"/2023/06/29/2023/06/251108/","tags":[["gs","/tags/gs/"]],"categories":[["Linux","/categories/Linux/"]],"content":"要使用Ghostscript合并多个PDF文件，可以按照以下步骤进行操作： 使用Ghostscript1. 安装Ghostscript：首先，确保已经安装了Ghostscript。你可以从Ghostscript官方网站（）下载并安装适用于你的操作系统的版本。2. 打开命令提示符或终端：在电脑上打开命令提示符（Windows）或终端（Mac和Linux）。3. 使用命令合并PDF文件：在命令提示符或终端中，使用以下命令将多个PDF文件合并为一个： 这个命令使用Ghostscript的pdfwrite设备将文件file1.pdf、file2.pdf和file3.pdf合并为一个名为merged.pdf的文件。 注意：在命令中将file1.pdf、file2.pdf和file3.pdf替换为你要合并的实际文件名，可以包括路径信息。 4. 运行命令：按下Enter键运行命令。Ghostscript将根据提供的文件列表合并PDF文件，并生成一个新的合并后的文件merged.pdf。5. 等待合并完成：合并文件的时间将取决于PDF文件的大小和数量。完成后，你将在相同的目录中找到生成的merged.pdf文件。请确保在执行此操作之前备份好你的PDF文件，以免意外丢失数据。此外，也可以在Ghostscript的文档中查找其他可用的选项和参数。 pdfbox itext或者openpdf "},{"title":"debian换源异常","date":"2023-06-25T16:00:00.000Z","url":"/2023/06/26/2023/06/141700/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"The following signatures couldn‘t be verified because the public key is not available https问题需要换成http源先安装以下依赖 添加公钥更换三方源没有对应的Key "},{"title":"Docker 技术分享","date":"2023-06-19T16:00:00.000Z","url":"/2023/06/20/2023/06/191726/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Docker 技术分享[TOC] 什么是 Docker Docker 架构 Docker 安装 Docker 镜像 Docker 容器 Docker Compose Docker 网络 Docker 数据管理 Docker 与持续集成&#x2F;持续部署 Docker 安全性 Docker 最佳实践 常见问题与解决方法 参考资料 1. 什么是 DockerDocker 是一种容器化平台，用于轻松打包、发布和运行应用程序。它能够在独立的、隔离的环境中运行应用程序，提供了更高的灵活性、可移植性和效率。 2. Docker 架构Docker 架构包括以下核心组件： Docker 客户端：与 Docker 服务器进行通信，通过命令行或图形界面管理 Docker。Docker 服务器：负责构建、运行和分发 Docker 容器。Docker 镜像：一个可执行软件包，包括运行应用程序所需的一切，如代码、运行时、库、环境变量等。Docker 容器：从 Docker 镜像创建的运行实例。 3. Docker 安装如何安装 Docker，并提供适用于不同操作系统的安装指南。 WindowsDocker Desktop on Windows WSL2 添加用户组 MacDocker Desktop on Mac LinuxUbuntu Centos ArchLinux 开机自启动 创建组并添加用户 4. Docker 镜像本节介绍 Docker 镜像的创建、管理和使用方法。涵盖以下主题： 拉取现有镜像 构建自定义镜像 FROM —设置容器基础镜像 LABEL —明确镜像的元数据信息的键值对 RUN —在基础镜像中执行指令并创建一个新层 CMD —容器启动后先执行的命令 EXPOSE —设置访问容器的端口,容器将会监听端口 MAINTAINER —显示镜像创建作者的信息 ENV —— 用来设置环境变量的键值对。这些变量在镜像创建的时候设置，并在容器创建好后可以使用。 COPY —— 用来拷贝本地文件至容器中。 ADD —— 具有与拷贝相同的功能，不过更进一步还可以提取本地的 tar 文件或者从 URL 拷贝文件。 ENTRYPOINT —— 用来设置镜像的主要命令。与 CMD 指令功能相同。不同的是 ENTRYPOINT 中的指令不会被重写。 VOLUME —— 该指令用来创建指定位置的挂载点。 USER —— 将设置运行镜像并使用的用户名称以及用户组。 WORKDIR —— 这会设置工作目录。如果目录不存在，则会创建。 Demo 使用 Dockerfile 创建 Docker 镜像 创建一个dockerfile文件 写入以下内容 :wq保存并退出 构建 Dockerfile创建jdk镜像 导出和导入 镜像仓库和注册表 docker hub Docker Hub 是 Docker 提供的一项服务，用于查找和共享容器镜像。 它是世界上最大的容器映像存储库，拥有一系列内容源，包括容器社区开发人员、开源项目和独立软件供应商 （ISV），在容器中构建和分发其代码。 自定义 Harbor Nexus 加速器 登录&amp;&amp;推送 5. Docker 容器讨论 Docker 容器的创建、启动、停止、删除等操作。包括以下内容： 进入容器 删除 其他命令 6. Docker Compose介绍 Docker Compose 工具，它允许通过一个 YAML 文件定义和管理多个 Docker 容器的服务。包括以下主题： 安装和配置 Docker Compose（安装docker时一起安装） 编写 Compose 文件 启动和管理 Compose 服务 nginx-wsgi-flask docker-compose 启动和管理 7. Docker 网络讨论 Docker 网络模型和网络配置。主题包括： Docker 网络类型创建自定义网络连接容器到网络 bridge 停止并移除网络 host 使用主机网络进行联网直接连接到 Docker 主机的网络，没有网络隔离 overlay macvlan这种类型的网络中，Docker 主机接受 请求在其 IP 地址处的多个 MAC 地址，并路由这些请求到相应的容器. 大多数云提供商阻止网络。需要物理访问到网络设备。 8. Docker 数据管理介绍 Docker 中的数据管理技术，包括： 挂载主机目录到容器数据卷和容器间共享数据 挂载 挂载成功后，容器从 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 目录下读取或写入数据，实际上都是从宿主机的 nginx-vol 数据卷中读取或写入数据。因此 Volumes 或 Bind mounts 也可以看作是容器和宿主机共享文件的一种方式。 -v 参数和 –mount 参数总的来说功能几乎相同，唯一的区别是在运行一个 service 时只能够 –mount 参数来挂载数据卷。 9. Docker 与持续集成&#x2F;持续部署Docker 在持续集成和持续部署（CI&#x2F;CD）流程中发挥着重要作用。以下是与 Docker 相关的一些关键概念和实践： 镜像构建与推送：使用 Docker 镜像可以轻松地构建和打包应用程序。结合 CI 工具（如 Jenkins、Travis CI、GitLab CI 等），可以自动化构建镜像并将其推送到 Docker 镜像仓库，以供部署使用。 容器化测试环境：Docker 提供了隔离和可重复的容器环境，使得在不同阶段进行单元测试、集成测试和端到端测试变得更加方便。可以在每个测试阶段使用不同的容器镜像，以确保环境的一致性。 持续部署与容器编排：借助容器编排工具（如 Kubernetes、Docker Swarm 等），可以将 Docker 容器作为应用程序的部署单位进行自动化部署和扩展。通过编排工具的配置文件，可以定义应用程序的整体架构、服务依赖关系和扩展规则。 版本控制与容器标签：使用 Docker 镜像标签可以轻松跟踪和管理应用程序的不同版本。结合版本控制系统（如 Git），可以在每个代码提交或版本发布时创建和标记对应的 Docker 镜像，以便追踪和回滚。 持久化数据管理：在 CI&#x2F;CD 流程中，需要考虑应用程序的持久化数据管理。可以使用 Docker 数据卷或云存储服务，将数据从容器中分离出来，以便在不同环境之间进行共享和保留。 10. Docker 安全性Docker 提供了一些功能和最佳实践，以增强容器环境的安全性。以下是一些关键点： 基础镜像安全性：选择来自可信源的基础镜像，并定期更新以获取最新的安全修复程序和补丁。 容器隔离：确保每个容器都在其自己的隔离环境中运行，限制容器之间的访问权限，以减轻潜在的攻击风险。 资源限制：对容器应用程序进行适当的资源限制，如 CPU、内存和存储，以防止资源耗尽和拒绝服务攻击。 容器映像验证：验证从外部源拉取的容器映像的完整性和真实性，以确保它们未被篡改。 容器漏洞扫描：使用容器漏洞扫描工具对容器映像进行扫描，以识别潜在的安全漏洞和软件包依赖性问题。 安全审计和监控：实施日志记录、监控和警报机制，以及容器的安全审计策略，以及对容器运行时进行实时监控，以检测和响应安全事件。 访问控制：使用适当的访问控制机制（如命名空间、用户命名空间、容器用户权限等）来限制容器的特权访问，以减少潜在的攻击面。 安全更新和漏洞修复：定期更新和升级容器中的软件包和依赖项，以修复已知的安全漏洞，并及时应用安全补丁。 11. Docker 最佳实践以下是一些 Docker 的最佳实践，可以提高开发和部署过程的效率和可靠性： 精简镜像：构建尽可能小而精简的镜像，只包含应用程序运行所需的最小依赖项，以减小镜像大小并提高部署效率。 容器重用：利用容器的可重用性，避免在每次部署时创建新的容器，而是重复使用现有的容器实例。 使用 Docker Compose：使用 Docker Compose 管理多个容器的服务，定义容器之间的依赖关系和网络配置，简化多容器应用程序的部署和管理。 安全配置：遵循安全最佳实践，对 Docker 守护程序和容器进行适当的安全配置，以减少潜在的攻击风险。 监控和日志记录：实施监控和日志记录机制，以便对容器的运行状况、性能和安全事件进行监控和分析。 自动化部署和集成：结合 CI&#x2F;CD 工具，实现自动化的镜像构建、测试和部署流程，提高开发团队的工作效率和持续集成能力。 12. 常见问题与解决方法本节列举一些常见的 Docker 问题，并提供相应的解决方法和建议。 容器无法启动或运行慢：检查容器配置、资源限制和宿主机性能，确保容器的要求得到满足。 镜像拉取失败：确认网络连接正常，检查镜像源是否可访问，并尝试使用其他镜像仓库。 容器间通信问题：确保容器在同一网络中，并正确配置容器之间的网络连接和端口映射。 持久化数据丢失：使用数据卷或云存储服务来管理容器中的持久化数据，并进行备份和恢复策略的规划。 安全漏洞和容器映像管理：定期更新容器映像和依赖项，使用容器漏洞扫描工具来检测潜在的安全漏洞。 资源管理和容器性能调优：合理分配宿主机资源，并监控容器的资源使用情况，进行性能调优和优化。 多环境部署：使用环境变量、配置文件或容器标签等机制来管理多个部署环境（如开发、测试、生产环境）的配置和参数。 容器监控和日志记录：使用监控工具和日志记录机制来收集和分析容器的运行状况、日志和性能数据。 13. 参考资料在文档的最后提供一些参考资料，供大家进一步学习和深入了解 Docker 技术： Docker 官方文档： Hub： 官方文档： Mastery: The Complete Toolset From a Docker Captain（书籍）Docker Deep Dive（书籍）"},{"title":"linux下使用dpkg安装的软件的卸载方法","date":"2023-06-11T16:00:00.000Z","url":"/2023/06/12/2023/06/121718/","tags":[["ubuntu","/tags/ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":"dpkg是Debian发行版操作系统类（ubuntu，deepin等）的包管理工具. 查询安装的软件包 根据名称卸载"},{"title":"ed25519签名验证","date":"2023-05-25T16:00:00.000Z","url":"/2023/05/26/2023/05/261044/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":" bouncycastle 创建服务器私钥"},{"title":"mongodb配置中的特殊字符处理","date":"2023-05-25T16:00:00.000Z","url":"/2023/05/26/2023/05/261612/","tags":[["mongodb","/tags/mongodb/"]],"categories":[["Java","/categories/Java/"]],"content":"SpringBoot-mongodb集群环境配置 spring.data.mongodb.uri 密码中存在:或者@特殊字符注意：username和password中含有:或@需要进行URLEncoder编码  curl "},{"title":"linux下解压iso文件","date":"2023-05-15T16:00:00.000Z","url":"/2023/05/16/2023/05/161515/","tags":[["iso","/tags/iso/"]],"categories":[["Linux","/categories/Linux/"]],"content":"linux下解压iso文件 mount 挂载 7z 解压 解压到指定目录 "},{"title":"js选择文件后自动上传","date":"2023-05-15T16:00:00.000Z","url":"/2023/05/16/2023/05/161558/","tags":[["js","/tags/js/"]],"categories":[["congco","/categories/congco/"]],"content":"选择文件后自动上传文件 form表单上传注意这里使用的Flask作为后端服务 ajax 方式上传"},{"title":"查看Mac上已连接WiFi的密码","date":"2023-04-29T16:00:00.000Z","url":"/2023/04/30/2023/04/301246/","tags":[["M1","/tags/M1/"]],"categories":[["congco","/categories/congco/"]],"content":"查看Mac上已连接WiFi的密码, Mac查看wifi密码的方法 创建服务器私钥 打开启动台 找到“其他”文件夹，打开“钥匙串访问”。 打开“钥匙串访问”后，在其左侧的“钥匙串”列表中选择“系统”，右侧栏就会出现与系统有关的各类密钥。 找到你需要连接的WiFi名称，右击，选择“将密码拷贝到剪贴板”。 这时候，系统会弹出提示框，让你输入本地用户的密码，输入之后回车即可 现在密码已经被复制到剪贴板了，随便打开一个可以输入文本内容的文本框，使用快捷键“Command V”就可以看到WiFi密码了。 "},{"title":"audio音频流支持跳转播放","date":"2023-04-22T16:00:00.000Z","url":"/2023/04/23/2023/04/231700/","tags":[["audio","/tags/audio/"]],"categories":[["Java","/categories/Java/"]],"content":"音频不是实时流，而是后台将文件转为流传回浏览器,音频可以播放，但是无法快进，即无法跳转某个播放位置继续播放 测试页面 添加响应头"},{"title":"Java中将输入流转成json字符串","date":"2023-04-17T16:00:00.000Z","url":"/2023/04/18/2023/04/031800/","tags":[["openssl","/tags/openssl/"]],"categories":[["Java","/categories/Java/"]],"content":"第三方回调接口 参数是以二进制的方式返回的。这里需要将流转换为我们需要的json "},{"title":"Django中配置mysql数据库","date":"2023-03-29T16:00:00.000Z","url":"/2023/03/30/2023/03/301740/","tags":[["MySQL","/tags/MySQL/"],["django","/tags/django/"]],"categories":[["Python","/categories/Python/"]],"content":"Django默认使用的数据库为sqlite. 默认配置 常见数据库引擎配置 安装mysqlclient 创建数据库 修改数据库配置配置文件 mysite/settings.py "},{"title":"生成自签pip/conda导出 requirements.txt","date":"2023-03-27T16:00:00.000Z","url":"/2023/03/28/2023/03/280942/","tags":[["pip","/tags/pip/"],["conda","/tags/conda/"]],"categories":[["Python","/categories/Python/"]],"content":"将当前环境的安装包依赖信息导出 pip&#x2F;conda提供了生成 requirements.txt 的功能，可以方便开发者在新的环境下进行一步式的依赖项安装。 pip pipreqs使用pipreqs，这个工具的好处是可以通过对项目目录的扫描，发现使用了哪些库，生成依赖清单。 重新安装依赖 conda 导出 导入安装 yaml文件方式 安装 "},{"title":"spring boot3.*集成Neo4j","date":"2023-03-25T16:00:00.000Z","url":"/2023/03/26/2023/03/201356/","tags":[["Neo4j","/tags/Neo4j/"],["spring boot","/tags/spring-boot/"]],"categories":[["Java","/categories/Java/"]],"content":"spring boot使用neo4j实现树形文件夹结构 neo4j介绍Neo4j是一种图形数据库，是一种非关系型数据库类型。它使用图形结构来存储数据，并提供了一种高效的方式来查询和操作这些数据。以下是Neo4j的一些特点和优点： 图形数据模型：Neo4j使用图形模型来存储数据，这种模型非常适合表达实体之间的关系。相比于传统的关系型数据库，图形模型可以更轻松地处理复杂的关系数据。 高性能：由于Neo4j使用图形模型，它能够快速地处理复杂的查询和操作，这使得它非常适合于处理大量和复杂数据。 可扩展性：Neo4j可轻松扩展，可以通过添加更多的服务器节点来提高读写吞吐量。 灵活性：Neo4j提供了灵活的数据存储方案，可以根据需要自定义数据结构。 开源：Neo4j是一款开源软件，可以免费使用和修改。 支持多种语言：Neo4j支持多种编程语言，包括Java、Python、Ruby、Scala等。这使得开发者可以使用他们熟悉的语言来操作Neo4j数据库。 可视化工具：Neo4j提供了一些可视化工具，可以帮助用户更轻松地查询和操作数据。 总的来说，Neo4j是一款强大的数据库，适合处理复杂的数据和关系，尤其适用于社交网络、网络安全、生物信息学和推荐系统等领域。 docker安装neo4j Mac上安装neo4j 默认情况下，Neo4j 的用户名和密码为 neo4j和neo4j 。但是，它要求更改新的帐户密码。因此，请运行以下命令： 浏览器打开ip:7474输入用户名密码进入 创建spring boot工程 定义简单实体 Neo4jRepository 配置文件 测试 参考资料 spring-neo4j filepath-neo4j "},{"title":"gnome 始终是 x11 启动，如何以 wayland 启动？","date":"2023-03-24T16:00:00.000Z","url":"/2023/03/25/2023/03/252007/","tags":[["nvidia","/tags/nvidia/"],["gnome","/tags/gnome/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 日志 解决方案修改配置文件，没有则新建 内核模块参数 在某些笔记本电脑上，要启用任何 NVIDIA 设置调整，您必须包含此选项，否则它会响应“不支持设置应用程序时钟”等。 挂起后保留视频内存 重新生成initramfs GDM强制启用 Wayland 参考资料 arch wiki bbs.archlinux "},{"title":"使用curl请求websocket","date":"2023-03-07T16:00:00.000Z","url":"/2023/03/08/2023/03/081007/","tags":[["websocket","/tags/websocket/"],["curl","/tags/curl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用curl请求websocket "},{"title":"idea运行项目 程序包*.*.*不存在的解决办法","date":"2023-03-05T16:00:00.000Z","url":"/2023/03/06/2023/03/031100/","tags":[["idea","/tags/idea/"]],"categories":[["Java","/categories/Java/"]],"content":" 最新解决方案删除文件夹下的.iml然后mvn idea:module重新生成也可以解决这个问题，交给maven会导致热部署实效，每次改代码都需要重新编译才行 选择JRE和Delegate IDE build&#x2F;run actions to MavenSetting——Build，Execution，Deployment——Maven——Runner—选择Delegate IDE，选择自己安装的JRE的路径 ☑️将IDE 构建&#x2F;运行操作委托给Maven☑️选择安装的jre "},{"title":"Linux系统下的desktop文件,Motrix","date":"2023-02-25T16:00:00.000Z","url":"/2023/02/26/2023/02/141047/","tags":[["motrix","/tags/motrix/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在 Linux 系统下，desktop 文件是用来创建桌面快捷方式的文件。比如要创建 Motrix 应用的 desktop 文件，可以按如下步骤操作: 创建desktop文件的步骤 创建一个空的文件，命名为 motrix.desktop。 用文本编辑器打开 motrix.desktop 文件，并写入以下内容： 保存文件并关闭。 将 motrix.desktop 文件移动到 &#x2F;usr&#x2F;share&#x2F;applications 或者 ~&#x2F;.local&#x2F;share&#x2F;applications 目录。 以上步骤即可在 Linux 系统桌面上创建 Motrix 应用快捷方式。请注意，上述代码中的 Exec 参数必须与系统上可执行的 Motrix 应用名称匹配。 appimage类型的应用.desktop"},{"title":"chilloutmix Ni AI绘画","date":"2023-02-16T16:00:00.000Z","url":"/2023/02/17/2023/02/151342/","tags":[["python","/tags/python/"],["AI","/tags/AI/"]],"categories":[["Python","/categories/Python/"]],"content":" 使用chilloutmix Ni生成人像 模型chilloutmix 下载: chilloutmix-Ni.safetensors 界面stable-diffusion-webui 安装 Linux 系统环境 Install the dependencies:(安装依赖) To install in /home/$(whoami)/stable-diffusion-webui/, run:（安装服务） Run webui.sh（启动服务器） 外网访问默认只能本地127.0.0.1:port访问 开启外部访问，编辑./webui-user.sh: win系统请编辑./webui-user.bat ERROR: Cannot activate python venv, aborting… 解决方案 魔法书魔法书 其他傻瓜包+模型下载分享 参考资料 github "},{"title":"使用RestTemplate发送一个带有JSON数据的POST请求","date":"2023-02-13T16:00:00.000Z","url":"/2023/02/14/2023/02/141037/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":"你可以使用 RestTemplate 发送一个带有 JSON 数据的 POST 请求，代码示例如下： RestTemplate 以上示例来自chatGPT"},{"title":"POST请求431 Request Header Fields Too Large","date":"2023-02-08T16:00:00.000Z","url":"/2023/02/09/2023/02/091328/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":"431 Request Header Fields Too Large 接口信息POST接口，非json传参 接口测试当参数较短时，两种方式都可以正常获取到参数 当参数过长时，第一种可以正常获取，第二种则会报错Request header is too large 建议除文件上传外，POST统一使用json传参 "},{"title":"JsonMappingException: (was java.lang.NullPointerException)","date":"2023-02-05T16:00:00.000Z","url":"/2023/02/06/2023/02/061122/","tags":[["json","/tags/json/"]],"categories":[["Java","/categories/Java/"]],"content":"com.fasterxml.jackson.databind.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: …… 原因 解决方案 修改getter方法，返回包装类型 修改getter，当值为null时，返回默认值 序列化的类 测试方法 ObjectMapperEnum "},{"title":"digital envelope routines::unsupported","date":"2023-02-02T16:00:00.000Z","url":"/2023/02/03/2023/02/030951/","tags":[["nodejs","/tags/nodejs/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Node JS 17 的 BUG 解决方案 也可设置环境变量 "},{"title":"文件名称包含特殊字符导致下载失败","date":"2023-01-30T16:00:00.000Z","url":"/2023/01/31/2023/01/312258/","tags":[["Mac","/tags/Mac/"]],"categories":[["work","/categories/work/"]],"content":"upstream connect error or disconnect&#x2F;reset before headers. reset reason: protocol error 网关错误网关不支持特殊字符 文件名称 该特殊字符在不同的系统上的显示效果 MacOS Linux Windows 解决方案，特殊字符替换 \\\\s+\\\\p&#123;C&#125;\\\\s+是一个正则表达式模式，用于匹配包含空格和Unicode控制字符的字符串。其中： \\\\s+表示一个或多个空格字符。 \\\\p&#123;C&#125;表示Unicode控制字符。 \\\\s+再次表示一个或多个空格字符。这个模式可以匹配包含一个或多个空格和Unicode控制字符的字符串，例如：” \\t\\r \\n\\u0009 “（其中包含了空格、制表符、回车符、换行符和Unicode控制字符）。这个模式不匹配不包含空格和Unicode控制字符的字符串。 需要注意的是，Unicode控制字符是一类特殊的字符，包括不可打印字符（如换行符、回车符、制表符等）和其他特殊字符（如替换字符、文件分隔符等）。使用\\\\p&#123;C&#125;可以匹配所有的Unicode控制字符，包括ASCII码和非ASCII码字符。"},{"title":"NoSuchFieldError: SIGNING_REGION","date":"2023-01-29T16:00:00.000Z","url":"/2023/01/30/2023/01/301727/","tags":[["AWS","/tags/AWS/"]],"categories":[["work","/categories/work/"]],"content":"Exception in thread “main” java.lang.NoSuchFieldError: SIGNING_REGION 类似的错误还可能是： Cannot create enum from “ + regionName + “ value! Regions查看对应的源码，发现版本不一致问题 而旧的版本中Regions区域不完整，所以就导致了上述问题。 pom配置 分析依赖 发现是在dependencyManagement标签中，引入了SpringCloud的包： 而SpringCloud的包指定了特定的AWS依赖版本 而该版本，正好是之前出现的1.11.125的版本 解决方案 亚马逊开发工具包依赖版本管理 Amazon SDK 的发布频率高于 Spring Cloud AWS。如果您需要使用比 由Spring Cloud AWS配置的那个，将SDK BOM添加到依赖项管理部分，确保它已声明 在配置 AWS 开发工具包依赖项的任何其他 BOM 依赖项之前。 参考资料 spring-cloud-aws 【AWS】NoSuchFieldError: SIGNING_REGION "},{"title":"已损坏，无法打开。 您应该将它移到废纸篓","date":"2023-01-25T16:00:00.000Z","url":"/2023/01/26/2023/01/161827/","tags":[["Mac","/tags/Mac/"]],"categories":[["work","/categories/work/"]],"content":"Mac M1安装程序后打开出现“已损坏，无法打开。 您应该将它移到废纸篓” 解决方案 然后打开 Finder进入 “应用程序” 目录，找到该软件，将其图标拖到终端窗口，获取到文件路径，最终拼接为以下命令 "},{"title":"win10下将桌面所有图标移动到外接显示器","date":"2023-01-15T16:00:00.000Z","url":"/2023/01/16/2023/01/161016/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":"笔记本外接显示器以后，因为外接显示器分辨率以及尺寸可能都会比笔记本自带的现实大，所以想要把所有图标移动到外接显示器达到更好的显示效果，更方便操作。 操作步骤进电脑设置-显示-屏幕 将外接显示器设为主屏 不生效问题一般经过上个步骤设置之后，所有的图标就会移动到外接显示器，如果没有成功，可能是第三方桌面整理软件的问题，比如酷呆桌面 解决办法 可以手动把盒子拖动到另一个屏幕，下次再切换屏幕，就会自动记住上次调好的布局，注意前提是盒子之间不能有重叠 卸载软件，换用其他同类型的软件，比如腾讯桌面整理独立版 "},{"title":"Prematurely reached end of stream","date":"2023-01-10T16:00:00.000Z","url":"/2023/01/11/2023/01/111337/","tags":[["mongodb","/tags/mongodb/"]],"categories":[["Java","/categories/Java/"]],"content":"com.mongodb.MongoSocketReadException: Prematurely reached end of stream 原因mongo没有配置空闲连接时间，而spring boot 默认的空闲连接时间为0，即永不超时。当连接闲置一段时间，由于防火墙或者负载均衡的原因，导致连接被关闭，而客户端并不知道，当客户端继续使用这个关闭的连接进行读写时就会出错。 解决方案 使用mongo 3.x 配置 参考资料 MONGODB01 - Prematurely reached end of stream 错误定位及修复 "},{"title":"ArrayIndexOutOfBoundsException: Index -1","date":"2023-01-06T16:00:00.000Z","url":"/2023/01/07/2023/01/071943/","tags":[["Spring","/tags/Spring/"]],"categories":[["Java","/categories/Java/"]],"content":"java.lang.ArrayIndexOutOfBoundsException: -1 错误信息 其他信息 Spring Boot 2.6.4 snakeyaml 1.28 原因及解决方案bug原因：sankeyaml版本的问题 解决方案：升级或者降级snakeyaml版本即可 参考资料 issues-30159 "},{"title":"TLS 密钥协商失败","date":"2023-01-06T16:00:00.000Z","url":"/2023/01/07/2023/01/072004/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"TLS key negotiation failed to occur within 60 seconds (check your network connectivity) 问题TLS 协商失败，并显示以下错误。 原因 防火墙规则阻止 UDP 或 TCP 流量。 您在 配置 （.ovpn） 文件。 客户端证书吊销列表 （CRL） 已过期。 解决办法检查计算机上的防火墙规则是否阻止入站或 端口 443 或 1194 上的出站 TCP 或 UDP 流量。 客户端 VPN 终端节点的防火墙规则不会阻止 TCP 或 UDP 端口 443 或 1194 上的流量。 配置文件包含正确的客户端密钥，证书。 CRL 仍然有效。 "},{"title":"Arch xfce4 安装解压缩软件","date":"2022-12-27T16:00:00.000Z","url":"/2022/12/28/2022/12/281148/","tags":[["xfce4","/tags/xfce4/"],["xarchiver","/tags/xarchiver/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Arch xfce4 安装解压缩软件 安装方法"},{"title":"rsync用法","date":"2022-12-25T16:00:00.000Z","url":"/2022/12/26/2022/12/252143/","tags":[["command","/tags/command/"]],"categories":[["Linux","/categories/Linux/"]],"content":"rsync 是一个常用的 Linux 应用程序，用于文件同步。 需求场景由于业务调整，需要将部分存储的文件迁移到另外一台服务器上，但是由于之前同步过一次，需要合并文件夹，文件夹层级也比较复杂。 rsync 其实就是”远程同步”（remote sync）的意思。与其他文件传输工具（如 FTP 或 scp）不同，rsync 的最大特点是会检查发送方和接收方已有的文件，仅传输有变动的部分（默认规则是文件大小或修改时间有变动）。 安装rsync 传输的双方都必须安装 rsync。 rsync的一些使用实例 远程同步rsync 默认使用 SSH 进行远程登录和数据传输 ssh 增量备份"},{"title":"vscode设置Java开发","date":"2022-12-21T16:00:00.000Z","url":"/2022/12/22/2022/12/221339/","tags":[["vscode","/tags/vscode/"]],"categories":[["Java","/categories/Java/"]],"content":"vscode+插件 插件 Extension Pack for Java Lombok Annotations Support for VS Code Spring Boot Extension Pack 设置 Maven 参考文档 vscode|java-tutorial "},{"title":"ArchLinux挂起后无法唤醒","date":"2022-12-21T16:00:00.000Z","url":"/2022/12/22/2022/12/222219/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Staring systemd-udevd version 252.4-2-arch 只能强制重启 系统信息 排查原因查看系统日志 gnome-logs 解决办法修改/etc/gdm/custom.conf关闭Wayland "},{"title":"Java方法返回多个值","date":"2022-12-20T16:00:00.000Z","url":"/2022/12/21/2022/12/211023/","tags":[["util","/tags/util/"]],"categories":[["Java","/categories/Java/"]],"content":"在进行方法重构时，会遇到返回多个值的场景。 封装一个实体类，将要返回的值封装进去； 通过map或者数组的方式来返回多个值； apache-commons|Pair、Triple "},{"title":"kex_exchange_identification","date":"2022-12-15T16:00:00.000Z","url":"/2022/12/16/2022/12/161421/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"在进行代码提交到github时出现kex_exchange_identification: Connection closed by remote host 原因&amp;&amp;解决方案开了代理解决办法 关闭代理 在代理规则里过滤掉服务器的ip "},{"title":"Apache Commons Pool","date":"2022-12-09T16:00:00.000Z","url":"/2022/12/10/2022/12/102226/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":"使用 Apache Commons Pool,创建一个连接池 客户端 池化对象工厂 实例化对象 参考资料 commons-pool "},{"title":"spring5中的WebClient","date":"2022-12-06T16:00:00.000Z","url":"/2022/12/07/2022/12/071010/","tags":[["webclient","/tags/webclient/"]],"categories":[["Java","/categories/Java/"]],"content":"AsyncRestTemplate已弃用， 已弃用as of Spring 5.0, in favor of org.springframework.web.reactive.function.client.WebClient WebClient的优势 非阻塞响应式IO，单位时间内有限资源下支持更高的并发量。 支持使用Java8 Lambda表达式函数。 支持同步、异步、Stream流式传输。 WebFlux使用maven依赖 WebClient实例创建 WebClient。create() WebClient.create(String baseUrl) WebClient.builder()：返回一个WebClient.Builder，该对象可以做链式调用，传递更多的参数。 支持的可选配置 uriBuilderFactory：自定义UriBuilderFactory灵活配置使用Url defaultHeader：为HTTP请求设置Headers请求头 defaultCookie：为HTTP请求设置Cookies defaultRequest：自定义HttpRequest filter：为HTTP请求增加客户端过滤器 exchangeStrategies：HTTP读写信息自定义 clientConnector：HTTP客户端连接器设置 获取响应结果的方式block()阻塞式获取响应结果 使用Mono和Flux接收返回结果，一个Mono对象包含0个或1个元素，而一个Flux对象包含1个或多个元素。 subscribe()非阻塞式获取响应结果 exchange()获取HTTP响应完整内容 占位符传参 数字占位符 参数名占位符 map传参 POST JSON 文件上传 文件下载 subscribe 订阅（非阻塞式调用） 参考资料 Spring5之WebClient简单使用 "},{"title":"Git rebase变基","date":"2022-12-05T16:00:00.000Z","url":"/2022/12/06/2022/12/062240/","tags":[["git","/tags/git/"]],"categories":[["Linux","/categories/Linux/"]],"content":"使用 git rebase 命令**，将很多提交压扁成一个提交 创建服务器私钥git rebase 命令会将一个提交链从其第一个父级中删除，并将其放置在另一个提交链的末尾，将两个提交链组合成一个长链，而不是两个并行链。 何时变基 “不要变基你存储库以外的的提交，那些提交可能是别人工作的基础。” 简而言之，如果你让一个本地分支来完成你的工作，变基是没有问题的。但一旦该分支被 推送push 了，就不要再变基该分支了。当然，你想要怎么做完全取决于你自己。 idea中使用变基 参考资料 掌握强大的 Git 变基命令 "},{"title":"使用第三方工具portainer提供的api操作docker","date":"2022-12-01T16:00:00.000Z","url":"/2022/12/02/2022/12/022243/","tags":[["docker","/tags/docker/"],["portainer","/tags/portainer/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 安装portainer 添加新的容器集群环境 过远程访问TCP：2375端口绑定环境 使用Remote要求被管理的主机开启docker守护线程监听端口 可以在&#x2F;etc&#x2F;docker&#x2F;daemon.json中添加如下配置 通过 Portainer 代理连接 使用Agent需要在要监控的主机上创建一个portainer agent容器 从菜单中选择Environments然后单击Add environment 。 创建access_token 调用api 其他api java调用示例 WebClient "},{"title":"K8s导出Event到ES","date":"2022-11-28T16:00:00.000Z","url":"/2022/11/29/2022/11/291654/","tags":[["k8s","/tags/k8s/"]],"categories":[["Java","/categories/Java/"]],"content":"Kubernetes 中的资源，例如 Pod、Deployment、Ingress、Service 事件用来指示状态更新或异常。大多数情况下，这些Event会被忽视，它们 1 小时的生命周期，可能会导致丢失重要的事件。它们也不可搜索且无法聚合。 使用event-exporter将Event导出到ES 用于后续的搜索聚合分析 Kubernetes-event-exporter 部署exporter 配置01.config.yaml 设置接收者为elasticsearch 启动event-exporter "},{"title":"Java docker|k8s client","date":"2022-11-10T16:00:00.000Z","url":"/2022/11/11/2022/11/111442/","tags":[["docker","/tags/docker/"],["k8s","/tags/k8s/"]],"categories":[["Java","/categories/Java/"]],"content":"使用Java调用docker或k8s docker-client 仓库地址：docker-client maven 使用示例 docker-java docker-java maven 配置Docker环境 DOCKER_HOSTDocker 主机 URL，例如tcp:&#x2F;&#x2F;localhost:2376unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock DOCKER_TLS_VERIFY启用&#x2F;禁用 TLS 验证（在协议之间切换）httphttps DOCKER_CERT_PATHTLS 验证所需证书的路径 DOCKER_CONFIG其他 docker 配置文件的路径（如.dockercfg) api.versionAPI 版本，例如1.23 registry.url您的注册表地址。 registry.username注册表用户名（推送容器时需要）。 registry.password您的注册表密码。 registry.email您的注册表电子邮件。 示例 fabric8io&#x2F;kubernetes-client k8s-client maven 创建client 配置客户端这将按以下优先级顺序使用来自不同源的设置： 系统属性 环境变量 Kube 配置文件,默认位于当前用户./kube/config 服务帐户令牌和挂载的 CA 证书 示例 "},{"title":"如何从Java调用Python","date":"2022-10-30T16:00:00.000Z","url":"/2022/10/31/2022/10/310927/","tags":[["python","/tags/python/"],["java","/tags/java/"]],"categories":[["Python","/categories/Python/"]],"content":"从 Java 调用 Python 代码的一些最常见方法 Python脚本 当前系统需要安装Python环境 Java调用一 使用ProcessBuilder 使用JythonJython是Python的Java平台实现，运行在JVM上。 直接使用 注意的几点： 由于PythonIntepreter实现了AutoCloseable，因此在处理此类时使用try-with-resources是一种很好的做法。 PythonInterpreter类名并不意味着我们的Python代码被解释。Jython中的Python程序由JVM运行，因此在执行之前编译为Java字节码 虽然Jython是Java的Python实现，但它可能不包含与本机Python相同的所有子包。 Apache Common Exec HTTP调用 考虑使用其他一些流行的框架来创建更强大的基于Python的Web服务或应用程序，它们是Flask和Django。 [How to Call Python From Java](How to Call Python From Java | Baeldung) "},{"title":"生成自签名证书","date":"2022-10-25T16:00:00.000Z","url":"/2022/10/26/2022/10/261339/","tags":[["openssl","/tags/openssl/"]],"categories":[["Linux","/categories/Linux/"]],"content":"生成自签名证书 创建服务器私钥 创建证书签名请求（CSR） 使用私钥和CSR对证书进行签名 现在，您已生成有效期为365天的SSL证书"},{"title":"Java开发环境配置|JDK&Maven","date":"2022-10-17T16:00:00.000Z","url":"/2022/10/18/2022/10/180941/","tags":[["maven","/tags/maven/"],["jdk","/tags/jdk/"]],"categories":[["Java","/categories/Java/"]],"content":"jdk下载，环境变量配置,maven下载&amp;仓库配置等 JDK OpenJDK zuluJDK OracleJDK，需要账号登录下载 java8 下载对应系统及架构的文件进行安装，也可下载压缩文件解压即可 配置环境变量JAVA_HOME Mac下配置环境变量 编辑~/.zshrc，添加以下内容 Maven 下载 maven下载地址 解压到目录中,例如/opt/maven(Linux),Win:D:\\\\Space\\\\maven 配置settings，在mirrors中配置阿里云仓库 配置idea Maven镜像使用 配置settings.xml 找到标签节点,添加一个的mirror子节点： 在项目pom中使用 打开项目配置文件 maven仓库搜索 官方仓库搜索:  备用仓库搜索:  "},{"title":"Docker远程连接设置","date":"2022-10-12T16:00:00.000Z","url":"/2022/10/13/2022/10/131733/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 开启Docker远程连接后，在开发和学习时都能带来便利，但请不要将此方式用在外部网络环境，以免带来安全问题 系统环境 修改配置 文件内容 添加配置 -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 到ExecStart 修改完毕后保存退出 重启docker Ubuntu开启docker远程访问步骤： 检查文件夹/etc/systemd/system/docker.service.d/是否存在，如果没有就创建；在文件夹/etc/systemd/system/docker.service.d/下新建文件override.conf，内容如下： 重新加载配置，再重启docker服务： "},{"title":"Java|PDF文档添加水印","date":"2022-09-27T16:00:00.000Z","url":"/2022/09/28/2022/09/281358/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":" itext PDF文档添加水印 itext7 PDF文档添加水印 adding-watermarks-to-images ebooks&#x2F;itext-7 参考文档 "},{"title":"Spring Cloud 配置中心多环境配置bootstrap.yml","date":"2022-09-22T16:00:00.000Z","url":"/2022/09/23/2022/09/231021/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":" spring boot 多环境配置文件spring boot可以通过文件名来区分配置，如下： spring cloud多环境配置文件使用yaml格式的配置文件 boostrap.yml 启动方式 java -jar Dockerfile 参考资料"},{"title":"Mac M1 删除文件提示\"operation not permitted\"解决方案","date":"2022-09-15T16:00:00.000Z","url":"/2022/09/16/2022/09/161513/","tags":[["m1","/tags/m1/"],["macos","/tags/macos/"],["troubleshooting","/tags/troubleshooting/"]],"categories":[["work","/categories/work/"]],"content":"在 Mac M1 上遇到某些监控软件无法通过常规方式卸载的问题，使用 rm 命令删除时提示”operation not permitted”错误。本文记录了几种解决方案的尝试过程。 问题描述在尝试删除某些系统级应用时遇到权限问题： 方案一：禁用 SIP（系统完整性保护）❌进入 macOS 恢复模式M1 芯片 Mac 进入恢复模式步骤： 关机：选择苹果菜单 &gt; 关机，等待 Mac 完全关机（屏幕全黑，所有指示灯关闭） 进入恢复模式：按住电源按钮直到出现”正在载入启动选项” 选择选项：点击”选项” → “继续” 身份验证： 选择要恢复的宗卷 → “下一步” 选择管理员账户 → “下一步” 输入管理员密码 → “继续” 打开终端：在恢复模式下，选择”实用工具” → “终端” SIP 相关命令 结果：此方案无法解决问题，文件仍然无法删除。 方案二：完全磁盘访问权限 ❌通过系统偏好设置 → 安全性与隐私 → 隐私 → 完全磁盘访问权限，为终端添加权限。 结果：此方案同样无法解决问题。 方案三：移除 schg 标志 ✅解决步骤 禁用相关权限：在系统设置中禁用目标应用的启动项、磁盘访问等相关权限 移除文件保护标志： 参数说明： chflags：修改文件标志的命令 -h：如果文件是符号链接，修改链接本身而不是目标 -v：显示详细输出 noschg：移除 system immutable 标志 schg：system immutable 标志，防止文件被修改或删除 批量处理如果有多个文件夹需要处理： 总结对于 Mac M1 上”operation not permitted”错误： SIP 禁用和完全磁盘访问权限方案均无效 移除 schg 标志是有效解决方案 某些系统级应用使用了文件保护标志来防止被删除 操作前建议先在系统设置中禁用相关应用的权限 参考资料 Apple Developer Forums - rm results in “operation not permitted” macOS File Flags Documentation "},{"title":"Docker容器中禁止生成core文件","date":"2022-09-13T16:00:00.000Z","url":"/2022/09/14/2022/09/141417/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题容器内有一个使用wps转pdf的服务，运行一段时间后，发现磁盘占用超过80%，然而这个服务并不会存储文件，进入服务器后发现目录下存在大量的core.*文件，遂删除，但是后续还会不断生成。 原因 当程序运行过程中出现Segmentation fault (core dumped)错误时，程序停止运行，并产生core文件。core文件是程序运行状态的内存映象。使用gdb调试core文件，可以帮助我们快速定位程序出现段错误的位置。 查询core文件的生成以及查看 core默认的文件名称是core.pid，pid指的是产生段错误的程序的进程号。 默认路径是产生段错误的程序的当前目录。 使用rpc调用wps进程进行转换，但是有一定的概率会调用失败，此时wps进程仍然存在，所以在后面进行的kill进程的处理，导致wps意外退出。 docker中禁止core文件生成 docker-compose limit k8s方法一：在 Pod 的启动命令中设置 ulimit 可以通过在 Pod 的启动命令中添加 ulimit -c 0 来禁止生成 core 文件。以下是一个示例 YAML 配置文件： 在这个示例中，ulimit -c 0 将 core 文件的大小限制为 0，从而禁用 core 文件的生成。然后 your_application_command 是你要运行的应用程序命令。 Linux关闭core文件生成 由于docker中设置的环境变量对wps进程无法生效，所以这种方案也失败了 如果是其他可以在终端中运行的服务，可以将环境变量直接写到 /root/.bashrc,或者写到/etc/profile,然后在.bashrc中写入 source /etc/profile 优雅关闭wps进程既然core文件是由于错误杀死wps进程导致的，那么就需要在代码中处理关闭wps进程的方案，如下 python3调用shell命令常用方法 os.system(cmd) os.popen(cmd,mod) subprocess 参考资料 Linux下使用gdb调试core文件 docker-compose.yaml传递 ‘ulimit’ 参数 ‘rtprio’ 和 ‘memlock’ 的选项 "},{"title":"在命令行中或者全局使用代理工具","date":"2022-09-06T16:00:00.000Z","url":"/2022/09/07/2022/09/071335/","tags":[["zsh","/tags/zsh/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 在命令行中或者全局使用代理工具 在~/.zshrc的文件后面添加两个命令： 配置结束后在命令行中执行： 打开代理 关闭代理 参考资料 链接 "},{"title":"rabbitmq设置帐号密码","date":"2022-09-05T16:00:00.000Z","url":"/2022/09/06/2022/09/061646/","tags":[["rabbitmq","/tags/rabbitmq/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题 解决办法把Virtua lHost为【&#x2F;】的set permission给用户 命令设置"},{"title":"Java合并图片","date":"2022-09-03T16:00:00.000Z","url":"/2022/09/04/2022/09/141646/","tags":[["image","/tags/image/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题有个小伙伴需要把pdf中的两页合并成一张图片 pdf转图片之前已经写过，这里主要写一下图片的合并 图片合并 图片合并成pdf 方案一，使用imagemagic 方案二，pdfbox "},{"title":"Mac下查看pdf文件字体信息","date":"2022-08-29T16:00:00.000Z","url":"/2022/08/30/2022/08/302136/","tags":[["pdf","/tags/pdf/"]],"categories":[["work","/categories/work/"]],"content":" 问题OS X的预览可以显示元数据，尽管子集的数量有限（在“工具”&gt;“显示检查器”下）。但是，使用“预览”看不到嵌入的字体信息。 命令行查看pdf字体 工具查看pdf字体 pdffonts poppler xpdf 参考资料 stackoverflow "},{"title":"python调用本地wps添加水印","date":"2022-08-29T16:00:00.000Z","url":"/2022/08/30/2022/08/301632/","tags":[["watermark","/tags/watermark/"]],"categories":[["Python","/categories/Python/"]],"content":" doc类型文件添加水印java POI等只能给docx类型的文件添加水印，其他需要收费 安装依赖 "},{"title":"python批量解密excel","date":"2022-08-24T16:00:00.000Z","url":"/2022/08/25/2022/08/252229/","tags":[["excel","/tags/excel/"]],"categories":[["Python","/categories/Python/"]],"content":" 批量解密excel食用方法: 填写密码 打开同一密码加密的excel文件夹 等待解密完成,解密后的文件位于子文件夹decrypted下 安装依赖 代码 pyinstaller打包成exe 执行命令 "},{"title":"技术调研","date":"2022-08-22T16:00:00.000Z","url":"/2022/08/23/2022/08/231709/","tags":[["fs","/tags/fs/"]],"categories":[["work","/categories/work/"]],"content":" 缩略图wps接口open.wps.cn 文档类型原始文件类型-&gt;pdf-&gt;png doc水印aspose aspose(收费)doc水印 aspose python调用本地安装的wpswin32com 文件合并docxpoi合并整个文件，不能按页码合并 word本身是不存在分页的概念，软件显示是按照视图来划分页码 pdf可以 [pdfbox] cpdf pdftk excel合并sheet prod "},{"title":"使用Dockerfile自定义镜像","date":"2022-08-04T16:00:00.000Z","url":"/2022/08/05/2022/08/051116/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 什么是DockerfileDockerfile 是附有构建 Docker 镜像说明的易于理解的文本文件。它囊括了用户在创建镜像时可以调用的所有命令。 我们可以使用 Dockerfile 创建自定义的镜像。可以通过 Docker Hub 分享的自定义 Docker 镜像。 docker hub 拉取镜像 运行 Dockerfile格式 FROM —设置容器基础镜像 LABEL —明确镜像的元数据信息的键值对 RUN —在基础镜像中执行指令并创建一个新层 CMD —容器启动后先执行的命令 EXPOSE —设置访问容器的端口,容器将会监听端口 MAINTAINER —显示镜像创建作者的信息 ENV —— 用来设置环境变量的键值对。这些变量在镜像创建的时候设置，并在容器创建好后可以使用。 COPY —— 用来拷贝本地文件至容器中。 ADD —— 具有与拷贝相同的功能，不过更进一步还可以提取本地的 tar 文件或者从 URL 拷贝文件。 ENTRYPOINT —— 用来设置镜像的主要命令。与 CMD 指令功能相同。不同的是 ENTRYPOINT 中的指令不会被重写。 VOLUME —— 该指令用来创建指定位置的挂载点。 USER —— 将设置运行镜像并使用的用户名称以及用户组。 WORKDIR —— 这会设置工作目录。如果目录不存在，则会创建。 Demo 使用 Dockerfile 创建 Docker 镜像 创建一个dockerfile文件 写入以下内容 :wq保存并退出 构建 Dockerfile创建jdk镜像 参考 如何使用 Dockerfile 创建自定义 Docker 镜像 "},{"title":"图片旋转","date":"2022-08-02T16:00:00.000Z","url":"/2022/08/03/2022/08/031324/","tags":[["image","/tags/image/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题 iOS设备拍摄的照片进行添加水印时会出现旋转的情况 解决方案 ImgUtils"},{"title":"Java流的多次读取","date":"2022-08-02T16:00:00.000Z","url":"/2022/08/03/2022/08/031514/","tags":[["InputStream","/tags/InputStream/"]],"categories":[["Java","/categories/Java/"]],"content":" 解决办法"},{"title":"brew|undefined method `<=' for nil:NilClass","date":"2022-07-31T16:00:00.000Z","url":"/2022/08/01/2022/08/010944/","tags":[["brew","/tags/brew/"]],"categories":[["work","/categories/work/"]],"content":" 问题执行brew upgrade时出现错误 解决办法"},{"title":"Navicat安装教程","date":"2022-07-29T16:00:00.000Z","url":"/2022/07/30/2022/07/302154/","tags":[["navicat","/tags/navicat/"]],"categories":[["work","/categories/work/"]],"content":" 下载 navicat安装包 注册机 第一步如果之前有安装过navicat需要删除之前的安装，或者清理注册表 断网 关闭杀毒软件，注册机可能会误报毒 安装navicat,记住安装的位置，打开软件选择激活 打开注册机工具 patch,选择对应的安装位置 生成注册码，（如果没有自动填充，则需要手动复制） 将软件中的key复制到注册机中 注册机选择激活，将生成的密钥复制到软件中，选择激活即可。 截图 这里截图错误，选择注册 No All Pattern Found！File Already Patched？如果出现这种错误 解决方案一 直接重新覆盖安装navicat，然后重新进行激活 方案二 如果一不生效 卸载navicat 清除注册表信息 快捷键win+r命令，然后输入regedit 点击确定，找到如下目录：计算机\\HKEY_CURRENT_USER\\Software\\PremiumSoft 把该目录下除了Data的数据删除 重新安装激活 "},{"title":"使用Linux开发是一种什么体验","date":"2022-07-26T16:00:00.000Z","url":"/2022/07/27/2022/07/272050/","tags":[["daily","/tags/daily/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题 文件路径问题 Linux使用正斜线（&#x2F;）而不是反斜线（\\）在文件路径中划分目录。在Linux中，反斜线用来标识转义字符 这表明文件a.txt位于Downloads目录，Downloads又位于cong目录中，cong则在home目录中。要注意的是，路径本身并没有提供任何有关文件究竟存放在哪个物理磁盘上的信息。 Linux介绍Linux可划分为以下四部分： Linux内核 GNU工具 图形化桌面环境 应用软件 优缺点 大量的可用软件及免费软件 Linux 系统上有着大量的可用软件，且绝大多数是免费的，比如声名赫赫的 Apache、Samba、PHP、MySQL 等，构建成本低廉，是 Linux 被众多企业青睐的原因之一。当然，这和 Linux 出色的性能是分不开的，否则，节约成本就没有任何意义。 配置环境容易，自由度高，安装软件也方便 良好的可移植性及灵活性 Linux 系统有良好的可移植性，它几乎支持所有的 CPU 平台，这使得它便于裁剪和定制。我们可以把 Linux 放在 U 盘、光盘等存储介质中，也可以在嵌入式领域广泛应用。 对系统的熟悉会决定开发效率。 优良的稳定性和安全性 著名的黑客埃里克•雷蒙德（EricS.Raymond）有一句名言：“足够多的眼睛，就可让所有问题浮现”。 Linux 开放源代码，将所有代码放在网上，全世界的程序员都看得到，有什么缺陷和漏洞，很快就会被发现，从而成就了它的稳定性和安全性。 常见Linux目录名称 目录 用途 &#x2F; 虚拟目录的根目录。通常不会在这里存储文件 &#x2F;bin 二进制目录，存放许多用户级的GNU工具 &#x2F;boot 启动目录，存放启动文件 &#x2F;dev 设备目录，Linux在这里创建设备节点 &#x2F;etc 系统配置文件目录 &#x2F;home 主目录，Linux在这里创建用户目录 &#x2F;lib 库目录，存放系统和应用程序的库文件 &#x2F;media 媒体目录，可移动媒体设备的常用挂载点 &#x2F;mnt 挂载目录，另一个可移动媒体设备的常用挂载点 &#x2F;opt 可选目录，常用于存放第三方软件包和数据文件 &#x2F;proc 进程目录，存放现有硬件及当前进程的相关信息 &#x2F;root root用户的主目录 &#x2F;sbin 系统二进制目录，存放许多GNU管理员级工具 &#x2F;run 运行目录，存放系统运作时的运行时数据 &#x2F;srv 服务目录，存放本地服务的相关文件 &#x2F;sys 系统目录，存放系统硬件信息的相关文件 &#x2F;tmp 临时目录，可以在该目录中创建和删除临时工作文件 &#x2F;usr 用户二进制目录，大量用户级的GNU工具和数据文件都存储在这里 &#x2F;var 可变目录，用以存放经常变化的文件，比如日志文件 发行版 Gentoo:基于源码的发行版，滚动升级，需要从源代码编译软件包 Debian: 上游最大的发行版，提供较多的软件二进制包 Ubuntu: 是一个非常流行的基于Debian的发行版，由 Canonical 公司提供商业支持 Kali:Kali Linux是基于Debian的Linux发行版， 设计用于数字取证操作系统。预装了许多渗透测试软件 Deepin:基于 DEB 包管理的一个独立操作系统 RedHat:Red Hat Enterprise Linux 是 Red Hat 公司的 Linux 发行版，面向商业市场，包括大型机。 Centos: 来自于Red Hat Enterprise Linux依照开放源代码规定发布的源代码所编译而成. Fedora: 由社区开发，并有红帽提供公司级支持。它是红帽版的技术前导版，对新技术的采用非常激进。Fedora 的软件包和项目会被引入 RHEL 中，并最终被其他发行版采用。采用 RPM 包，用 DNF 包管理器并且提供图形化的包管理工具. openSUSE:以 RPM 格式软件包为中心。提供了优秀的 YaST2 图形配置工具。 Linux Mint:一个 Ubuntu 的衍生版本 distrowatch ArchLinux Arch Linux 是通用 x86-64 GNU&#x2F;Linux 发行版。Arch采用滚动升级模式，尽全力提供最新的稳定版软件。初始安装的Arch只是一个基本系统，随后用户可以根据自己的喜好安装需要的软件并配置成符合自己理想的系统。 ArchWiki 安装程序包管理器package manager”（或“软件包管理器”）是一种工具，它允许用户在操作系统上安装、删除、升级、配置和管理软件包。软件包管理器可以是像“软件中心”这样的图形化应用，也可以是像 apt-get 或 pacman 这样的命令行工具。 配置环境变量 开发工具 idea eclipse vscode pycharm datagrip navicat vscode换背景 openvpn vim 下载工具 aria2 motrix you-get uGet cURL Wget 百度网盘 NextCloud Seafile FTP FileZilla GNOME Files 系统工具Htop SwitchHosts! 即时通讯 钉钉 腾讯会议 微信 录屏 OBS 文档 wps OnlyOffice LibreOffice 输入法 fcitx5 ibus google-pinyin rime 搜狗 百度 远程桌面 remmina 终端 gnoem-terminal xfce4-terminal sakura tabby konsole 远程登陆ssh 新建配置文件 写入以下内容 登陆 浏览器 chrome firefox edge dev-doc 虚拟化 docker docker系统内核版本不低于 3.10 ，并且是 64 位系统,BIOS 上启用了 VT（虚拟化技术）在终端上运行以下命令验证内核以及架构详细信息： 什么是DockerfileDockerfile 是附有构建 Docker 镜像说明的易于理解的文本文件。它囊括了用户在创建镜像时可以调用的所有命令。 我们可以使用 Dockerfile 创建自定义的镜像。可以通过 Docker Hub 分享的自定义 Docker 镜像。 docker hub 拉取镜像 运行 Dockerfile格式 FROM —设置容器基础镜像 LABEL —明确镜像的元数据信息的键值对 RUN —在基础镜像中执行指令并创建一个新层 CMD —容器启动后先执行的命令 EXPOSE —设置访问容器的端口,容器将会监听端口 MAINTAINER —显示镜像创建作者的信息 ENV —— 用来设置环境变量的键值对。这些变量在镜像创建的时候设置，并在容器创建好后可以使用。 COPY —— 用来拷贝本地文件至容器中。 ADD —— 具有与拷贝相同的功能，不过更进一步还可以提取本地的 tar 文件或者从 URL 拷贝文件。 ENTRYPOINT —— 用来设置镜像的主要命令。与 CMD 指令功能相同。不同的是 ENTRYPOINT 中的指令不会被重写。 VOLUME —— 该指令用来创建指定位置的挂载点。 USER —— 将设置运行镜像并使用的用户名称以及用户组。 WORKDIR —— 这会设置工作目录。如果目录不存在，则会创建。 Demo 使用 Dockerfile 创建 Docker 镜像 创建一个dockerfile文件 写入以下内容 :wq保存并退出 构建 可视化管理容器-portainer Java python anaconda3&#x2F;miniconda3 文件分享 CLI查看天气 tcpdump分析DNS查询 安装tcpdump 分析 查看网卡信息 监听网络请求 lrzsz文件传输 在命令行中使用代理工具在~/.zshrc的文件后面添加两个命令： 配置结束后在命令行中执行： 打开代理 关闭代理 ffmpeg多媒体处理 参考资料 Wsl 软件列表 Linux发行版 "},{"title":"Docker|oci runtime error","date":"2022-07-22T16:00:00.000Z","url":"/2022/07/23/2022/07/232234/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" Error response from daemon: oci runtime error: container with id exists原因：升级docker版本之后，启动服务报错。升级之前没有关闭容器。 解决方案删除原有运行时产生的文件，再重新运行 Unknown runtime specified docker-runc 从不兼容的版本升级docker并且升级后无法启动docker容器时会出现这种情况 "},{"title":"minikube start cn（国内）","date":"2022-07-20T16:00:00.000Z","url":"/2022/07/21/2022/07/211326/","tags":[["k8s","/tags/k8s/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题 解决方案 代理 设置使用国内仓库 参考资料 minikube start命令的国内使用方法 "},{"title":"大事记","date":"2022-07-18T16:00:00.000Z","url":"/2022/07/19/2022/07/190000/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":" 特殊的日子前一天便定了第二天六点的闹钟，因为今天是拍婚纱照的日子。 八点需要到指定的地点做准备，拍照之前的准备主要就是化妆和换衣服了。男生是没有选择的权力的，一切跟着女士的选择来搭配。因此我还闹了个笑话，在换黑色礼服的时候，工作人员帮我选了一件蓝色的西装，我觉得不对啊，之前化妆师安排的是黑色礼服啊，咋是蓝色的呢，问了才知道，黑色礼服是女士的衣服。 因为天气原因以及个人喜好吧，没有选择外景的拍摄，但是由于下雨，上海倒是没有前两天那么热了，但是由出外景回来的一个哥们反馈，差点喜事变丧事了，整个气温比较湿热，而且中途还下雨了，唯一好的就是没那么人了，不然真的要死人了。 一共五套衣服，拍完已经是十一点半左右了。腿是疼的，肌肉也有点僵，全程假笑也蛮困难的。 女方要更为辛苦一些，单独的照片动作表情要求都会多一点，男的更多就是饶头。哈哈。 一个很有意义的一天。"},{"title":"tcpdump分析DNS查询","date":"2022-07-11T16:00:00.000Z","url":"/2022/07/12/2022/07/122300/","tags":[["tcp","/tags/tcp/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题在使用wps转换pdf的过程中，发现在没网的情况下，转换速度较慢。猜测wps会请求联网，导致转换慢。 安装tcpdump 分析 查看网卡信息 监听网络请求 使用WPS转换 查看结果 帮助 tcpdump基本语法"},{"title":"Spring项目启动时执行一次的方法","date":"2022-07-04T16:00:00.000Z","url":"/2022/07/05/2022/07/052308/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":" 需求在使用rabbimq的过程中会出现某一业务方突发大量请求的情况,故需要对之前的单一队列按业务进行拆分,之后获取业务的队列列表,然后轮询队列处理消息. 可以通过mq的api获取交换机绑定的所有队列,这个请求需要在服务启动时执行一次即可,后续新增通过消息通知. Spring MVC 实现ApplicationListener,并实现onApplicationEvevt方法 SpringBoot ApplicationRunner CommandLineRunner @PostConstruct 输出"},{"title":"Redis java.lang.IllegalStateException:Cannot connect, Event executor group is terminated","date":"2022-07-04T16:00:00.000Z","url":"/2022/07/05/2022/07/072204/","tags":[["redis","/tags/redis/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题需求接上篇，启动服务之后，第一次初始化队列，但是在后续的新增队列之后，需要获取新的队列，添加到队列中，第一种方案，使用redis set来存储队列信息。之后会介绍另一种实现方案。 使用redis来存储队列信息，轮询时就会有一个线程一直轮询请求redis获取数据。 关闭应用时就会出现一个错误。 github 解决方案在程序关闭时，停止线程，不再请求redis 实现DisposableBean 使用@PreDestory 参考资料 cnblogs "},{"title":"java获取本机ip地址","date":"2022-07-03T16:00:00.000Z","url":"/2022/07/04/2022/07/042158/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" java获取本机ip地址"},{"title":"Redis|OOM command not allowed when used memory > ‘maxmemory","date":"2022-06-23T16:00:00.000Z","url":"/2022/06/24/2022/06/242148/","tags":[["redis","/tags/redis/"]],"categories":[["work","/categories/work/"]],"content":" 问题服务突然出现了访问错误,排查日志发现有这么一个错误信息 发生在每次向redis中存储新数据的时候.这个错误信息表示内存满了,无法存储更多的数据 可以通过以下方式查看内存信息 修复OOM 增加内存,修改redis配置文件/etc/redis/*.conf,找到maxmemory设置使用的内存限制,修改配置文件之后需要重启 修改过期策略,默认为maxmemory-policy volatile-lru,注意,修改为其他策略,可能会导致未持久化到磁盘上的数据丢失 减少存储的数据量,减少非必要数据写入redis "},{"title":"Nexus","date":"2022-06-08T16:00:00.000Z","url":"/2022/06/09/2022/06/091127/","tags":[["nexus","/tags/nexus/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 需求使用docker安装nexus,配置maven私服&amp;docker镜像私服。 安装nexus 登录  默认账号：admin 默认密码位置：.&#x2F;data&#x2F;nexus-data&#x2F;admin.password 配置maven私服aliyun代理仓库 阿里云maven仓库地址 修改maven-public顺序 maven配置私服地址 settings.xml pom.xml 配置docker私服 hosted本地仓库 proxy代理仓库 group仓库组 docker(hosted) http端口设置为8082 ☑️勾选Enable Doeker V1 API 其他默认，保存 dokcer(proxy) 新建 docker-aliyun-proxy http留空 ☑️勾选Allow anonymous docker pull ☑️勾选Enable Doeker V1 API Docker Index 选择 Use Docker Hub 其余默认，保存 docker(group) 新建docker-group http端口设置为8083 ☑️勾选Allow anonymous docker pull ☑️勾选Enable Doeker V1 API group添加上面设置的host与proxy仓库 保存 配置docker Realm设置-&gt;Security-&gt;Realms-&gt; 将Docker Bearer Token Realm配置到右边,save保存 角色与权限 添加角色Roles:dokcer,权限为nx-repository-view-docker-*-* 添加Users，填写对应的用户名，邮箱密码，状态（Status）设置为Active,角色为docker 修改docker配置 登录docker仓库 下载镜像 推送镜像"},{"title":"Nginx超时","date":"2022-06-07T16:00:00.000Z","url":"/2022/06/08/2022/06/081123/","tags":[["nginx","/tags/nginx/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题 相关配置 其他情况第三方网关设置，比如阿里云的网关配置"},{"title":"日志|查找符合条件的结果","date":"2022-06-01T16:00:00.000Z","url":"/2022/06/02/2022/06/021102/","tags":[["grep","/tags/grep/"]],"categories":[["work","/categories/work/"]],"content":" 需求需要在日志文件中查找报错的请求接口信息. 多条件筛选 查找关键信息"},{"title":"今日解封","date":"2022-05-31T16:00:00.000Z","url":"/2022/06/01/2022/06/011103/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":" 六一快乐今日解封2022-03-10 至 2022-06-01"},{"title":"FastAPI","date":"2022-05-19T16:00:00.000Z","url":"/2022/05/20/2022/05/202032/","tags":[["fastapi","/tags/fastapi/"]],"categories":[["Python","/categories/Python/"]],"content":" 安装依赖 实例 启动"},{"title":"requests下载文件","date":"2022-05-19T16:00:00.000Z","url":"/2022/05/20/2022/05/202045/","tags":[["requests","/tags/requests/"]],"categories":[["Python","/categories/Python/"]],"content":" 代码实例"},{"title":"Ubuntu中文显示","date":"2022-05-09T16:00:00.000Z","url":"/2022/05/10/2022/05/101655/","tags":[["ubuntu","/tags/ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题 解决方案"},{"title":"docker提交本地镜像","date":"2022-05-09T16:00:00.000Z","url":"/2022/05/10/2022/05/101700/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题docker commit :从容器创建一个新的镜像。 OPTIONS说明：-a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 实例"},{"title":"容器中使用wps转pdf","date":"2022-05-05T16:00:00.000Z","url":"/2022/05/06/2022/05/061908/","tags":[["docker","/tags/docker/"],["wps","/tags/wps/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 需求容器中使用wps转换pdf的功能 基础镜像 安装wps 安装字体建议复制Windows下的所有字体,否则转换之后会因为字体缺失导致转换后的pdf和win下预览的不一致. wps-font-symbols 安装python环境 缺少python3-lxml会报错:libxslt.so.1: cannot open shared object file 转换脚本doc-&gt;pdf excel-&gt;pdf ppt-&gt;pdf 转换pdf 问题 libQt5Core.so.5: cannot open shared object file ImportError: &#x2F;usr&#x2F;lib&#x2F;office6&#x2F;libstdc++.so.6: version &#96;GLIBCXX_3.4.29’ not found (required by &#x2F;usr&#x2F;lib&#x2F;libQt5Core.so.5) openjdk8 docker打包 Dockerfile new.desktop docker-compose 参考资料 docker-ubuntu-vnc-desktop docker commit 用docker创建ubuntu VNC桌面 wps RUN ON SERVER 镜像地址 制作镜像 更换国内源 制作镜像 安装wps 安装字体  安装python环境 文档互转 WPS文字 WPS演示 WPS表格 Run On Server dummy.conf Now start the Xorg (with root): EULA 转换pdf 问题 转换卡顿问题或超时 本地部署一般为局域网访问，不开通外网，wps有外网连接请求，在没网的环境下会导致转换过慢，甚至会导致转换超时失败。 解决方案 将resolv.conf中的DNS解析服务器设置为空 docker中可以挂载一个空文件映射到&#x2F;etc&#x2F;resolv.conf &#x2F;etc&#x2F;hosts 添加以下内容(由于有些部署方式会替换容器host，这里没有在容器内修改，可以在配置中添加主机别名) libQt5Core.so.5: cannot open shared object file ImportError: &#x2F;usr&#x2F;lib&#x2F;office6&#x2F;libstdc++.so.6: version &#96;GLIBCXX_3.4.29’ not found (required by&#x2F;usr&#x2F;lib&#x2F;libQt5Core.so.5) et转换失败 error: libltdl.so.7: cannot open shared object file: No such file or directory 使用示例 start.sh "},{"title":"django静态资源","date":"2022-04-27T16:00:00.000Z","url":"/2022/04/28/2022/04/281449/","tags":[["django","/tags/django/"]],"categories":[["Python","/categories/Python/"]],"content":" 需求在项目目录下自定义静态资源文件夹static,根据请求返回该文件的静态资源显示在页面上. 配置settings.py 页面 方法"},{"title":"SpringBoot2.4.*使用Nacos配置中心","date":"2022-04-27T16:00:00.000Z","url":"/2022/04/28/2022/04/281511/","tags":[["nacos","/tags/nacos/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题springboot 加载不到nacos配置中心的配置问题 解决办法Spring Boot 2.4版本开始，配置文件加载方式进行了重构,重新引入 "},{"title":"grep","date":"2022-04-23T16:00:00.000Z","url":"/2022/04/24/2022/04/242131/","tags":[["shell","/tags/shell/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 对输入（如文件、键盘）的每一行文本，grep命令进行如下操作。（1）将该行文本读入相应的缓冲区，该缓冲区又被称为模式空间。（2）对模式空间应用正则表达式进行对比。（3）如果匹配，则该行信息从模式空间被复制到标准输出（显示器）。 实例"},{"title":"Linux下删除文件中的Windows换行符","date":"2022-04-17T16:00:00.000Z","url":"/2022/04/18/2022/04/182145/","tags":[["js","/tags/js/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题windows下换行符为\\n\\r,linux下为\\n查看cat -A temp.md 解决方案方法1：在命令模式下:输入:%s/^M//g 然后,回车即可替换 注,其中”^M”的输入,分别是“Ctrl+v”、“Ctrl+M”键生成的 方法2: 使用vi打开文本文件vi dos.txt命令模式下输入 方法3: 使用sed 工具 方法4: 既然window下的回车符多了‘\\r’，那么当然通过删除‘\\r’ ，也可以实现： 方法5:（最常用的方法） 在终端下敲命令： 直接转换成unix格式，就OK了！～"},{"title":"OnlyOffice使用案例分享","date":"2022-04-15T16:00:00.000Z","url":"/2022/04/16/2022/04/161126/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" Onlyoffice介绍ONLYOFFICE文档企业版能够提供强大的文档、电子表格与演示文稿编辑器，便于与当前使用的文档方案进行集成. 特性 用于专业编辑、文档协作并在线 表单创建的所有工具。 轻松部署和集成。使用deb&#x2F;rpm、exe或通过Docker进行安装，还可使用AWS&#x2F;Univention上的即用映像。 可以编辑流行格式的文档、电子表格和电子幻灯片：DOC、DOCX、ODT、RTF、TXT、XLS、XLSX、ODS、CSV、PPTX、PPT、ODP 国际化语言多样化，切换灵活 方案中台服务中心-文件服务系统集成OnlyOffice之后,支持大多数文档格式在线编辑,协同编辑,以及多种文件格式的在线预览.满足了业务线的需要. OnlyOffice多样化的部署方式,可以满足多场景落地使用. 预览文档 编辑文档 协同编辑 历史记录"},{"title":"JavaScript heap out of memory error","date":"2022-04-15T16:00:00.000Z","url":"/2022/04/16/2022/04/161155/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 问题 解决办法 参考资料stackoverflow"},{"title":"Linux下安装字体后刷新字体","date":"2022-04-14T16:00:00.000Z","url":"/2022/04/15/2022/04/152057/","tags":[["font","/tags/font/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 安装字体 建立字体缓存 刷新缓存"},{"title":"源码安装Ghostscript","date":"2022-04-14T16:00:00.000Z","url":"/2022/04/15/2022/04/152101/","tags":[["gs","/tags/gs/"]],"categories":[["Linux","/categories/Linux/"]],"content":" Centos默认安装的gs版本过低,在实现pdf压缩时,出现反向压缩的奇怪现象,压缩后的文件比压缩前要大. 下载地址 ghostscript-9.56.1.tar.gz 安装 问题编译安装的gs位于/usr/local/bin下,优先级低于默认的/usr/bin下的gs,执行gs命令会发现没有变化 可以移除旧版本的gs 可将新编译安装的版本放在/usr/bin下. gs压缩pdf pdf转图片 在使用 gs（Ghostscript）将 PDF 转换为图片时，避免失真的关键在于以下几点： 分辨率 (-r**) 的设置**：设置足够高的分辨率（例如 300 DPI 或更高）。 颜色模式 (-sDEVICE**) 的选择**：选择支持高质量颜色的设备，例如 png16m 或 jpeg，而不是 png256（256 色可能会导致颜色失真）。 输出文件质量参数：对于 JPEG 图像，可以设置质量参数以提高输出质量。 示例命令： 以下是一些高质量的转换设置： 转换为高质量 PNG： 参数解释： • -sDEVICE&#x3D;png16m：使用 24 位彩色（真彩色） PNG 格式，避免颜色失真。 • -r300：分辨率设置为 300 DPI，适合大多数用途。如果需要更清晰的图像，可以提高到 600 DPI 或更高。 • -dFirstPage 和 -dLastPage：指定转换的页范围。 转换为高质量 JPEG： 参数解释： • -sDEVICE&#x3D;jpeg：输出 JPEG 格式。 • -r300：分辨率设置为 300 DPI。 • -dJPEGQ&#x3D;95：设置 JPEG 图像的质量为 95（默认是 75）。范围为 0-100，值越高，图像质量越好，但文件大小也会增加。 确保页面比例不被拉伸： 如果担心生成的图片被拉伸，可以设置页面裁剪模式： -dPDFFitPage 该参数会根据目标设备的大小调整内容比例，确保不失真。 例如： 提高分辨率时的注意事项： • 高分辨率会显著增加文件大小。如果生成的图片太大，可以适当降低分辨率（如 -r150）。 • 分辨率过低（如小于 150 DPI）可能会导致细节模糊，建议根据需要调整。 总结： • 最推荐的格式：png16m（无损），适合保留所有细节。 • 推荐分辨率：-r300 或更高（如 -r600）。 • 颜色质量控制：对于 JPEG，增加 -dJPEGQ 的值。"},{"title":"Linux版钉钉中文输入","date":"2022-04-10T16:00:00.000Z","url":"/2022/04/11/2022/04/111826/","tags":[["dingtalk","/tags/dingtalk/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 安装 中文输入编辑 /usr/bin/dingtalk "},{"title":"ArchLinux记录","date":"2022-04-10T16:00:00.000Z","url":"/2022/04/11/2022/04/110936/","tags":[["Arch","/tags/Arch/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 安装参照 archwiki使用 archinstall 脚本安装 Arch Linux下载地址 安装参考文档 安装gnome iwd连接网络 NetworkManager使用iwd连接网络 问题 sign_and_send_pubkey: signing failed: agent refused operation 原因：ssh-agent 已经在运行了，但是找不到附加的任何keys，就是说你生成的key，没有附加到ssh-agent上，需要附加一下解决办法 密钥错误 archlinuxcn 在 Arch Linux 上启用蓝牙 检查蓝牙服务的状态并查看其是否正在运行。 确保蓝牙未被阻止： 使用以下命令取消阻止： 蓝牙不可用问题描述: 正常情况下通过上面的设置可以正常使用蓝牙,但是后期突然不可用,经排查发现,由于安装了一个gnome-shell插件,附带了一个蓝牙服务,两者存在冲突 其他情况可以查看参考资料中的链接 Pipewire 与 Pulseaudio只需要安装其中一种 如果使用 Pipewire，请确保安装了 pipewire-pulse 如果使用 Pulseaudio，需要安装bluez 和 pulseaudio-bluetooth 。 应用 Remmina 远程桌面连接 picgo 图床 variety 壁纸 flameshot 截图 Motrix 下载工具 steam 游戏 steam 开启multilib仓库,并安装steam软件包yay steam 安装32位版本的OpenGL图形驱动(N卡)yay lib32-nvidia-utils 安装字体ttf-liberation 安装wqy-zenhei支持中文 小键盘GNOME 运行下面命令: 其他zsh 参考资料 stackoverflow "},{"title":"zuul|文件上传中文名称乱码","date":"2022-03-29T16:00:00.000Z","url":"/2022/03/30/2022/03/301430/","tags":[["zuul","/tags/zuul/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题文件上传之后发现中文文件名称出现？？乱码。 解决办法上传文件如果通过zuul的话, 在接口地址前面加上zuul, 如果你是file&#x2F;upload, 那么就要在页面使用时, zuul&#x2F;file&#x2F;upload 方案2 add this config in zuul service application.properties 参考资料 zuul proxy file upload, file name is Chinese garbled "},{"title":"关于gitee不再支持个人图床的解决方案","date":"2022-03-28T16:00:00.000Z","url":"/2022/03/29/2022/03/290946/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":"2022-07-07 jsdelivr已经无法正常访问，该方法失效。 问题gitee不再允许使用仓库作为图床 解决办法使用GitHub，并且使用jsdelivr picgo配置 参考资料 github  "},{"title":"GitLab远程仓库代码回滚","date":"2022-03-24T16:00:00.000Z","url":"/2022/03/25/2022/03/251422/","tags":[["git","/tags/git/"]],"categories":[["work","/categories/work/"]],"content":" 问题修改代码的时候没有注意当前分支，导致将更新推送到保护分支上，需要进行回滚。 解除保护设置-仓库-分支保护 查找回滚记录 本地查找sha码 远程查找 Branch-History 回滚 提交"},{"title":"Java|pdfbox操作pdf","date":"2022-03-22T16:00:00.000Z","url":"/2022/03/23/2022/03/232047/","tags":[["pdf","/tags/pdf/"]],"categories":[["Java","/categories/Java/"]],"content":" 依赖 拆分|合并文档"},{"title":"docker容器内中文乱码","date":"2022-03-14T16:00:00.000Z","url":"/2022/03/15/2022/03/151700/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题中文字符乱码，显示?????? 解决方案"},{"title":"文件md5","date":"2022-03-10T16:00:00.000Z","url":"/2022/03/11/2022/03/111926/","tags":[["file","/tags/file/"]],"categories":[["work","/categories/work/"]],"content":" 问题md5计算文件,文件流只能读取一次 测试代码 测试 "},{"title":"MySQL高可用id自增问题","date":"2022-03-07T16:00:00.000Z","url":"/2022/03/08/2022/03/082059/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":" 问题业务发现之前稳定运行的服务，突然大概率出线问题，问题出现在上次升级数据库高可用之后。查询日志发现有一个业务异常，文件记录找不到，根据fileId查询可以发现对应的数据，但是根据主键id发现查询不到该记录。虽然定义的类id类型为String，实际上并没有使用，而是数据库默认使用id自增，在高可用场景下，主键自增不一致，也就是问题所在。 原因 关键代码 修改主键字段类型 注意在代码中生成uuid设置到id中。 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;title: MySQL自增id在高可用场景下的问题date: 2022-03-08categories: - worktags: - MySQLprev: 051619.mdnext: 111926.md 问题业务发现之前稳定运行的服务，突然大概率出现问题，时间点在上次升级数据库高可用之后。查询日志发现有一个业务异常，文件记录找不到，根据fileId查询可以发现对应的数据，但是根据主键id发现查询不到该记录。虽然定义的类id类型为String，实际上并没有使用，而是数据库默认使用id自增，在高可用场景下，主键自增不一致，也就是问题所在。 原因 关键代码 修改主键字段类型 注意在代码中生成uuid设置到id中。"},{"title":"pdf转图片|多线程","date":"2022-03-04T16:00:00.000Z","url":"/2022/03/05/2022/03/051619/","tags":[["tool","/tags/tool/"]],"categories":[["Java","/categories/Java/"]],"content":" 需求将pdf文件转换成图片，一页一张 依赖 转换 程序 方案二,gs 方案三,pdftoppm 安装 命令示例 镜像构建 问题解答 Syntax Error: Gen inside xref table too large (bigger than INT_MAX) issue "},{"title":"The 'Access-Control-Allow-Origin' header contains multiple values '*, *', but only one is allowed","date":"2022-02-28T16:00:00.000Z","url":"/2022/03/01/2022/03/011430/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 问题hls视频播放出现了跨域问题，已经配置了跨域 解决办法去除其中一个的跨域即可"},{"title":"CentOS安装LibreOffice","date":"2022-02-21T16:00:00.000Z","url":"/2022/02/22/2022/02/221517/","tags":[["office","/tags/office/"]],"categories":[["Linux","/categories/Linux/"]],"content":" LibreOffice下载下载地址 选择需要的版本以及平台进行下载，CentOS选择rpm 安装 依赖安装 下载LibreOffice_6.4.6.2_Linux_x86-64_rpm.tar.gz 解压 安装 中文字体直接将Windows下的中文字体复制到 /usr/share/fonts/下 更新缓存fc-cache -vf 转pdf 参考资料 使用LibreOffice实现Word转PDF "},{"title":"ffmpeg|threads多线程","date":"2022-02-20T16:00:00.000Z","url":"/2022/02/21/2022/02/211648/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题视频使用了GPU不支持的编码格式，只能使用cpu进行转码，在没有限制cpu使用率的情况下，cpu使用率很快就达到了100%。 解决方案指定 -threads,该参数非全局（global），所以需要在输出端指定 FFmpeg 总是有一个主线程来完成大部分处理。在多个输入的情况下，还有用于解复用的输入线程(每个输入 1 个线程)；单输入解复用是在主线程上完成的。 在输入上设置“线程 N”(其中 N &gt; 1)可以启用多线程解码，这可以为每个支持它的解码器生成 N 个附加线程。在您的情况下，视频解码器支持它而音频解码器不支持，因此它是 3 个线程 - 1 个主线程 + 2 个用于视频解码的线程 类似地，在输出上设置“线程 N”可以启用多线程过滤和编码，这可以为每个过滤器图生成 N 个附加线程(我认为在较旧的 ffmpeg 版本中，这是“每个过滤器最多 N 个线程”)和每个支持它的编码器。还有一个重要的警告 - 这仅适用于通过 ffmpeg 进行线程管理的编码器； libx264 不会这样做 - 它会将请求的线程计数转发到 x264 库，该库执行自己的线程管理。 x264 然后可能会创建最多 2*N 个线程(确切数量取决于许多编码参数)。因此，对于具有单输出的“线程 2”，您将获得 1 个主线程 + 2 个用于缩放器的线程 + 至少 2 个用于 libx264 的线程。 $综上，使用h264编码时，如果需要限制转换过程中cpu的使用率，最好就是限制输出线程数量。$ 获取ffmpeg使用的线程数量转换命令 结果⚠️这里只针对h264编码器 参考资料 FFmpeg 在使用 -threads 时使用的线程比我预期的要多 "},{"title":"python读写文件","date":"2022-02-07T16:00:00.000Z","url":"/2022/02/08/2022/02/081745/","tags":[["file","/tags/file/"]],"categories":[["Python","/categories/Python/"]],"content":" 安装python环境 写入文件 w代表写入 r+代表读写 a代表追加 with语法写入 读取数据 with语法读取"},{"title":"shell链接文件","date":"2022-01-28T16:00:00.000Z","url":"/2022/01/29/2022/01/290939/","tags":[["shell","/tags/shell/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 程序设计 结合实际的业务需求，但是也要兼顾业务发展，以避免后期频繁重构修改 链接文件“接文件是Linux文件系统的一个优势。如需要在系统上维护同一文件的两份或多份副本，除了保存多份单独的物理文件副本之外，还可以采用保存一份物理文件副本和多个虚拟副本的方法。这种虚拟的副本就称为链接 。”[1] 符号链接一个实际存在的文件，它指向存放在虚拟目录结构中某一个地方的另一个文件。这两个通过符号链接在一起的文件，彼此的内容并不相同。 硬链接“硬链接 会创建独立的虚拟文件，其中包含了原始文件的信息及位置。但是它们从根本上而言是同一个文件。” 参考资料 摘录来自Linux命令行与shell脚本编程大全（第3版）[美] Richard Blum0 "},{"title":"Java MD5","date":"2022-01-23T16:00:00.000Z","url":"/2022/01/24/2022/01/241414/","tags":[["md5","/tags/md5/"]],"categories":[["Java","/categories/Java/"]],"content":" 介绍 jdk版本md5 commons-codec pom code "},{"title":"js下载文件流","date":"2022-01-23T16:00:00.000Z","url":"/2022/01/24/2022/01/241634/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" Controller js下载 参考资料 如何通过ajax实现文件流下载 "},{"title":"Java|文件拆分与合并","date":"2022-01-17T16:00:00.000Z","url":"/2022/01/18/2022/01/181003/","tags":[["io","/tags/io/"],["file","/tags/file/"]],"categories":[["Java","/categories/Java/"]],"content":" 示例代码 测试结果 注意文件拆分合并后要进行MD5校验，确保文件没有被篡改。"},{"title":"一条SQL引发的OOM","date":"2022-01-13T16:00:00.000Z","url":"/2022/01/14/2022/01/141038/","tags":[["sql","/tags/sql/"]],"categories":[["work","/categories/work/"]],"content":" 问题状态发现是因为有业务反应某一环境的请求会出现padding，经测试发现，状态诡异，时好时坏，多次请求会出现一次超时。于是怀疑某一台服务器出了状况，Eureka中显示所有的服务都在线，但由于eureka只能保证AP，服务出问题，但是心跳还在，并没有被下线，查询日志发现，某台服务器只有心跳检测，没有任何服务相关的响应。于是先重启该服务。进一步查看日志。 日志排查改服务器在前一天的某刻出现OOM 继续查看发现有一条SQL Error SQL语句没有什么问题，但是查询条件有问题，相当于全部查询，数据量太大，内存于是就爆了。剩下的就是找到对应的方法，修改即可。 总结 为了保证高可用，服务一般会做负载均衡，多台服务器，还有一种情况就是MQ消费者，当你发现请求部分成功，部分失败的时候，就有很大的理由怀疑是其中一台的问题。顺藤摸瓜就能定位问题所在。 SQL 查询要加limit限制，否则数据量大服务就挂了。可以先看一下SQL执行计划，不过在本次事件中，并非手写SQL而是框架封装。 "},{"title":"Java8中Lambda表达式示例","date":"2022-01-12T16:00:00.000Z","url":"/2022/01/13/2022/01/131450/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 例1 用Lambda表达式实现Runnable接口 输出: 这个例子使我们学到了java8中Lambda表达式的书写方式：(参数) -&gt; 表达式(参数) -&gt; 语句(参数) -&gt; { 语句 }例如，如果你的方法只是在控制台打印信息，则可以这么写： 如果你的方法接收两个参数，那么： 顺带提一句，一般来说在Lambda表达式中要尽量保持变量的简洁性。这会使你的代码简短而能保持在一行之内。所以像上面的代码可以选择变量名类似a,b或者x,y之类的，比起even和odd来会更好。 例2 用Lambda表达式写事件监听程序要是你用过Swing API，那就少不了事件监听代码，这是使用匿名类的经典例子。现在我们可以用Lambda表达式来抒写更好的事件处理代码。 另外一个常用匿名类的地方是给Collections.sort()方法提供自定义的Comparator接口实现。这个地方也可以用Lambda表达式。 例3 用Lambda表达式进行List迭代 &#x2F;&#x2F; 用java8的方法引用更好，方法引用由::(双冒号)操作符来完成,看起来像c++中的作用域操作符 输出: 例4 使用Lambda表达式和函数式接口Predicate除了提供函数式编程语言级别的支持外，java8同时也新增了一个新的包java.util.function。其中包含了许多类来支持java函数式编程。其中之一是Predicate接口，使用这个接口和lamb表达式就可以以更少的代码为API方法添加更多的动态行为。以下是Predicate的使用范例，展示了过滤集合数据的许多共性。 输出: Java代码 可以看到Stream API的filter方法也接受一个Predicate，意味着可以用内联代码直接替换我们自定义的filter()方法。这就是Lambda表达式的威力所在。除此之外Predicate接口也可以测试多个条件，将会在下面的例子中加以说明。 例5: Lambda表达式结合Predicate就像上个例子所说，Predicate允许组合两个以上的条件，它提供了类似于逻辑与和或的操作and(),or()和xor()，这些方法可以用来组合传递到filter方法中的多个条件。例如为了获取所有以J开头并有四个字符长度的语言，可以定义两个单独的Predicate实例覆盖每个条件然后用and方法将他们组合在一起。看例子： 类似可以用or或者xor。这个例子也强调了单独用或者按需组合用Predicate的重要性。简而言之用Predicate和Lambda表达式的优势你可以写的更少做得更多。 例6 Map和Reduce的例子6.1 Map在这个例子中，我们要将costBeforeTax的每个元素以加上他们的增值税。传递一个Lambda表达式给map方法使之应用于每个元素，之后在用forEach打印结果。 输出 6.2 Reduce还有另外一个函数reduce可以将所有值转换为一个值。map跟reduce操作是函数式编程的核心，reduce也被称作折叠操作。reduce并不是一种新的操作，在SQL中我们用的一些聚集函数比如sum，avg，count等他们实际上也是reduce操作，因为他们也是将多个值进行操作然后返回一个值。Stream API定义了reduce函数，可以接受一个Lambda表达式然后组合所有值。Stream类中像IntStream都有内置的方法像average(), count(), sum(), mapToLong(), mapToDouble()等转换方法。我们可以用内置的方法也可以自定义。 输出 例7 用filter创建一个字符串List在java开发中对大的集合进行过滤是常用的操作。用Lambda表达式和Stream API会让操作变得简单易懂。Stream提供了一个filter()方法，接受一个Predicate对象。这意味着可以传递一个Lambda表达式作为过滤逻辑，看例子: 输出 : 例8 给每个List元素应用函数在工作中我们经常会碰到这样的情况：给List中每个元素加以一定的操作例如乘以或者除以某个值等。这些操作用map方法再好不过了，我们可以将转换逻辑以Lambda表达式传给map方法来应用于每个元素：&#x2F;&#x2F;将字符串转为大写然后用逗号连起来 输出: 例9 复制不同值到子列表本例演示如何利用Stream类的distinct方法过滤重复值到集合中。 输出 : 例10 计算List中元素的最大，最小，和以及平均值在Stream类中像IntStream, LongStream and DoubleStream有一个非常有用的方法summaryStattics()，返回IntSummaryStatistics, LongSummaryStatistics or DoubleSummaryStatistics其描述了这个流中元素的统计数据。下面的例子中我们用这个方法来计算一个List中的最大最小值总和以及均值: 输出: "},{"title":"Ubuntu20.04更换清华源","date":"2022-01-10T16:00:00.000Z","url":"/2022/01/11/2022/01/111040/","tags":[["Ubuntu","/tags/Ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 查看系统版本信息 配置清华源 备份sudo mv /etc/apt/sources.list /etc/apt/sources.list.back 修改sudo vim /etc/apt/sources.list 更新sudo apt update 无法拉取 https 源 解决The repository ‘ focal main’ does not have a Release file 修改https为http 安装https相关包sudo apt install apt-transport-https ca-certificates 执行sudo apt update 改回https "},{"title":"局域网文件传输","date":"2022-01-06T16:00:00.000Z","url":"/2022/01/07/2022/01/071127/","tags":[["python","/tags/python/"]],"categories":[["Python","/categories/Python/"]],"content":" python"},{"title":"maven跳过测试","date":"2022-01-03T16:00:00.000Z","url":"/2022/01/04/2022/01/041746/","tags":[["maven","/tags/maven/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题打包时希望跳过maven的单元测试 pom配置"},{"title":"sudo source|linux root运行","date":"2021-12-31T16:00:00.000Z","url":"/2022/01/01/2022/02/111630/","tags":[["shell","/tags/shell/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题sudo source找不到命令问题是source是一个bash build-in命令(不是程序 – 如ls或grep)。 以root用户身份登录，然后执行source命令 另一种解决方案 以root用户启动一个新的bash "},{"title":"csv使用excel打开数字过长问题","date":"2021-12-29T16:00:00.000Z","url":"/2021/12/30/2021/12/301646/","tags":[["csv","/tags/csv/"]],"categories":[["work","/categories/work/"]],"content":" 问题在使用python爬取数据保存为csv文本之后，id为数字，并且超过了15位，使用excel打开出现两个问题，一是显示为科学记数法，二是数字末尾部分替换为0。 解决方案 wps 数据 选择数据 选择编码 选择分隔符 列数据类型（id文本） "},{"title":"SpringBoot外部配置文件","date":"2021-12-20T16:00:00.000Z","url":"/2021/12/21/2021/12/211525/","tags":[["Spring Boot","/tags/Spring-Boot/"]],"categories":[["Java","/categories/Java/"]],"content":" Spring Boot 将在应用程序启动时自动查找并从以下位置加载文件：application.properties,application.yaml 外部应用程序属性很多时候针对不同的环境会修改对应的配置文件，默认配置文件会打包至jar中，部分属性修改，不想再次打包，Spring Boot提供外部加载配置文件的方式 从类路径 a. 类路径根目录 b. 类路径包/config 从当前目录 a. 当前目录 b. 当前目录中的子目录/config c. 子目录的直接子目录&#x2F;config 该列表按优先级排序（$较低项的值将覆盖较早的项$）。加载文件中的文档被添加到 Spring 中。PropertySourcesEnvironment 如果您不喜欢作为配置文件名，则可以通过指定环境属性切换到另一个文件名。例如，要查找 和 文件，可以按如下方式运行应用程序：applicationspring.config.namemyproject.propertiesmyproject.yaml 还可以使用 environment 属性引用显式位置。此属性接受要检查的一个或多个位置的逗号分隔列表。spring.config.location下面的示例演示如何指定两个不同的文件 参考资料  "},{"title":"导航","date":"2021-12-19T16:00:00.000Z","url":"/2021/12/20/2021/12/201345/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" BookmarkssoftMPC-BE - Browse Files at SourceForge.net Awesome Wallpapers - wallhaven.cc Xwinwrap - openSUSE Wiki 小众软件 PDF转Excel转换器 - 免费服务 webFlask快速入门，知识整理 - 听风。 - 博客园 蓝图和视图 - Flask 1.0.2文档 Linux安装NVIDIA显卡驱动的正确姿势_FlyWine的博客-CSDN博客_linux安装nvidia驱动 插画交流网站[pixiv] Handbook - Apache ECharts 海贼王 第902集_视频高清在线观看_海贼王 - 樱花动漫 tool凡人修仙传txt下载,凡人修仙传txt电子书下载-TXT图书下载网 steamcommunity 302 Ver.2【fix社区118】 | Dogfight360 收件箱 - qianbanmu@gmail.com - Gmail 图解算法数据结构 - LeetBook - 力扣（LeetCode）全球极客挚爱的技术成长平台 014.Nginx跨域配置 - 木二 - 博客园 openssl生成自签名证书(完整版) - 简书 完整教程：Springboot 2.2整合elasticsearch 7.x (spring-boot-starter-data-elasticsearch)_程裕强的专栏-CSDN博客_springboot集成elasticsearch7 folium库的使用 - 知乎 存储换算器 - 文件大小转换 - 电脑容量换算 Process - Java 11中文版 - API参考文档 RabbitMQ 手动确认模式(项目开发常用模式)_一只Black的博客-CSDN博客 spring-boot + rabbitmq消息手动确认模式的几点说明（重试机制）_chyk1414的博客-CSDN博客 jsDelivr - A free, fast, and reliable CDN for open source World-class heap Dump analysis - Java, Android memory dump analyzer F 搜 (java office转pdf) MS Office2010、WPS2015、OpenOffice4用Java将Office文档转换为PDF，WIN7 64位系统_huitoukest的专栏-CSDN博客 Magi Java利用jacob实现wps转换pdf_LittleBoy-CSDN博客_java wps转pdf Windows远程桌面出现CredSSP加密数据修正问题解决方案_沧海黎明-CSDN博客_credssp加密数据库修正 Springboot 启动jar包，指定外部配置文件_山神的博客-CSDN博客_springboot启动指定外部配置文件 如何将Spring Boot项目部署为Windows服务，并设置开机启动 - 简书 MySQL存储的字段是不区分大小写的，你知道吗？-mysql教程-PHP中文网 登录 - Adminer SpringBoot整合Mybatis注解版 - 翱翔的天空的博客 - CSDN博客 第七章、Spring Boot MyBatis升级篇 - weixin_38492276的博客 - CSDN博客 Spring Boot(十一)：Spring Boot 中 MongoDB 的使用 - 纯洁的微笑 - 博客园 腾讯文档 浙江图书馆 Navicat For Linux - rainerosion Kibana raw.githubusercontent.com服务器iP raw.githubusercontent.com域名解析 raw.githubusercontent.comiP查询 raw.githubusercontent.com域名iP查询 安装Kubernetes报错：The connection to the server localhost:8080 was refused - 简书 MySQL :: MySQL 8.0参考手册:: 8.4.7表列数和行大小的限制 计算机类 - 第9页| Vue3 Ajax(axios) | 菜鸟教程 Figma: the collaborative interface design tool. Road 2 Coding congfxsjy&#x2F;jieba: 结巴中文分词 快速入门 — Arthas 3.5.0 文档 Spring Cloud Schema Registry 分布式锁中的王者方案-Redisson - 悟空聊架构 - 博客园 腾讯文档 易快报 SnowNull python[pytorch中文文档] torch.optim - pytorch中文网 Datasets | Kaggle 用 PyTorch 迁移学习（Transfer Learning）实现图像分类-PyTorch 中文网 PyTorch的入门教程实战 - wzy_zju的博客 - CSDN博客 使用字符级RNN进行名字分类 - PyTorch官方教程中文版 如何在 Tkinter 中将 Enter 键绑定到函数 | D栈 - Delft Stack Folium — Folium 0.11.0 documentation Python Tutorial: A Tutorial gameAwesome Wallpapers - wallhaven.cc SpringCloud Finchley Gateway 统一异常处理 - SegmentFault 思否 TL-WAR1200L MW300R Spring Boot 快速集成第三方登录功能 | CodingDiary Beautiful Free Images &amp; Pictures | Unsplash 【新提醒】【2017&#x2F;09&#x2F;10更新 支持Steam版本】仙剑4宽屏&amp;UI修正工具 v1.41 - 第66页 - 《仙剑奇侠传五前传》 - 3DMGAME论坛 - Powered by Discuz! congcoCongco (congco) - 路过图床 有声小说,听小说,有声书,在线听书电台-喜马拉雅FM YY影院-最新电影,最新电视剧 SnowNull Index of &#x2F;100G_Super_Big_Collection&#x2F;中华文化工具书&#x2F;语言文字（工具书、词典）&#x2F; springcloud分布式事务处理方案 - zhangxing - CSDN博客 qbmzc (congco) congco@healthskeeper.com - 邮箱 - ewomail.com 有哪些免费好用的电子书下载网站？ - 知乎 中国图书网（中图网）：网上书店，尾货特色书店，5万种特价书低至2折！ pytorch主页 - PyTorch中文文档 Training a Classifier — PyTorch Tutorials 1.1.0.dev20190507 documentation 主页 - Keras 中文文档 Transfer Learning Tutorial — PyTorch Tutorials 1.1.0 documentation"},{"title":"主机远程唤醒（Wake-On-LAN）","date":"2021-12-13T16:00:00.000Z","url":"/2021/12/14/2021/12/141319/","tags":[["tool","/tags/tool/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 需求笔记本电脑外接了显示器，不想每次都打开盖子按电源键开机，查询了一下，支持远程唤醒 系统休眠对于不需要一直运行的电脑，使用 root 的 cron 定时任务（即 &#x2F;etc&#x2F;crontab）可以可靠地关闭电脑。这个例子创建一个 root 定时任务实现每天晚上 11 点 15 分定时关机。 周一至周五 自动唤醒 首先，查看系统主板 BIOS 是否支持 Wake-On-LAN ，要是支持的话，必须先启动它，因为它被默认为禁用。 查看网卡是否支持 Wake-On-LAN Supports Wake-on d – 禁用 p – 物理活动唤醒 u – 单播消息唤醒 m – 多播（组播）消息唤醒 b – 广播消息唤醒 a – ARP 唤醒 g – 特定数据包magic packet唤醒 s – 设有密码的特定数据包magic packet唤醒 启用 想要实现远程唤醒主机，必须支持的功能是 g —— 特定数据包唤醒 定时任务 Windows BIOS打开唤醒设置 在BIOS电源相关选项寻找Resume By LAN,Enable Wake ON LAN 类似选项开启 网卡设置 设备管理器-&gt;对应网卡-&gt;属性-&gt;电源管理-&gt;✅允许此设备唤醒计算机 关闭快速启动 参考资料 如何自动唤醒和关闭 Linux "},{"title":"icns图标制作","date":"2021-12-09T16:00:00.000Z","url":"/2021/12/10/2021/12/101720/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 问题最近闲暇之余，使用electron制作了一个简单的应用程序，打包的时候就需要自定义一下应用图标了。 步骤 创建一个文件夹icon,存放源文件pic.png(1024*1024)。 使用命令mkdir tmp.iconset在当前目录下创建一个临时文件，用来存放不同分辨率的图片 当前路径下，执行以下命令，生成临时文件 在icon目录下，用下面的命令 生成icns文件 "},{"title":"秋收冬藏","date":"2021-12-08T16:00:00.000Z","url":"/2021/12/09/2021/12/081018/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 写在开始最近的一些人，事，物，以及一些感想 关于一些人程序员的圈子总是很封闭的，当然限于我，已经有多久没有认识新的朋友了，由于上家公司待得比较久，新同事认识的都少的可怜。 社交软件以及一些社交游戏的发展，填补了一些交流的需求。有共同兴趣爱好的人更容易成为朋友，还有就是到了新的公司，认识了新的同事 其中有一些人还是当年网上冲浪时认识的网友，很久不联系，突然群里冒泡，都已毕业工作，不胜唏嘘。这些是因为大家都喜欢同类型的音乐认识的。 还有一些，至今会在微信群里叽叽喳喳，有幸第一次面基，也是这里的朋友。这些也是因为共同的兴趣爱好-游戏，认识的。 人与人之间的关系纽带可能会很复杂，也可能会比较简单，自我认知，不是那种和谁都能谈得来的人。自然也不会有很多新的朋友。 一些事情昨天起的头，但是今天开写的时候却又不知从何写起了。今年也是不一样的一年吧，算是半步迈入了人生新阶段。 从原来的公司离职，进入新的公司已过半年，度过了试用期，小公司相对自由宽松的环境，也让自己成长了不少，无论是技术上还是为人处事方面。希望一切安好。 对了，昨晚睡的早，收到一条消息，早上起来的时候才看到。组装电脑的机会没有了。 那些年看过的小说最近，又有一部小说被改编成了电视剧了,还在读大学时追的一部小说，当时是在一个有声平台听的有声书，后来更新跟不上，等不久就去看了原著。被当时的同学念叨小说中的人物名称，没想到过了这么多年了，也即将步入自己的人生新阶段了。 去年的一段时间，工作不忙，突然看到以前的小说改编的电视剧，就看了好多，虽然改编的不太行，但是吧，让人想起了以前，学业繁忙，却还是抽出时间追小说。想着以后大学了，毕业了，就有大把的时间，要把想看的小说看完，甚至还有自己动笔写小说的想法，后来尝试过，终归不是那块料吧，认识的朋友中有一位作者，很厉害，能够做到自己曾经想做的事情。 当年一段相对来说比较困苦的日子里，仅有的一些慰藉，可能就是每天更新的有声书吧，奈何作者更新太慢，所有有声的更新也是断断续续的，不知道现在更新完了没有。 一些总结 2021-12-27一年又一年，年尾个大app推出了年终总结，今天看了热搜，网易云音乐上榜，但是由于之前的版权问题，我转用qq音乐很久了，所以今年的总结不是那么的准确。好在QQ音乐也推出了类似的总结，后面可以预知的还有支付宝总结–年度账单。看看钱都花哪去了。 结语是想写一个年度总结来着，慢慢补上吧。"},{"title":"wget","date":"2021-12-08T16:00:00.000Z","url":"/2021/12/09/2021/12/070959/","tags":[["tool","/tags/tool/"]],"categories":[["Linux","/categories/Linux/"]],"content":" wget -help 示例 下载单个文件 下载并以不同的文件名保存 限速下载 更多用法请查看上面的帮助信息"},{"title":"Windows安装外部项目为服务","date":"2021-12-08T16:00:00.000Z","url":"/2021/12/09/2021/12/241003/","tags":[["Spring Boot","/tags/Spring-Boot/"]],"categories":[["Java","/categories/Java/"]],"content":" 需求使用Spring Boot开发的Web项目，打包生成了一个jar包，部署在Windows服务器中，设置为开机启动 其他方案 cmd窗口运行java -jar,需要窗口一直开着 计划任务bat脚本 WinSW WinSWgithub-winssw 下载最后的2.*版本，3目前为预览版 配置文件与主程序名称保持一致，和jar放在同一目录,日志也会在当前目录中 配置文件内容 安装服务 当前目录下cmd执行 管理服务 "},{"title":"electron国内镜像","date":"2021-12-05T16:00:00.000Z","url":"/2021/12/06/2021/12/061017/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 安装 可以使用 –platform 来指定开发平台 (例如, win32、linux 等): 自定义镜像在安装过程中，electron 模块会通过 electron-download 为您的平台下载 Electron 的预编译二进制文件。 这将通过访问 GitHub 的发布下载页面来完成 ($VERSION, 这里的 $VERSION 是 Electron 的确切版本). 如果您无法访问GitHub，或者您需要提供自定义构建，则可以通过提供镜像或现有的缓存目录来实现。 镜像 可以使用环境变量来覆盖基本URL,也就是ELECTRON_MIRROR 例如淘宝镜像 参考资料 安装指导 "},{"title":"H5去掉滚动条","date":"2021-12-03T16:00:00.000Z","url":"/2021/12/04/2021/12/041956/","tags":[["css","/tags/css/"]],"categories":[["work","/categories/work/"]],"content":" 问题在使用electron制作桌面壁纸时，发现右侧会有滚动条出现，遂干掉 解决方案 壁纸项目地址 "},{"title":"Maven加载本地包","date":"2021-12-02T16:00:00.000Z","url":"/2021/12/03/2021/12/031319/","tags":[["maven","/tags/maven/"]],"categories":[["Java","/categories/Java/"]],"content":" pom文件scope的system属性 打包按照上面的配置，打包没有问题，如果是在项目根路径下lib打包，之后运行可能会找不到。 这样配置，打包就可以了 使用mvn命令将jar安装到本地仓库中 其他方案 上传到中央仓库 搭建nexus私服 本地新建lib包，之后导入（eclipse直接build path）（idea：File→project structure→libraries→+jar包） "},{"title":"20210202完全对称","date":"2021-12-01T16:00:00.000Z","url":"/2021/12/02/2021/12/020000/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 并没有什么特殊的事情发生，也是平淡的一天。 早上喝了红薯粥，红薯是家里寄过来的，然后出门，走路到地铁站。从家到公司需要一个小时左右的时间。 新买的保温壶也到了，满足一天的喝水量。 处理需求，编码，测试。"},{"title":"M1使用iterm2的rzsz功能","date":"2021-11-29T16:00:00.000Z","url":"/2021/11/30/2021/11/301650/","tags":[["m1","/tags/m1/"]],"categories":[["work","/categories/work/"]],"content":" 安装lrzsz 安装执行脚本将iterm2-send-zmodem.sh和iterm2-recv-zmodem.sh保存到/usr/local/bin目录下。 iterm2-send-zmodem.sh iterm2-recv-zmodem.sh 设置Iterm2的Tirgger特性 参考资料  "},{"title":"markdown编辑器","date":"2021-11-28T16:00:00.000Z","url":"/2021/11/29/2021/11/291436/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":" typora实时预览以及图床的配置是一直以来使用的动力，在更新到1.0版本之后，开始收费，可能就不会再使用了吧，但是一个习惯的养成是比较困难的 Vnote接触这个编辑器应该是最早的，做了很多的个人笔记，可能还是因为预览和编写是分离的，所以当时转到了typora,还有就是使用vuepress搭建了个人博客,这个渐渐的也就不用了，当时的一些备份还是可以在百度网盘中找到的。 vscode这个应该是未来长期的选择了。 图片插件使用picgo"},{"title":"jvm|执行引擎","date":"2021-11-28T16:00:00.000Z","url":"/2021/11/29/2021/11/292226/","tags":[["jvm","/tags/jvm/"]],"categories":[["Java","/categories/Java/"]],"content":" TODO"},{"title":"虚拟机基础工具","date":"2021-11-26T16:00:00.000Z","url":"/2021/11/27/2021/11/271943/","tags":[["jvm","/tags/jvm/"]],"categories":[["Java","/categories/Java/"]],"content":" jps:虚拟机进程状况工具jps(JVM Process Status Tool):可以列出正在运行的虚拟机进程，并显示虚拟机执行主类（Main.class）名称以及这些进程的本地虚拟机唯一ID jsp命令格式 其他常见选项 选项 作用 -q 只输出绿米的，省略主类的名称 -m 输出虚拟机进程启动时传递给主类main()函数的参数 -l 输出主类的名称，如果执行的JAR包，则输出JAR路径 -v 输出虚拟机进程启动时的JVM参数 jstat：虚拟机统计信息监视工具常见用法：jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] jstack:查看线程堆栈信息解决的问题： cpu占用过高 cpu使用率100% 内存溢出OOM top找到CPU消耗最高的进程，记住PID top 查看进程中使用cpu最高的线程top -Hp &lt;pid&gt; 线程ID转换为十六进制:printf &quot;%xn&quot; &lt;tid&gt; 查看线程jstack &lt;pid&gt;|grep &lt;tid&gt; -A 30 jinfojinfo可以查看当前JVM线程配置的系统属性，以及运行时设置的参数值。 jmap 用JMap导出当前进程的内存镜像 mat 在线分析hprof文件: mat使用 jhatJVM Heat Analysis Tool,分析jmap生成的堆转储快照，比较简陋，一般不使用其他分析工具 VisualVM Eclipse Memeory Analyzer IBM HeapAnalyzer(J9虚拟机) "},{"title":"no-referrer-when-downgrade","date":"2021-11-25T16:00:00.000Z","url":"/2021/11/26/2021/11/261418/","tags":[["http","/tags/http/"]],"categories":[["work","/categories/work/"]],"content":" 问题 业务方遇到一个问题，有的域名下上传文件上传不了，个别人上传不了 报错截图只有一个no-referrer-when-downgrade，未提供控制台错误信息 原因业务方测试环境使用了https，而文件服务器使用的是http,不符合新的安全策略 no-referrer-when-downgrade表示从https协议降为http协议时不推荐发送referrer给跳转网站的服务器 解决方案"},{"title":"Linux同步时间","date":"2021-11-24T16:00:00.000Z","url":"/2021/11/25/2021/11/251016/","tags":[["ntp","/tags/ntp/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题每天的自动点餐定时任务执行时间为上午十点整，但是点餐之后的确认时间却多了40秒左右，服务器查看时间，显示比正常时间慢了，所以需要同步一下时间 ## 时间同步ntp 安装ntp 启动ntp服务 修改配置文件 重启服务 设定时区为东八区 验证 参考资料 中国时间服务器，国内阿里云时间服务器  "},{"title":"API文档工具","date":"2021-11-23T16:00:00.000Z","url":"/2021/11/24/2021/11/241056/","tags":[["tool","/tags/tool/"]],"categories":[["work","/categories/work/"]],"content":" DashDash is an API Documentation Browser and Code Snippet Manager DevDocs API Documentation可以安装为应用，也有桌面版本 Zeal - offline API documentation browser免费 Velocity - The Documentation and Docset Viewer for WindowsVelocity 的收费模式和 Dash 一样，不 Buy 的话偶尔会有弹窗提示，打开文档等几秒等小 trick"},{"title":"留言板","date":"2021-11-22T16:00:00.000Z","url":"/2021/11/23/2021/11/231600/","categories":[["wuw","/categories/wuw/"]],"content":" 留言板"},{"title":"Linux|du命令","date":"2021-11-22T16:00:00.000Z","url":"/2021/11/23/2021/11/221717/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" du 命令，全称是 disk usage，du实用程序显示每个文件参数以及文件层次结构中以每个目录参数为根的每个目录的文件系统块使用情况。如果未指定任何文件，则将显示根在当前目录中的层次结构的块使用情况。 常见用法 更多用法参见"},{"title":"ffmepg在docker中使用gpu进行转码","date":"2021-11-18T16:00:00.000Z","url":"/2021/11/19/2021/11/191600/","tags":[["docker","/tags/docker/"],["ffmpeg","/tags/ffmpeg/"],["cuvid","/tags/cuvid/"],["nvidia","/tags/nvidia/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 需求项目中视频转码需要使用到硬件也就是GPU，docker在版本19以后加个参数–gpus all 即可 Ubuntu安装toolkit其他系统参见参考资料链接里面的安装指南 设置存储库和 GPG 密钥：stable 更新包列表和安装包 新启动 Docker以完成安装 运行cuda容器测试结果 base镜像较小没有nvcc命令，但可以正常使用 Dockerfile run.sh libcuda.so.1&#x2F;libnvidia-encode.so.1&#x2F;libnvcuvid.so.1等找不到 [AVHWDeviceContext @ 0x55fe15809040] Cannot load libcuda.so.1 [h264_nvenc @ 0x55bf603b6400] Cannot load libnvidia-encode.so.1 原因：cuda:11.2.2-base-ubuntu20.04镜像中存在的是软连接，需要对应的目标文件，目标文件与当前宿主机的显卡驱动版本相关，所以需要拷贝文件到容器中,但是,nvidia早就想到了一切，不需要copy 添加环境变量即可 或者在启动时指定 因为服务可能发布在k8s集群中,不同的环境驱动肯定是不一样的，直接copy文件根本不现实。 测试结果 参考资料   Linux安装NVIDIA显卡驱动的正确姿势 docker使用GPU总结 环境搭建 Docker内FFmpeg GPU加速 NVidia driver libraries in nvidia&#x2F;cuda image "},{"title":"redis 交集、并集、差集","date":"2021-11-16T16:00:00.000Z","url":"/2021/11/17/2021/11/171014/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" [TOC] sinter|sunion|sdiff sinter key [key …] 查看一个集合的全部成员，该集合是所有给定集合的交集。 sunion key [key …] 查看一个集合的全部成员，该集合是所有给定集合的并集。 sdiff key [key …] 查看所有给定 key 与第一个 key 的差集 sinterstore、sunionstore、sdiffstore sinterstore destination key [key …] 将 交集 数据存储到某个对象中 sunionstore destination key [key …] 将 并集 数据存储到某个对象中 sdiffstore destination key [key …] 将 差集 数据存储到某个对象中 RedisTemplate Redis 集合命令 序号 命令及描述 1 SADD key member1 [member2] 向集合添加一个或多个成员 2 SCARD key 获取集合的成员数 3 SDIFF key1 [key2] 返回第一个集合与其他集合之间的差异。 4 SDIFFSTORE destination key1 [key2] 返回给定所有集合的差集并存储在 destination 中 5 SINTER key1 [key2] 返回给定所有集合的交集 6 SINTERSTORE destination key1 [key2] 返回给定所有集合的交集并存储在 destination 中 7 SISMEMBER key member 判断 member 元素是否是集合 key 的成员 8 SMEMBERS key 返回集合中的所有成员 9 SMOVE source destination member 将 member 元素从 source 集合移动到 destination 集合 10 SPOP key 移除并返回集合中的一个随机元素 11 [SRANDMEMBER key count] 返回集合中一个或多个随机数 12 [SREM key member1 member2] 移除集合中一个或多个成员 13 [SUNION key1 key2] 返回所有给定集合的并集 14 [SUNIONSTORE destination key1 key2] 所有给定集合的并集存储在 destination 集合中 15 [SSCAN key cursor MATCH pattern] [COUNT count] 迭代集合中的元素 参考资料   "},{"title":"java|Wps转PDF","date":"2021-11-04T16:00:00.000Z","url":"/2021/11/05/2021/11/051639/","tags":[["java","/tags/java/"]],"categories":[["work","/categories/work/"]],"content":" 问题由于线上使用的编辑和预览方案都是wps,而转换引擎使用的为MS Office&amp;LibreOffice,二者在文件的呈现上存在兼容性的差异，为了使文件转换后与编辑预览时保持效果一致，针对WPS编辑的文件使用WPS引擎转换。 流程图 文件元信息读取首先需要获取到文件的编辑工具是office还是WPS，这里使用tika进行元数据的获取。 Apache Tika Apache Tika用于文件类型检测和从各种格式的文件内容提取的库。 在内部，Tika使用现有的各种文件解析器和文档类型的检测技术来检测和提取数据。 使用Tika，人们可以开发出通用型检测器和内容提取到的不同类型的文件，如电子表格，文本文件，图像，PDF文件甚至多媒体输入格式，在一定程度上提取结构化文本以及元数据。 Tika提供用于解析不同文件格式的一个通用API。它采用83个现有的专业解析器库，为每个文档类型。 所有这些解析器库是根据一个叫做Parser接口单一接口封装。 解析示例 元信息 jacob调用wps转PDF需要导入jacob-1.20.jar包，以及将jacob-1.20-x64.dll或者jacob-1.20-x86.dll放置到jdk的bin目录（或者windows的System32&#x2F;SysWoW64目录下） 示例 WPS 实际测试下来 没有发现该限制 方式一实际使用下来发现，在另存为时可能会出现进程卡住的情况，建议使用方式二转换ComThread.InitSTA(),该方法会导致进程卡住,建议注释掉 方式二"},{"title":"十一月记录","date":"2021-11-03T16:00:00.000Z","url":"/2021/11/04/2021/11/040943/","tags":[["daily","/tags/daily/"]],"categories":[["work","/categories/work/"]],"content":" file-search错误排查 RuntimeException: java.io.IOException: error: Invalid byte 3 of 3-byte UTF-8 sequence. RuntimeException: org.apache.tika.exception.TikaException: Error creating OOXML extractor 文件编码错误，需要另存为操作一次可解决 RuntimeException: org.apache.poi.ooxml.POIXMLException: Strict OOXML isn’t currently supported, please see bug #57699这是由于当你使用XSSFWorkbook加在excel流的时候，加载的Excel是Strict Open XML格式的。 转换成 Excel 工作簿（*.xlsx）就ok了。 RuntimeException: org.apache.tika.exception.TikaException: Error creating OOXML extractor文件内容异常 "},{"title":"npm|yarn使用国内镜像","date":"2021-11-02T16:00:00.000Z","url":"/2021/11/03/2021/11/011531/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 临时使用 配置 验证 cnpm yarn 查询当前 设置淘宝镜像 恢复原设置 旧淘宝npm源切换到新源npmmirror 相关网站 npm淘宝镜像 nodejs&amp;&amp;npm安装升级系统环境Ubuntu20.04 安装 查看当前版本 升级到最新版本 卸载 "},{"title":"ffmpeg转换失败记录","date":"2021-10-27T16:00:00.000Z","url":"/2021/10/28/2021/10/280938/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["work","/categories/work/"]],"content":" 转换记录 SQL "},{"title":"Mac M1 Java开发环境配置","date":"2021-10-26T16:00:00.000Z","url":"/2021/10/27/2021/10/271631/","tags":[["M1","/tags/M1/"],["JDK","/tags/JDK/"]],"categories":[["Java","/categories/Java/"]],"content":" JDK卸载卸载其他版本的jdk JDK配置目前 Zulu JDK 支持 M1芯片，下载Download Azul Zulu Builds of OpenJDK | Azul 且支持的版本较多， 下载后点击安装，在控制台输入java -version 多版本JDK下载不同版本安装即可，Zulu JDK默认安装在/Library/Java/JavaVirtualMachines目录下，配置 JAVA_HOME切换 JDK版本, 修改~/.zshrc文件,没有创建一个, 将配置加入到文件末尾 Maven下载maven, 解压后移动到 /opt目录下（不移动到/opt下也行），配置MAVEN_HOME, 修改~/.zshrc文件， 将配置加入到文件末尾 mvn -version Git⚙️ ​一、安装 Git​ ​检查是否已安装​ 终端输入 git --version，若显示版本号（如 git version 2.32.0）说明已安装。 ​安装方法​ ​推荐（Homebrew）​​： ​其他方式​： Xcode Command Line Tools：终端运行 xcode-select --install。 手动安装：从 Git官网 下载 .dmg 文件安装。 ⚙️ ​二、基础配置​ ​用户身份设置​ （此信息用于标识提交记录）。 ​SSH 密钥配置（连接远程仓库）​​ 生成密钥： 添加公钥到 GitHub&#x2F;GitLab： 测试连接： （解决权限错误：运行 ssh-add ~/.ssh/id_ed25519）。 ⚙️ ​三、基础操作​ ​操作​ ​命令​ ​说明​ ​初始化仓库​ git init 当前目录创建新仓库。 ​克隆仓库​ git clone &lt;远程仓库URL&gt; 下载远程仓库到本地。 ​添加文件​ git add &lt;文件名&gt; 或 git add . 将文件加入暂存区。 ​提交更改​ git commit -m &quot;提交说明&quot; 保存暂存区内容到本地仓库。 ​查看状态​ git status 显示修改&#x2F;未跟踪文件。 ​查看历史​ git log 显示提交记录。 ​撤销修改​ git checkout -- &lt;文件名&gt; 丢弃未提交的修改。 ⚙️ ​四、分支管理​ ​创建与切换分支​ ​合并与删除分支​ （合并冲突需手动解决）。 ⚙️ ​五、远程仓库操作​ ​关联远程仓库​ ​推送与拉取代码​ （首次推送需加 -u：git push -u origin main）。 ⚙️ ​六、高级技巧​ ​回滚代码​ ​配置别名简化命令​ ​解决权限问题​ 若提示 dubious ownership： 💡 ​Git 命令速查表​ ​场景​ ​命令​ 初始化仓库 git init 克隆仓库 git clone &lt;URL&gt; 提交更改 git add . → git commit -m &quot;消息&quot; 分支操作 git branch &#x2F; git checkout &#x2F; git merge 远程同步 git push &#x2F; git pull 查看日志 git log --oneline --graph 💎 ​提示​： ​权限问题​：若遇到 Permission denied，检查 SSH 密钥是否添加至 ssh-agent（eval &quot;$(ssh-agent -s)&quot; → ssh-add ~/.ssh/id_ed25519）。 ​换行符处理​：Mac&#x2F;Linux 建议设置 git config --global core.autocrlf input 避免跨平台换行符问题。 ​图形工具​：可使用 ​Sourcetree​ 可视化操作（下载地址：Sourcetree官网）。 更完整命令参考 Git 官方文档。 "},{"title":"wallpaper｜壁纸爬虫","date":"2021-10-26T16:00:00.000Z","url":"/2021/10/27/2021/10/271342/","tags":[["wallpaper","/tags/wallpaper/"]],"categories":[["Python","/categories/Python/"]],"content":" Wallpaper.py ## fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached问题分析查看源码settings.py中可以发现，里面包含在线获取useragent 其获取useragent的流程为：在CACHE-SERVER中在线下载包含了useragent的json文件，保存为DB变量中的fake_useragent_{version}，存储路径为python的临时文件夹(通过tempfile.gettempdir()获得)。 出现上述问题，是由于DB无法获得在线的useragent的文件 解决方案当然，最简单方便的就是开代理 将CACHE-SERVER中的文件手动下载到本地，保存到临时文件夹，DB变量就不会在线获取了，避免了联网过程，就避免了联网次数超时错误。 在浏览器中打开链接。注意：{version}改成你的fake-useragent模块的版本(下同)。我这里是0.1.11。  另存为fake_useragent_&#123;version&#125;.json,我的为fake_useragent_0.1.11.json 另存为即可。保存路径应该为python的临时存储路径，不知道的可以用以下方式查看。 接下来就正常了。 fake_useragent_0.1.11.json 参考资料 已解决：FAKE-USERAGENT MAXIMUM AMOUNT OF RETRIES REACHED "},{"title":"linux查看gz文件","date":"2021-10-20T16:00:00.000Z","url":"/2021/10/21/2021/10/211356/","tags":[["gz","/tags/gz/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 查看打包封存的日志文件"},{"title":"no exact representable decimal result","date":"2021-10-18T16:00:00.000Z","url":"/2021/10/19/2021/10/191450/","tags":[["java","/tags/java/"]],"categories":[["work","/categories/work/"]],"content":" 问题线上接口报错 问题定位根据cat-traceId到kibana，选择对应的时间以及关键词匹配cat-filter-source-id 错误信息显示问题位于31这台机器，name也具有迷惑性（应该是设置问题），请忽略 ssh到31机器上查看日志，根据关键词fileName检索日志 JAVA中如果用BigDecimal做除法的时候一定要在divide方法中传递第二个参数，定义精确到小数点后几位，否则在不整除的情况下，结果是无限循环小数时，就会抛出以上异常 代码 "},{"title":"JVM｜MaxDirectMemorySize","date":"2021-10-11T16:00:00.000Z","url":"/2021/10/12/2021/10/121604/","tags":[["JVM","/tags/JVM/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题运维的同事反映，生产环境的服务器内存超过90%的占用，jvm相关配置如下 配置 物理机内存为16G,该服务为文件服务，文件上传中有使用到直接内存（堆外内存）。 meminfo内容分析 实际内存大小为15.5 堆内存设置为12 堆外为2 其他320M 所以占用92%为正常情况 meminfo内容分析 属性 大小(k) 说明 扩展说明 MemTotal: 16266400 可供linux内核分配的内存总量。 比物理内存总量少一点，因为主板&#x2F;固件会保留一部分内存、linux内核自己也会占用一部分内存。 MemFree: 251600 表示系统尚未分配的内存。 MemAvailable: 3468952 当前可用内存。 MemFree只是尚未分配的内存，并不是所有可用的内存。有些已经分配掉的内存是可以回收再分配的。比如cache&#x2F;buffer、slab都有一部分是可以回收的，这部分可回收的内存加上MemFree才是系统可用的内存，即MemAvailable。同时要注意，MemAvailable是内核使用特定的算法估算出来的，并不精确。 Buffers: 93764 块设备(block device)所占用的特殊file-backed pages，包括：直接读写块设备，以及文件系统元数据(metadata)比如superblock使用的缓存页。 Buffers内存页同时也在LRU list中，被统计在Active(file)或Inactive(file)之中。 Cached: 3216600 所有file-backed pages 用户进程的内存页分为两种：file-backed pages（与文件对应的内存页），和anonymous pages（匿名页），比如进程的代码、映射的文件都是file-backed，而进程的堆、栈都是不与文件相对应的、就属于匿名页。file-backed pages在内存不足的时候可以直接写回对应的硬盘文件里，称为page-out，不需要用到交换区(swap)；而anonymous pages在内存不足时就只能写到硬盘上的交换区(swap)里，称为swap-out。 SwapCached: 0 SwapCached包含的是被确定要swap-out，但是尚未写入交换区的匿名内存页。 SwapCached内存页会同时被统计在LRU或AnonPages或Shmem中，它本身并不占用额外的内存。 Active: 12247224 active包含active anon和active file LRU是一种内存页回收算法，Least Recently Used,最近最少使用。LRU认为，在最近时间段内被访问的数据在以后被再次访问的概率，要高于最近一直没被访问的页面。于是近期未被访问到的页面就成为了页面回收的第一选择。Linux kernel会记录每个页面的近期访问次数，然后设计了两种LRU list: active list 和 inactive list, 刚访问过的页面放进active list，长时间未访问过的页面放进inactive list，回收内存页时，直接找inactive list即可。另外，内核线程kswapd会周期性地把active list中符合条件的页面移到inactive list中。 Inactive: 1213960 inactive包含inactive anon和inactive file Active(anon): 10450776 活跃匿名页，anonymous pages（匿名页）。 Inactive(anon): 1755780 非活跃匿名页 Active(file): 1796448 活跃文件内存页 Inactive(file): 1512640 非活跃文件内存页 Unevictable: 0 因为种种原因无法回收(page-out)或者交换到swap(swap-out)的内存页 Unevictable LRU list上是不能pageout&#x2F;swapout的内存页，包括VM_LOCKED的内存页、SHM_LOCK的共享内存页（同时被统计在Mlocked中）、和ramfs。在unevictable list出现之前，这些内存页都在Active&#x2F;Inactive lists上，vmscan每次都要扫过它们，但是又不能把它们pageout&#x2F;swapout，这在大内存的系统上会严重影响性能，unevictable list的初衷就是避免这种情况的发生。 Mlocked: 0 被系统调用”mlock()”锁定到内存中的页面。Mlocked页面是不可收回的。 被锁定的内存因为不能pageout&#x2F;swapout，会从Active&#x2F;Inactive LRU list移到Unevictable LRU list上。Mlocked与以下统计项重叠：LRU Unevictable，AnonPages，Shmem，Mapped等。 SwapTotal: 8388604 swap空间总计 SwapFree: 8388604 当前剩余swap Dirty: 736 需要写入磁盘的内存页的大小 Dirty并不包括系统中全部的dirty pages，需要再加上另外两项：NFS_Unstable 和 Writeback，NFS_Unstable是发给NFS server但尚未写入硬盘的缓存页，Writeback是正准备回写硬盘的缓存页。 Writeback: 0 正在被写回的内存页的大小 AnonPages: 12205280 Anonymous pages(匿名页)数量 + AnonHugePages(透明大页)数量 进程所占的内存页分为anonymous pages和file-backed pages，理论上，所有进程的PSS之和 = Mapped + AnonPages。PSS是Proportional Set Size，每个进程实际使用的物理内存（比例分配共享库占用的内存），可以在/proc/[1-9]*/smaps中查看。 Mapped: 73716 正被用户进程关联的file-backed pages Cached包含了所有file-backed pages，其中有些文件当前不在使用，但Cached仍然可能保留着它们的file-backed pages；而另一些文件正被用户进程关联，比如shared libraries、可执行程序的文件、mmap的文件等，这些文件的缓存页就称为mapped。 Shmem: 1276 Shmem统计的内容包括：1.shared memory;2.tmpfs和devtmpfs。所有tmpfs类型的文件系统占用的空间都计入共享内存，devtmpfs是&#x2F;dev文件系统的类型，&#x2F;dev&#x2F;下所有的文件占用的空间也属于共享内存。可以用ls和du命令查看。如果文件在没有关闭的情况下被删除，空间仍然不会释放，shmem不会减小，可以用 lsof -a +L1 /&lt;mount_point&gt; 命令列出这样的文件。 shared memory被视为基于tmpfs文件系统的内存页，既然基于文件系统，就不算匿名页，所以不被计入&#x2F;proc&#x2F;meminfo中的AnonPages，而是被统计进了：Cached或Mapped(当shmem被attached时候)。然而它们背后并不存在真正的硬盘文件，一旦内存不足的时候，它们是需要交换区才能swap-out的，所以在LRU lists里，它们被放在Inactive(anon) 或 Active(anon)或 unevictable （如果被locked的话）里。注意：&#x2F;proc&#x2F;meminfo中的 Shmem 统计的是已经分配的大小，而不是创建时申请的大小。 Slab: 276736 通过slab分配的内存，Slab&#x3D;SReclaimable+SUnreclaim slab是linux内核的一种内存分配器。linux内核的动态内存分配有以下几种方式：1.alloc_pages/__get_free_page:以页为单位分配。2.vmalloc:以字节为单位分配虚拟地址连续的内存块。3.slab:对小对象进行分配，不用为每个小对象分配一个页，节省了空间；内核中一些小对象创建析构很频繁，Slab对这些小对象做缓存，可以重复利用一些相同的对象，减少内存分配次数。4.kmalloc：以slab为基础，以字节为单位分配物理地址连续的内存块。 SReclaimable: 246604 slab中可回收的部分。 SUnreclaim: 30132 slab中不可回收的部分。 KernelStack: 8096 给用户线程分配的内核栈消耗的内存页 每一个用户线程都会分配一个kernel stack（内核栈），内核栈虽然属于线程，但用户态的代码不能访问，只有通过系统调用(syscall)、自陷(trap)或异常(exception)进入内核态的时候才会用到，也就是说内核栈是给kernel code使用的。在x86系统上Linux的内核栈大小是固定的8K或16K。Kernel stack（内核栈）是常驻内存的，既不包括在LRU lists里，也不包括在进程的RSS&#x2F;PSS内存里。RSS是Resident Set Size 实际使用物理内存（包含共享库占用的内存），可以在/proc/[1-9]*/smaps中查看。 PageTables: 33060 Page Table的消耗的内存页 Page Table的用途是翻译虚拟地址和物理地址，它是会动态变化的，要从MemTotal中消耗内存。 NFS_Unstable: 0 发给NFS server但尚未写入硬盘的缓存页 Bounce: 0 bounce buffering消耗的内存页 有些老设备只能访问低端内存，比如16M以下的内存，当应用程序发出一个I&#x2F;O 请求，DMA的目的地址却是高端内存时（比如在16M以上），内核将在低端内存中分配一个临时buffer作为跳转，把位于高端内存的缓存数据复制到此处。 WritebackTmp: 0 正准备回写硬盘的缓存页 CommitLimit: 16521804 overcommit阈值，CommitLimit &#x3D; (Physical RAM * vm.overcommit_ratio &#x2F; 100) + Swap Linux是允许memory overcommit的，即承诺给进程的内存大小超过了实际可用的内存。commit(或overcommit)针对的是内存申请，内存申请不等于内存分配，内存只在实际用到的时候才分配。但可以申请的内存有个上限阈值，即CommitLimit，超出以后就不能再申请了。 Committed_AS: 15198636 所有进程已经申请的内存总大小 VmallocTotal: 34359738367 可分配的虚拟内存总计 VmallocUsed: 38120 已通过vmalloc分配的内存，不止包括了分配的物理内存，还统计了VM_IOREMAP、VM_MAP等操作的值 VM_IOREMAP是把IO地址映射到内核空间、并未消耗物理内存 VmallocChunk: 34359608232 通过vmalloc可分配的虚拟地址连续的最大内存 HardwareCorrupted: 0 因为内存的硬件故障而删除的内存页 AnonHugePages: 8065024 AnonHugePages统计的是Transparent HugePages (THP)，THP与Hugepages不是一回事，区别很大。Hugepages在&#x2F;proc&#x2F;meminfo中是被独立统计的，与其它统计项不重叠，既不计入进程的RSS&#x2F;PSS中，又不计入LRU Active&#x2F;Inactive，也不会计入cache&#x2F;buffer。如果进程使用了Hugepages，它的RSS&#x2F;PSS不会增加。而AnonHugePages完全不同，它与&#x2F;proc&#x2F;meminfo的其他统计项有重叠，首先它被包含在AnonPages之中，而且在&#x2F;proc&#x2F;&#x2F;smaps中也有单个进程的统计，与进程的RSS&#x2F;PSS是有重叠的，如果用户进程用到了THP，进程的RSS&#x2F;PSS也会相应增加，这与Hugepages是不同的。 Transparent Huge Pages 缩写 THP ，这个是 RHEL 6 开始引入的一个功能，在 Linux6 上透明大页是默认启用的。由于 Huge pages 很难手动管理，而且通常需要对代码进行重大的更改才能有效的使用，因此 RHEL 6 开始引入了 Transparent Huge Pages （ THP ）， THP 是一个抽象层，能够自动创建、管理和使用传统大页。THP 为系统管理员和开发人员减少了很多使用传统大页的复杂性 , 因为 THP 的目标是改进性能 , 因此其它开发人员 ( 来自社区和红帽 ) 已在各种系统、配置、应用程序和负载中对 THP 进行了测试和优化。这样可让 THP 的默认设置改进大多数系统配置性能。但是 , 不建议对数据库工作负载使用 THP 。这两者最大的区别在于 : 标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式。 HugePages_Total: 0 预分配的可使用的标准大页池的大小。HugePages在内核中独立管理，只要一经定义，无论是否被使用，都不再属于free memory。 Huge pages(标准大页) 是从 Linux Kernel 2.6 后被引入的，目的是通过使用大页内存来取代传统的 4kb 内存页面， 以适应越来越大的系统内存，让操作系统可以支持现代硬件架构的大页面容量功能。 HugePages_Free: 0 标准大页池中尚未分配的标准大页 HugePages_Rsvd: 0 用户程序预申请的标准大页，尚未真的分配走 HugePages_Surp: 0 标准大页池的盈余 Hugepagesize: 2048 标准大页大小，这里是2M DirectMap4k: 210816 映射为4kB的内存数量 DirectMap所统计的不是关于内存的使用，而是一个反映TLB效率的指标。TLB(Translation Lookaside Buffer)是位于CPU上的缓存，用于将内存的虚拟地址翻译成物理地址，由于TLB的大小有限，不能缓存的地址就需要访问内存里的page table来进行翻译，速度慢很多。为了尽可能地将地址放进TLB缓存，新的CPU硬件支持比4k更大的页面从而达到减少地址数量的目的， 比如2MB，4MB，甚至1GB的内存页，视不同的硬件而定。所以DirectMap其实是一个反映TLB效率的指标。 DirectMap2M: 9226240 映射为2MB的内存数量 DirectMap1G: 9437184 映射为1GB的内存数量 参考资料 JVM源码分析之堆外内存完全解读 堆外内存 之 DirectByteBuffer 详解 linux内存占用分析之meminfo "},{"title":"docker常用服务安装命令","date":"2021-10-08T16:00:00.000Z","url":"/2021/10/09/2021/10/091725/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" Nginx Docker-compose docker安装Mariadb Docker-compose docker安装Redis Redis中设置密码 docker-compose 在上面的配置中，将your_redis_password替换为你想要设置的实际密码。然后，使用docker-compose up命令启动Redis服务，它将使用提供的密码进行身份验证。 docker安装elasticsearch docker安装code-server docker安装RabbitMQ docker安装adguardhome 包含Redis、MySQL、Kafka和MinIO的示例docker-compose.yaml文件 qbittorrent postgres melody 阅读 参考文档 docker hub "},{"title":"十月记录","date":"2021-10-07T16:00:00.000Z","url":"/2021/10/08/2021/10/081218/","tags":[["daily","/tags/daily/"]],"categories":[["work","/categories/work/"]],"content":" 存储类型影响范围以及预估时间数据库 添加字段,旧数据兼容 模块 新建存储模块 *FileHandler 拆分处理逻辑与存储逻辑 文件下载兼容旧数据 测试 由于影响的较为基础，需要测试全面覆盖 预计时间15个工作日 br 文件后缀名 jira任务 RabbitMQ队列测试受影响的消息队列 队列 交换机 路由 doc_convert_test fs_file_converter_exchange safe_convert doc_convert_test_larger_file fs_file_converter_exchange safe_larger_file_convert mq.document.process fs_file_converter_exchange unsafe_convert mq.document.process.larger.file fs_file_converter_exchange unsafe_larger_file_convert RabbitMQ消息丢失问题9-23日，第三方服务报告pdf转换一直未完成，开始排查 登录转换服务器系统发现消息未接收到，fs系统日志显示消息已投递 涉及到的文件id如下 MQ管理界面发现消费者下线了，之前有过一次UAT环境MQ无法连接的情况，fs服务无法重新连接，只能重启服务的情况。 联系运维恢复了mq的服务，消息被灾备环境消费掉， 问题，灾备环境应同步（复制）消息，而不是将消息消费掉，导致服务恢复后消息丢失，无法继续处理。 复现方案启动一个转换任务，由于消费者掉线，生产者的消息投递成功，但是可能被灾备环境的队列的消费者消费掉，消费者服务恢复后，消息丢失的情况 问题消费者掉线，没有自动重连 fs任务优先级 onlyoffice 前端对接 pdf文件图片批注 pdf文件预览签字模块感知回调 开启txt格式文件在线编辑 MQ测试 传开始时间 上传结束时间 转换完成时间 样本个数 完成个数 完成率 2021-10-21 18:40:09 2021-10-21 19:16:55 2021-10-21 20:58:15 4008 4004 问题排查：转换请求未完成，实际请求转换个数为4004个，完成率为100% 统计SQL 时 间 统 计 ： 597711 2021-10-20 18:20:42.490 INFO 1778390 — [io-11002-exec-3] c.c.r.r.s.impl.RabbitTest ServiceImpl : end: 1634724644779 -start:1634725242490&#x3D;:597711 2021-10-20 18:20:42.490 INFO 1778390 — [io-11002-exec-3] c.c.r.r.s.impl.RabbitTest ServiceImpl : 时 间 统 计 ： 597711 1634725422605 2021-10-20 18:35:07.004 INFO 1778789 — [ntContainer#0-1] com.cong.rabbitdemo.worke r.Worker : 系 统 时 间 ： 1634726107004 CICD SQL 添加字段"},{"title":"nginx|代理下载服务只能下载1G的问题","date":"2021-09-29T16:00:00.000Z","url":"/2021/09/30/2021/09/300000/","tags":[["nginx","/tags/nginx/"],["linux","/tags/linux/"],["file","/tags/file/"]],"categories":[["work","/categories/work/"]],"content":" 问题文件服务出现了一个bug,大文件下载只能下载1g的大小，而且出现了&#96;&#96;错误，一开始怀疑是等待超时，但是下载的速度很快，时间并没花费很久。而且下载的大小限制在1G,所以怀疑是文件大小的配置方面的问题。 复现模拟文件下载服务 生成一个2G的文件 文件下载服务 nginx配置 默认配置大小 经测试发现，文件下载到1G就会断开连接，copy result不会打印，配置proxy_max_temp_file_size为允许的下载大小即可。 之前的文件下载测试，默认走的都是nginx代理的静态资源，不会有大小的限制。 参考资料  "},{"title":"xwinwrap|Linux下动态视频壁纸","date":"2021-09-28T16:00:00.000Z","url":"/2021/09/29/2021/09/291121/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" xwinwrapxwinwrap是一种实用软件，由David Reveman&#x2F;Novell编写，它使您的桌面变得很cool。您可以运行屏幕保户程序，播放电影等并且将使它们看起来是您桌面背景的一部分。 安装 mplayer mpv ReadMe.txt 参考资料   "},{"title":"Linux|dd命令","date":"2021-09-27T16:00:00.000Z","url":"/2021/09/28/2021/09/281610/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" ddLinux dd 命令用于读取、转换并输出数据。 dd 可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。 注意：指定数字的地方若以下列字符结尾，则乘以相应的数字：b&#x3D;512；c&#x3D;1；k&#x3D;1024；w&#x3D;2 生成一个2G大小的测试文件 参数说明 实例 &#x2F;dev&#x2F;null&#x2F;dev&#x2F;null——它是空设备，也称为位桶（bit bucket）。任何写入它的输出都会被抛弃。如果不想让消息以标准输出显示或写入文件，那么可以将消息重定向到位桶。 &#x2F;dev&#x2F;zero&#x2F;dev&#x2F;zero，是一个输入设备，你可你用它来初始化文件。该设备无穷尽地提供0，可以使用任何你需要的数目——设备提供的要多的多。他可以用于向设备或文件写入字符串0。 创建交换临时文件 参考   "},{"title":"AmazonWorkDocs","date":"2021-09-25T16:00:00.000Z","url":"/2021/09/26/2021/09/261019/","tags":[["java","/tags/java/"],["WorkDocs","/tags/WorkDocs/"]],"categories":[["work","/categories/work/"]],"content":" 介绍 Amazon WorkDocs 是一项完全托管的用于实现安全的内容创建、存储和协作的服务。借助 Amazon WorkDocs，您可以轻松创建、编辑和共享内容，并且由于它集中存储在 AWS 上，因此可以从任何位置在任何设备上进行访问。Amazon WorkDocs 可使您轻松地与其他人协作，而且还可以让您轻松地共享内容，提供丰富的反馈，以及协作编辑文档。 更多介绍 QuickStartpom 获取用户 内容管理器 上传文档 下载"},{"title":"AWS-S3","date":"2021-09-21T16:00:00.000Z","url":"/2021/09/22/2021/09/221552/","tags":[["java","/tags/java/"],["aws-s3","/tags/aws-s3/"]],"categories":[["work","/categories/work/"]],"content":" AWS-S3fs功能 普通文件上传下载 分片文件上传下载 上传文件转换为pdf 可通过配置代理访问 Demo 创建存储桶 上传对象putObject方法，并为其提供存储桶名称、键名称和要上传的文件。存储桶必须存在，否则将出现错误。 下载对象使用卓越亚马逊客户端的getObject方法，并向其传递要下载的存储桶和对象的名称。如果成功，此方法将返回一个 S3Object。指定的存储桶和对象键必须存在，否则将出现错误。 您可以通过对 getObjectContent 调用 S3Object 来获取对象的内容。这将返回一个 S3ObjectInputStream，其行为与标准 Java InputStream 对象的相同。 以下示例从 S3 下载一个对象，然后将该对象的内容保存到一个文件（使用与对象键相同的名称）： 复制、移动或重命名对象您可以使用 AmazonS3 客户端的copyObject方法。它采用要从中复制的存储桶的名称、要复制的对象以及目标存储桶名称。 注意 您可以将 copyObject 与 deleteObject 配合使用来移动或重命名对象，方式是先将对象复制到新名称 (您可以使用与源和目标相同的存储桶)，然后从对象的旧位置删除对象。 删除元数据deleteObject 一次性删除多个对象 相关文档   "},{"title":"重试｜RetryTemplate&for","date":"2021-09-16T16:00:00.000Z","url":"/2021/09/17/2021/09/171441/","tags":[["java","/tags/java/"]],"categories":[["work","/categories/work/"]],"content":" for循环 RetryTemplate "},{"title":"Java|Spring&Guaua监听订阅（观察者）","date":"2021-09-12T16:00:00.000Z","url":"/2021/09/13/2021/09/131532/","tags":[["spring","/tags/spring/"],["guava","/tags/guava/"]],"categories":[["Java","/categories/Java/"]],"content":" 前言“观察者模式（Observer Pattern）也叫做发布订阅模式（Publish&#x2F;subscribe）,它是一个在项目中经常使用的模式” “定义对象间一种一对多的依赖关系，使得每当一个对象改变状态，则所有依赖于它的对象都会得到通知并被自动更新。” 摘录来自: . “设计模式之禅（第2版）（华章原创精品）。” Apple Books. SpringBoot使用ApplicationEventuser controller UserService UserRegisterEvent 监听者在Spring内部中有多种方式实现监听如：@EventListener注解、实现ApplicationListener泛型接口、实现SmartApplicationListener接口等 @EventListener实现监听 ApplicationListener实现监听这种方式也是Spring之前比较常用的监听事件方式，在实现ApplicationListener接口时需要将监听事件作为泛型传递 SmartApplicationListener实现有序监听 SmartApplicationListener接口继承了全局监听ApplicationListener，并且泛型对象使用的ApplicationEvent来作为全局监听，可以理解为使用SmartApplicationListener作为监听父接口的实现，监听所有事件发布。 既然是监听所有的事件发布，那么SmartApplicationListener接口添加了两个方法supportsEventType、supportsSourceType来作为区分是否是我们监听的事件，只有这两个方法同时返回true时才会执行onApplicationEvent方法。 可以看到除了上面的方法，还提供了一个getOrder方法，这个方法就可以解决执行监听的顺序问题，return的数值越小证明优先级越高，执行顺序越靠前。 Guava|EventBusEventBus是Guava中实现的用于发布&#x2F;订阅模式的事件处理组件，它是设计模式中观察者模式的实现 分布式请使用消息队列MQ参考资料   "},{"title":"页面加载完成后执行js方法","date":"2021-09-07T16:00:00.000Z","url":"/2021/09/08/2021/09/081512/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" js中页面加载完成后执行的几种方式及执行顺序在js和jquery使用中，经常使用到页面加载完成后执行某一方法。通过整理，大概是五种方式（其中有的只是书写方式不一样）。 1：使用jQuery的$(function)&#123;&#125;; 2：使用jquery的$(document).ready(function()&#123;&#125;);前两者本质上没有区别，第1种是第2种的简写方式。两个是document加载完成后就执行方法。 3：使用jQuery的$(window).load(function()&#123;&#125;); 4:使用window.onload = function()&#123;&#125; 第3种和第4种都是等到整个window加载完成执行方法体。两者也没有区别，只是一个使用dom对象，一个使用jQuery对象。 5：在标签上静态绑定onload事件，&lt;body onload=&quot;aaa()&quot;&gt;等待body加载完成，就会执行aaa()方法。 那么，这五种方式，执行的先后顺序是怎么样的呢？ 通过下方代码验证发现： ​1. jQuery的$(function)&#123;&#125;和2. jquery的$(document).ready(function()&#123;&#125;);无论位置放置在哪里，总是优先其余三种方式（原因是：这两种方式是在document加载完成后就执行，后三种是等到整个window页面加载完成后才执行），这两者之间的执行顺序是谁在上方谁优先执行.​3. jQuery的$(window).load(function()&#123;&#125;);4. ​5. &lt;body onload=&quot;aaa()&quot;&gt;总是最后执行。 原文地址"},{"title":"docker安装onlyoffice","date":"2021-09-05T16:00:00.000Z","url":"/2021/09/06/2021/09/031322/","tags":[["docker","/tags/docker/"],["onlyoffice","/tags/onlyoffice/"]],"categories":[["work","/categories/work/"]],"content":" docker命令 onlyoffice-Callback handler文档编辑服务使用JavaScript API的回调Url通知文档存储服务有关文档编辑的状态。文档编辑服务使用 POST 请求与身体中的信息。 Parameter Description Type Presence actions 定义用户使用文档采取行动时收到的对象。类型字段值可以具有以下值：0 - 用户断开文档共同编辑，1 - 新用户连接到文档共同编辑，2 - 用户单击强制按钮。使用字段值是用户标识符。 array of object optional(自选) changeshistory 定义对象阵列与文档更改历史记录。当状态值仅等于2或3时，对象就存在。必须作为属性更改发送对象作为参数发送到刷新历史方法。自版本 4.2 后删除，请改用历史记录。 array of object optional changesurl 使用用于跟踪和显示文档更改历史记录的文档编辑数据定义到文件的链接。当状态值仅等于2或3时，该链接就存在。文件必须保存，其地址必须作为更改Url参数发送，使用设置历史数据方法显示与特定文档版本对应的更改。 string optional forcesavetype 执行强制保存请求时定义启动器的类型。可以具有以下值：0 - 向指挥部执行部队节省请求，1 - 每次完成保存时都会执行强制保存请求（例如单击”保存”按钮），只有在强制选项设置为真实时才能使用。2 - 强制保存请求由定时器执行，并带有服务器配置的设置。当状态值仅等于6或7时，该类型就存在。 integer optional history 使用文档定义对象更改历史记录。当状态值仅等于2或3时，对象就存在。它包含对象更改和服务器反转，必须随着属性更改而发送，并且作为对刷新历史方法的参数发送对象的服务器反转。 object optional key 定义经过编辑的文档标识符。 string required status 定义文档的状态。可以具有以下值：1 - 正在编辑文档，2 - 文档已准备好保存，3 - 文档保存错误已经发生，4 - 文档关闭而无更改，6 - 文档正在编辑中，但当前文档状态已保存，7 - 强制保存文档时发生了错误。 integer required（必须） url 定义要通过文档存储服务保存的编辑文档的链接。当状态值仅等于2或3时，该链接就存在 string optional userdata 定义发送到命令服务的自定义信息，以防其在请求中出现。 string optional users 定义打开文档进行编辑的用户标识符列表;更改文档后，用户将返回最后一个编辑文档的用户标识符（用于状态2和状态 6回复）。 array of string optional 自版本 5.5 以来，根据请求的状态选择回调Url。从版本 4.4 到版本 5.5 开始，从加入共同编辑的最后一个用户开始使用回调Url。在版本 4.4 之前，当共同编辑时，从首次打开文件进行编辑的用户中使用回调Url。 状态 1接收每个用户连接到文档共同编辑或断开连接。他的回电乌尔被使用。 状态 2 （3） 在文档关闭10 秒后收到，以便使用最后一个向文档编辑服务发送更改的用户标识符进行编辑。使用对文件进行最后更改的用户的回调Url。 状态 4在文档关闭后接收，以便编辑，但最后一个用户没有更改。他的回电乌尔被使用。 执行节省力请求时，将收到状态6 （7）。由于版本 6.2，回调乌尔取决于强制类型参数。如果强制类型参数设置为1，则使用单击”保存”按钮的用户的回调Url。如果强制类型参数设置为0或2，则使用对文件进行最后更改的用户的回调Url。从版本 5.5 到版本 6.1 开始，始终使用对文件进行最后更改的用户的回调Url。 文档存储服务的响应文档存储服务必须返回以下响应，否则文档编辑器将显示错误消息 流程图 Java Demo 回调消息 页面 Controller pom"},{"title":"SSH","date":"2021-09-05T16:00:00.000Z","url":"/2021/09/06/2021/09/061803/","tags":[["linux","/tags/linux/"],["ssh","/tags/ssh/"]],"categories":[["work","/categories/work/"]],"content":" 验证 使用安全外壳协议连接远程计算机。 安装 启动 查看IP地址要通过 SSH 连接到远程计算机，必须知道其 IP 地址或可解析的主机名。 账号密码登陆 生成密钥 使用 ssh-copy-id 命令把公钥发送到远程机器上，在此之前要先确保具有远程计算机的 SSH 访问权限。 添加配置文件~/.ssh/config 登陆 sftp 传输文件 问题no matching host key type found. Their offer: ssh-rsa新建.ssh/config,写入以下内容注意 .ssh文件夹 Windows位于自己的用户文件夹下 pem密钥登陆 其中： &#x2F;path&#x2F;to&#x2F;private_key.pem 是您在步骤1中复制的.pem私钥文件的路径。 user 是您要登录的服务器上的用户名。 server_ip_address 是您要连接的服务器的IP地址。 如果您第一次连接服务器，则可能会收到一个提示，询问您是否要将服务器的公钥添加到您本地计算机的 known_hosts 文件中。如果您信任该服务器，可以输入 yes 并按 Enter 键。 输入您的服务器登录密码，如果您的私钥已正确设置，则应该可以成功登录到服务器。 注意：在某些情况下，您的私钥可能不起作用。例如，如果服务器管理员禁用了 SSH 密钥登录或者服务器上的防火墙阻止了 SSH 连接。如果您无法使用 SSH 密钥登录，请联系服务器管理员。"},{"title":"鼠标点击地图获取经纬度","date":"2021-09-01T16:00:00.000Z","url":"/2021/09/02/2021/09/021503/","tags":[["flask","/tags/flask/"],["map","/tags/map/"]],"categories":[["Python","/categories/Python/"]],"content":" 申请key高德地图​ 页面获取鼠标点击经纬度​ 提交到后台服务 Flask后端 Java后端SpringBoot Lnglat.java"},{"title":"Tika读取文件信息","date":"2021-08-30T16:00:00.000Z","url":"/2021/08/31/2021/08/311553/","tags":[["tiki","/tags/tiki/"]],"categories":[["work","/categories/work/"]],"content":" 新版本更新时间：2021-10-28 鉴于早期tika版本的内存占用较高,新版本进行了一些优化，这里升级版本和更新使用方式  示例 使用自动检测分析器进行分析 选择不同的输出格式有了 Tika，您可以以多种不同的格式返回文件的文本内容。这些可以是文件的一部分的纯文本，html，xhtml，xhtml等。这是根据您向 Parser 提供的内容处理器进行控制的。 BodyContentHandler 解析到纯文本 ToXMLContentHandler 解析到 XHTML 将纯文本分块流式传输 需求 解决方案 PDF—-&gt;PDFbox excel—-&gt; EasyExcel 适合对excel操作要求较高的场景 Office—-&gt;POI Txt&#x2F;csv—&gt;FileUtils(Apache Common) all–&gt;Apache Tika 以上 可以使用 Maven依赖 Tika读取PDF Tika读取Office文件 Tika读取txt"},{"title":"ArchLinux字体安装&硬盘测速","date":"2021-08-28T16:00:00.000Z","url":"/2021/08/29/2021/08/291509/","tags":[["font","/tags/font/"],["hdd","/tags/hdd/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 字体安装 更新缓存 硬盘测速"},{"title":"Java获取文件编码","date":"2021-08-22T16:00:00.000Z","url":"/2021/08/23/2021/08/231055/","tags":[["encode","/tags/encode/"]],"categories":[["Java","/categories/Java/"]],"content":" 问题 通过头信息获取 使用juniversalchardet 支持的编码 代码实现 github   google  注意：maven仓库有三个版本，分别是谷歌，molliza以及GitHub版本的 最新的为GitHub版本，支持的编码格式较多。用法上区别不大。"},{"title":"8月记录","date":"2021-08-16T16:00:00.000Z","url":"/2021/08/17/2021/08/161812/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" OnlyOffice 本地化部署nacos文档 清理缓存 安装依赖 8ae5e60b7b71c1f4017b71c83e1c0005 8ae5e60b7b71c1f4017b71ca8df90006 "},{"title":"Java压缩解压zip文件","date":"2021-08-16T16:00:00.000Z","url":"/2021/08/17/2021/08/171717/","tags":[["zip","/tags/zip/"]],"categories":[["work","/categories/work/"]],"content":" "},{"title":"MacOS常用软件安装","date":"2021-08-14T16:00:00.000Z","url":"/2021/08/15/2021/08/152013/","tags":[["mac","/tags/mac/"]],"categories":[["work","/categories/work/"]],"content":" homebrew brew 更新cask 更新 brew cask 安装的软件 tabby 支持lrzsz文件上传的终端 keepassxc密码管理 motrix下载工具  VSCode typoramarkdown编辑器 PicGo neofetch VLC&#x2F;IINA播放器 Calibre电子书管理 toolbox jetbrains开发全家桶 listen1 drawio画图工具 "},{"title":"Linux文件名称长度限制","date":"2021-08-09T16:00:00.000Z","url":"/2021/08/10/2021/08/100953/","tags":[["file","/tags/file/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 查看限制 limits.h 上述文件内容分别说明了文件名和相对路径的最大长度，字符指的是ASCII字符，如果是中文字符，需要视编码而定utf-8和gbk对应的长度不一样。 测试脚本"},{"title":"MQ异步任务设计","date":"2021-08-08T16:00:00.000Z","url":"/2021/08/09/2021/08/091101/","tags":[["MQ","/tags/MQ/"]],"categories":[["work","/categories/work/"]],"content":" 流程图 Future自带的异步模式，依赖于当前的应用，队列任务位于内存中，服务一旦停止，未处理的任务即消失，适用场景有限。基于健壮性考虑，应使用消息队列执行异步任务。 任务表的设计，应包含足够的信息，例如请求参数，如果较多，可使用json字符串longtext类型保存；包含业务相关请求参数，比如请求的来源，业务系统等；包含任务状态tinyint类型；包含异常信息，如果任务无法正常完成，那么原因是什么，最好包含具体的错误情况。 消息发送，应该在任务保存之后进行，否则可能会出现任务查询不到，或者任务查询需要的相关信息无法获取到，比如文件保存的操作比较耗时。 MQ交换机类型选择direct/topic路由模式，广播模式会使路由失效，所有绑定该交换机的队列都会收到消息。 重试次数限制，异常需要为AMQE或者其子类 任务完成之后可以使用消息通知调用者，也可以提供接口，调用方轮询任务查询结果。 异步任务要考虑到超时的问题，经过一段时间没有完成，抛出异常，取消该次任务（任务失败）。 任务超时伪代码 MQ消息接收之后数据库查询失败的问题处理异步任务时，任务先提交至数据库保存，然后发送MQ消息通知消费者进行任务的下一步处理。 但是在消费者处理任务时，查询数据库却发现此任务不存在。 排查之后发现，由于在提交任务时存在事务，尚未提交，即发送了消息，所以另外一个事务无法读取未提交的事务，因此查询不到此次任务。 解决方法去除任务提交时的事务 topic交换机Topic Exchange Topic Exchange 转发消息主要是根据通配符。 在这种交换机下，队列和交换机的绑定会定义一种路由模式，那么，通配符就要在这种路由模式和路由键之间匹配后交换机才能转发消息。 在这个例子中，我们将发送所有跟动物有关的消息，这些消息将会发送到由三个单词，两个点号组成的routing key，第一个单词了表示的是速度，第二个单词表示颜色，第三个单词表示种类： “..“。 我们创建三个绑定关系：队列Q1绑定到绑定键*.orange.* ，队列Q2绑定到*.*.rabbit和lazy.#。 总结下来就是： 队列Q1对橘黄色（orange）颜色的所有动物感兴趣；队列Q2对所有的兔子（rabbit）和所有慢吞吞（lazy）的动物感兴趣。一个路由为 “quick.orange.rabbit”的消息，将会被转发到这两个队列，路由为”lazy.orange.elephant”的消息也被转发给这两个队列，路由为 “quick.orange.fox”的消息将只被转发到Q1队列，路由为 “lazy.brown.fox”的消息将只被转发到Q2队列。”lazy.pink.rabbit” 只被转发到Q2队列一次（虽然它匹配绑定键*.*.rabbit和lazy.#），路由为 “quick.brown.fox”的消息与任何一个绑定键都不匹配，因此将会被丢弃。 如果我们发送的消息的的路由是由一个单词“orangle”或4个单词”quick.orangle.male.rabbit“将会怎样？会因为与任何一个绑定键不匹配而被丢弃。 另一方面，路由为 “lazy.orange.male.rabbit”的消息，因为匹配”lazy.#”绑定键，因而会被转发到Q2队列。 Topic交换器非常强大，可以像其他类型的交换器一样工作： 当一个队列的绑定键是”#”是，它将会接收所有的消息，而不再考虑所接收消息的路由键，就像是fanout交换器一样； 当一个队列的绑定键没有用到”#“和”*“时，它又像direct交换一样工作。 在这种交换机模式下： 路由键必须是一串字符，用句号（.） 隔开，比如说 agreements.us，或者 agreements.eu.stockholm 等。路由模式必须包含一个 星号（*），主要用于匹配路由键指定位置的一个单词，比如说，一个路由模式是这样子：agreements..b.*，那么就只能匹配路由键是这样子的：第一个单词是 agreements，第四个单词是 b。 井号（#）就表示相当于一个或者多个单词，例如一个匹配模式是 agreements.eu.berlin.#，那么，以agreements.eu.berlin 开头的路由键都是可以的"},{"title":"微信小程序-云函数","date":"2021-08-05T16:00:00.000Z","url":"/2021/08/06/2021/08/061327/","tags":[["js","/tags/js/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 官方文档 登录 查询 更新 时间格式化添加moment-time依赖到package.json 当前目录下执行npm install "},{"title":"application.yml配置对象集合","date":"2021-08-05T16:00:00.000Z","url":"/2021/08/06/2021/08/061429/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":" 对象 配置类 自定义配置文件提示 配置文件 单元测试"},{"title":"ping Operation not permitted","date":"2021-08-05T16:00:00.000Z","url":"/2021/08/06/2021/08/061511/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" ping: icmp open socket: Operation not permitted 的解决办法为ping加上suid即可。 然后就能正常执行了。"},{"title":"docker制作libreoffice镜像","date":"2021-08-02T16:00:00.000Z","url":"/2021/08/03/2021/08/031522/","tags":[["docker","/tags/docker/"],["libreoffice","/tags/libreoffice/"]],"categories":[["work","/categories/work/"]],"content":" 基础安装包 libreoffice 历史版本 openjdk oracle-jdk 以上下载好和dockerfile放在一个文件夹 Dockerfile build 运行 参考"},{"title":"Linux-log日志搜索","date":"2021-07-22T16:00:00.000Z","url":"/2021/07/23/2021/07/231527/","tags":[["log","/tags/log/"]],"categories":[["Linux","/categories/Linux/"]],"content":" less grep cat tail sed find"},{"title":"HLS视频播放压力测试","date":"2021-07-20T16:00:00.000Z","url":"/2021/07/21/2021/07/211833/","tags":[["hls","/tags/hls/"]],"categories":[["work","/categories/work/"]],"content":" 测试工具 Usage 编译好的应用下载地址"},{"title":"命令行检查文件夹的大小","date":"2021-07-19T16:00:00.000Z","url":"/2021/07/20/2021/07/201548/","tags":[["file","/tags/file/"]],"categories":[["work","/categories/work/"]],"content":" Windows Linux"},{"title":"Linux下删除乱码文件","date":"2021-07-12T16:00:00.000Z","url":"/2021/07/13/2021/07/131402/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 问题通过lrzsz工具上传文件，有时候失败会产生很多乱码文件，又能删除当前文件夹，即在不知道文件名称的前提下如何删除文件。 rm可以直接删除指定名称的文件 fide删除首先进入乱码文件所在的目录 删除 非空文件夹删除 参考资料 linux下删除乱码文件、目录 "},{"title":"ffmpeg硬件转码No decoder surfaces left","date":"2021-07-02T16:00:00.000Z","url":"/2021/07/03/2021/07/031809/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["work","/categories/work/"]],"content":" 版本 错误信息 解决方案查看视频文件使用的编码器，部分编码器不支持GPU解码。 h264即为编码器，使用对应的硬解h264_civid ffmpeg支持的硬解 选择对应的硬解 使用hevc_nvenc编码器， 但是存在web端解码问题，只有声音没有画面，因为目前浏览器对于h265的支持不是很好，使用VLC等可以很好的播放，关于h265和h264这里不多作说明 指定解码器为h264_cuvid 查看帮助"},{"title":"FFmpeg硬件加速","date":"2021-06-30T16:00:00.000Z","url":"/2021/07/01/2021/07/011029/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["work","/categories/work/"]],"content":" 安装显卡驱动以及cuda系统信息 自动安装 手动安装驱动Nvidia 查找对应的型号下载安装 也可以使用ubuntu仓库中的进行安装即可 验证输出 ffmpeg 第三方编译安装  BtbN 编译安装请参考官方文档，ffmpeg或者Nvidia的指南，不建议参照第三方文档 命令 验证 可以通过nvidia-smi查看使用硬件转码的情况 示例 cpu多码率转码 单分辨率转码 GPU转码 参考资料 Using_FFmpeg_with_NVIDIA_GPU_Hardware_Acceleration "},{"title":"Linux查看IO","date":"2021-06-29T16:00:00.000Z","url":"/2021/06/30/2021/06/301123/","tags":[["io","/tags/io/"]],"categories":[["Linux","/categories/Linux/"]],"content":" iostat安装方式 demo 选项 说明 rrqm&#x2F;s 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm&#x2F;s 每秒对该设备的写请求被合并次数 r&#x2F;s 每秒完成的读次数 w&#x2F;s 每秒完成的写次数 rkB&#x2F;s 每秒读数据量(kB为单位) wkB&#x2F;s 每秒写数据量(kB为单位) avgrq-sz 平均每次IO操作的数据量(扇区数为单位) avgqu-sz 平均等待处理的IO请求队列长度 await 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位) svctm 平均每次IO请求的处理时间(毫秒为单位) %util 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 iotop安装 top vmstat percona-toolkit demo 参考资料 linux查看磁盘io使用情况 "},{"title":"wsl2","date":"2021-06-23T16:00:00.000Z","url":"/2021/06/24/2021/06/242211/","tags":[["wsl","/tags/wsl/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 安装wsl2 需要先启用“适用于 Linux 的 Windows 子系统”可选功能，然后才能在 Windows 上安装 Linux 分发。 安装 WSL 2 之前，必须启用“虚拟机平台”可选功能。 下载 Linux 内核更新包 安装ArchLinux 安装LxRunOffline 安装LxRunOffline 下载ArchLinux Archlinux 找到对应的版本 安装 进入系统 修改国内源 安装 设置 wsl中文显示修改配置文件，取消注释zh_CN.UTF-8 en_US.UTF-8 WSL2迁移 参考 适用于 Linux 的 Windows 子系统安装指南 (Windows 10) 知乎-在WSL2中安装ArchLinux WSL2迁移方法 "},{"title":"FFmpeg使用GPU硬件转码","date":"2021-06-21T16:00:00.000Z","url":"/2021/06/22/2021/06/222237/","tags":[["ffmpeg","/tags/ffmpeg/"]],"categories":[["work","/categories/work/"]],"content":" 系统环境 安装显卡驱动自动安装方式 手动安装方式Nvidia驱动下载 安装 验证 CUDA 其他桌面版本可能需要禁用nouveau ffmpeg FfmpegUtils.java"},{"title":"shell执行时间计时以及写入文件","date":"2021-06-14T16:00:00.000Z","url":"/2021/06/15/2021/06/152258/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" TIME 参数 -o 或 –output&#x3D;FILE：设定结果输出档。这个选项会将 time 的输出写入 所指定的档案中。如果档案已经存在，系统将覆写其内容。 -a 或 –append：配合 -o 使用，会将结果写到档案的末端，而不会覆盖掉原来的内容。 -f FORMAT 或 –format&#x3D;FORMAT：以 FORMAT 字串设定显示方式。当这个选项没有被设定的时候，会用系统预设的格式。不过你可以用环境变数 time 来设定这个格式，如此一来就不必每次登入系统都要设定一次。 1)实际时间(real time): 从command命令行开始执行到运行终止的消逝时间； 2)用户CPU时间(user CPU time): 命令执行完成花费的用户CPU时间，即命令在用户态中执行时间总和； 3)系统CPU时间(system CPU time): 命令执行完成花费的系统CPU时间，即命令在核心态中执行时间总和。 其中，用户CPU时间和系统CPU时间之和为CPU时间，即命令占用CPU执行的时间总和。实际时间要大于CPU时间，因为Linux是多任务操作系统，往往在执行一条命令时，系统还要处理其它任务。 时间 计算分秒 写入文件"},{"title":"异步之MQ","date":"2021-06-05T16:00:00.000Z","url":"/2021/06/06/2021/06/062210/","tags":[["MQ","/tags/MQ/"]],"categories":[["work","/categories/work/"]],"content":" 多线程关于异步任务的处理，首先想到的是使用多线程，CompleateFuture进行任务的处理，请求直接返回，等待完成之后，再通知调用方。代码以及测试都没有问题，但是忽略了一个重要问题，基于多线程的任务异步处理，都是提交到内存中的队列，假如服务因为不可抗拒因素宕机，那么任务就会消失。 MQ异步MQ的作用 解耦 异步 削峰 流程图任务&#x3D;&#x3D;&#x3D;&#x3D;&gt;MQ&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;处理&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;MQ&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;通知调用者已处理完成 代码示例 MQ方案 Rabbit MQ Kafka 消息延迟 低(毫秒级) 高(秒级) 一致性 强 弱 消息吞吐量 低 高 重复消费 不支持 支持 在数据吞吐量高的场景：比如数据清洗，日志同步，痕迹处理，需要用kafka做消息中间件在一致性要求高的场景：比如状态一致性同步，分布式事务，需要用rabbitMq rabbitMq使用过程中需要注意的点 消息需要设置存活时间(Time-To-Live)，防止消息队列堆积过大 消费者要设置ACK次数 消费者在消费消息的时候，要确保幂等性，防止消息重复消费导致的数据重复异常 "},{"title":"AES&RSA加密","date":"2021-05-29T16:00:00.000Z","url":"/2021/05/30/2021/05/302220/","tags":[["AES","/tags/AES/"],["RSA","/tags/RSA/"]],"categories":[["work","/categories/work/"]],"content":" Java AES 加密 | 解密 Java RSA 加密|解密"},{"title":"HLS杂项","date":"2021-05-22T16:00:00.000Z","url":"/2021/05/23/2021/05/232234/","tags":[["hls","/tags/hls/"]],"categories":[["work","/categories/work/"]],"content":" ffmpeg 1 2 ffmpeg多线程转码 java对视频进行转码  ffmpeg图片尺寸 ffmpeg无损压缩图片 ffmpeg转码保持原始尺寸比例 可能会遇到以下错误 [libx264 @ 0x2f08120] height not divisible by 2 (640x853) "},{"title":"Jquery添加删除元素","date":"2021-05-15T16:00:00.000Z","url":"/2021/05/16/2021/05/162004/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 需求工作中有需要使用动态生成条件进行多条件查询，页面上有就难免需要用到元素添加和删除。 添加 append() - 在被选元素的结尾插入内容 prepend() - 在被选元素的开头插入内容 after() - 在被选元素之后插入内容 before() - 在被选元素之前插入内容 append&#x2F;prepend 是在选择元素内部嵌入。 after&#x2F;before 是在元素外面追加。 append after 删除 remove() - 删除被选元素（及其子元素） empty() - 从被选元素中删除子元素 过滤被删除的元素jQuery remove() 方法也可接受一个参数，允许您对被删元素进行过滤。 该参数可以是任何 jQuery 选择器的语法。 下面的例子删除 class=&quot;italic&quot; 的所有 &lt;p&gt; 元素 如果子元素符合过滤器中条件而父元素不符合的话，是不会删除符合条件的子元素，即过滤器中条件只能作用于同级，不能作用于子元素。 如果想删除,选择器选择子元素列表，然后再筛选其中符合条件的进行删除。 "},{"title":"shell文本重复统计","date":"2021-05-14T16:00:00.000Z","url":"/2021/05/15/2021/05/150000/","tags":[["shell","/tags/shell/"]],"categories":[["Linux","/categories/Linux/"]],"content":" count.sh"},{"title":"幂等","date":"2021-05-13T16:00:00.000Z","url":"/2021/05/14/2021/05/142133/","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 幂等**幂等 F(F(x)&#x3D;F(x))**多次运算结果一致 常见幂等 1)select查询天然幂等 2)delete删除也是幂等,删除同一个多次效果一样 3)update直接更新某个值的,幂等 4)update更新累加操作的,非幂等 5)insert非幂等操作,每次新增一条 产生原因由于重复点击或者网络重发 eg: 1)点击提交按钮两次; 2)点击刷新按钮; 3)使用浏览器后退按钮重复之前的操作，导致重复提交表单; 4)使用浏览器历史记录重复提交表单; 5)浏览器重复的HTTP请; 6)nginx重发等情况; 7)分布式RPC的try重发等; 解决方案 前端js提交之后禁止按钮 使用Post&#x2F;Redirect&#x2F;Get模式，提交之后执行页面重定向，跳转到成功页面 session中存放一个特殊标记，表单中隐藏字段，提交后与session中存储的值进行比较，一致首次提交后移除session中的标记，再次提交不一致不再处理。 借助header头设置缓存控制头Cache-control 数据库：insert唯一索引，update乐观锁，大数据高并发不适用 悲观锁，锁住，先查询是否已经存在之后再执行对应的操作 本地锁，只适用于单机部署的应用(ConcurrentHashMap) 分布式锁Redis(Redison) "},{"title":"JDK14:ArrayList扩容","date":"2021-05-10T16:00:00.000Z","url":"/2021/05/11/2021/05/111512/","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" API说明List接口的可调整大小的阵列实现。 实现所有可选列表操作，并允许所有元素，包括null 。 除了实现List接口之外，此类还提供了一些方法来操作内部用于存储列表的数组的大小。 （这个类大致相当于Vector ，除了它是不同步的。）该size ， isEmpty ， get ， set ， iterator和listIterator操作在固定时间内运行。 add操作以分摊的常量时间运行，即添加n个元素需要O（n）时间。 所有其他操作都以线性时间运行（粗略地说）。 与LinkedList实施相比，常数因子较低。 每个ArrayList实例都有一个容量 。 容量是用于存储列表中元素的数组的大小。 它始终至少与列表大小一样大。 随着元素添加到ArrayList，其容量会自动增加。 除了添加元素具有恒定的摊销时间成本这一事实之外，未指定增长策略的详细信息。 在使用ensureCapacity操作添加大量元素之前，应用程序可以增加ArrayList实例的容量。 这可能会减少增量重新分配的数量。 请注意，此实现不同步。 如果多个线程同时访问ArrayList实例，并且至少有一个线程在结构上修改了列表，则必须在外部进行同步。 （结构修改是添加或删除一个或多个元素的任何操作，或显式调整后备数组的大小;仅设置元素的值不是结构修改。）这通常通过同步一些自然封装的对象来实现。名单。 如果不存在此类对象，则应使用Collections.synchronizedList方法“包装”该列表。 这最好在创建时完成，以防止意外地不同步访问列表： 此类的iterator和listIterator方法返回的迭代器是快速失败的 ：如果在创建迭代器之后的任何时候对列表进行结构修改，除了通过迭代器自己的remove或add方法之外，迭代器将抛出ConcurrentModificationException 。 因此，在并发修改的情况下，迭代器快速而干净地失败，而不是在未来的未确定时间冒任意，非确定性行为的风险。 请注意，迭代器的快速失败行为无法得到保证，因为一般来说，在存在不同步的并发修改时，不可能做出任何硬性保证。 失败快速迭代器以尽力而为的方式抛出ConcurrentModificationException 。 因此，编写依赖于此异常的程序以确保其正确性是错误的： 迭代器的快速失败行为应该仅用于检测错误。 该集合是Java Coions Framework的成员。 更多信息： 个别方法介绍 返回值类型 方法 描述 boolean removeIf​(Predicate&lt;? super E&gt; filter) 删除此集合中满足给定谓词的所有元素。 void ensureCapacity​(int minCapacity) 如有必要，增加此 ArrayList实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 向 ArrayList 添加大量元素之前最好先使用ensureCapacity 方法，以减少增量重新分配的次数。 扩容以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。 grow&amp;newCapacity hugeCapacity 扩容时机当插入数据，导致size + 1 &gt; elementData.length，也就是需要从容量超过目前数组长度时，需要进行扩容。 k&#x3D;1.5时，就能充分利用前面已经释放的空间。如果k &gt;&#x3D; 2，新容量刚刚好永远大于过去所有废弃的数组容量。 为什么不取扩容固定容量呢？ 扩容的目的需要综合考虑这两种情况： 扩容容量不能太小，防止频繁扩容，频繁申请内存空间 + 数组频繁复制 扩容容量不能太大，需要充分利用空间，避免浪费过多空间； 而扩容固定容量，很难决定到底取多少值合适，取任何具体值都不太合适，因为所需数据量往往由数组的客户端在具体应用场景决定。依赖于当前已经使用的量 * 系数， 比较符合实际应用场景。比如，我现在已经用到一个数组100的容量，接下来很可能会有这个数量级的数据需要插入。 为什么是1.5，而不是1.2，1.25，1.8或者1.75？ 因为1.5 可以充分利用移位操作，减少浮点数或者运算时间和运算次数。 "},{"title":"多路复用","date":"2021-05-08T16:00:00.000Z","url":"/2021/05/09/2021/05/091630/","tags":[["io","/tags/io/"]],"categories":[["work","/categories/work/"]],"content":" select，poll，epoll都是IO多路复用的机制。所谓I&#x2F;O多路复用机制，就是说通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I&#x2F;O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I&#x2F;O则无需自己负责进行读写，异步I&#x2F;O的实现会负责把数据从内核拷贝到用户空间。 select和poll的实现比较相似，目前也有很多为人诟病的缺点，epoll可以说是select和poll的增强版。 一、select实现1、使用copy_from_user从用户空间拷贝fd_set到内核空间2、注册回调函数pollwait3、遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）4、以tcp_poll为例，其核心实现就是pollwait，也就是上面注册的回调函数。5、__pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk-&gt;sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。6、poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。7、如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。8、把fd_set从内核空间拷贝到用户空间。总结：select的几大缺点：（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大（3）select支持的文件描述符数量太小了，默认是1024 二、poll实现poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构。其他的都差不多。 三、epoll实现epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。 对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。说明一下这个回调机制的原理，其实很简单，看一下select和epoll在把current加入fd对应的设备等待队列时使用的代码：select: 其中init_waitqueue_entry实现如下： 上面的代码是说建立一个poll_table_entry结构entry，首先把current设置为entry-&gt;wait的private成员，同时把default_wake_function设为entry-&gt;wait的func成员，然后把entry-&gt;wait链入到wait_address中（这个wait_address就是设备的等待队列，在tcp_poll中就是sk_sleep）。再看一下epoll： 其中init_waitqueue_func_entry的实现如下： 可以看到，总体和select的实现是类似的，只不过它是创建了一个eppoll_entry结构pwq，只不过pwq-&gt;wait的func成员被设置成了回调函数ep_poll_callback（而不是default_wake_function，所以这里并不会有唤醒操作，而只是执行回调函数），private成员被设置成了NULL。最后吧pwq-&gt;wait链入到whead中（也就是设备等待队列中）。这样，当设备等待队列中的进程被唤醒时，就会调用ep_poll_callback了。 再梳理一下，当epoll_wait时，它会判断就绪链表中有没有就绪的fd，如果没有，则把current进程加入一个等待队列(file-&gt;private_data-&gt;wq)中，并在一个while（1）循环中判断就绪队列是否为空，并结合schedule_timeout实现睡一会，判断一会的效果。如果current进程在睡眠中，设备就绪了，就会调用回调函数。在回调函数中，会把就绪的fd放入就绪链表，并唤醒等待队列(file-&gt;private_data-&gt;wq)中的current进程，这样epoll_wait又能继续执行下去了。对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max察看,一般来说这个数目和系统内存关系很大。总结：1、select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。2、select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。"},{"title":"计算机网络","date":"2021-04-28T16:00:00.000Z","url":"/2021/04/29/2021/04/292237/","tags":[["web","/tags/web/"]],"categories":[["work","/categories/work/"]],"content":" OSI与TCP&#x2F;IP 五层协议 应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 、FTP等协议。数据单位为报文。 传输层 ：传输层对上层应用层，提供处于网络连接中的两台计算机之间的数据传输。由于应用层协议很多，定义通用的传输层协议就可以支持不断增多的应用层协议。运输层包括两种协议：TCP(Transmission Control Protocol,传输控制协议 )，提供面向连接、可靠的数据传输服务，数据单位为报文段；用户数据报协议 UDP，提供无连接、尽最大努力的数据传输服务，数据单位为用户数据报。TCP 主要提供完整性服务，UDP 主要提供及时性服务。 网络层 ：为主机提供数据传输服务。而传输层协议是为主机中的进程提供数据传输服务。网络层把传输层传递下来的报文段或者用户数据报封装成分组。 数据链路层 ：网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同一链路的主机提供数据传输服务。数据链路层把网络层传下来的分组封装成帧。 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 三次握手四次挥手 客户端–发送带有 SYN 标志的数据包–⼀次握⼿–服务端 服务端–发送带有 SYN&#x2F;ACK 标志的数据包–⼆次握⼿–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握⼿–服务端 为什么要三次握⼿三次握⼿的⽬的是建⽴可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，⽽三次握⼿最主要的⽬的就是双⽅确认⾃⼰与对⽅的发送与接收是正常的 四次挥⼿ 客户端-发送⼀个 FIN，⽤来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回⼀ 个 ACK，确认序号为收到的序号加1 。和 SYN ⼀样，⼀个FIN 将占⽤⼀个序号 服务器-关闭与客户端的连接，发送⼀个FIN给客户端 客户端-发回 ACK 报⽂确认，并将确认序号设置为收到序号加1 为什么要四次挥⼿任何⼀⽅都可以在数据传送结束后发出连接释放的通知，待对⽅确认后进⼊半关闭状态。当另⼀⽅也没有数据再发送的时候，则发出连接释放通知，对⽅确认后就完全关闭了TCP连接。 TCP 协议如何保证可靠传输 应⽤数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每⼀个包进⾏编号，接收⽅对数据包进⾏排序，把有序数据传送给应⽤层。 校验和： TCP 将保持它⾸部和数据的检验和。这是⼀个端到端的检验和，⽬的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报⽂段和不确认收到此报⽂段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每⼀⽅都有固定⼤⼩的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收⽅来不及处理发送⽅的数据，能提示发送⽅降低发送的速率，防⽌包丢失。TCP 使⽤的流量控制协议是可变⼤⼩的滑动窗⼝协议。 （TCP 利⽤滑动窗⼝实现流量控制） 拥塞控制： 当⽹络拥塞时，减少数据的发送。 ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完⼀个分组就停⽌发送，等待对⽅确认。在收到确认后再发下⼀个分组。 超时重传： 当 TCP 发出⼀个段后，它启动⼀个定时器，等待⽬的端确认收到这个报⽂段。如果不能及时收到⼀个确认，将重发这个报⽂段。 DNSDNS（Domain Name System）服务是和 HTTP 协议一样位于应用层的协议。它提供域名到 IP 地址之间的解析服务。 计算机既可以被赋予 IP 地址，也可以被赋予主机名和域名。比如 www.hackr.jp。用户通常使用主机名或域名来访问对方的计算机，而不是直接通过 IP 地址访问。因为与 IP 地址的一组纯数字相比，用字母配合数字的表示形式来指定计算机名更符合人类的记忆习惯。 但要让计算机去理解名称，相对而言就变得困难了。因为计算机更擅长处理一长串数字。 为了解决上述的问题，DNS 服务应运而生。DNS 协议提供通过域名查找 IP 地址，或逆向从 IP 地址反查域名的服务。 URL&amp;URI URL（Uniform Resource Locator，统一资源定位符）。 URL 正是使用 Web 浏览器等访问 Web 页面时需要输入的网页地址。 URI 是 Uniform Resource Identifier 的缩写,统一资源标识符. URI 就是由某个协议方案表示的资源的定位标识符。协议方案是指访问资源所使用的协议类型名称。 URI 用字符串标识某一互联网资源，而 URL 表示资源的地点（互联网上所处的位置）。可见 URL 是 URI 的子集。 参考资料《图解HTTP》"},{"title":"Spring Bean生命周期","date":"2021-04-27T16:00:00.000Z","url":"/2021/04/28/2021/04/280000/","tags":[["Spring","/tags/Spring/"]],"categories":[["Java","/categories/Java/"]],"content":" 首先说一下Servlet的生命周期：实例化，初始init，接收请求service，销毁destroy；Spring上下文中的Bean生命周期也类似，如下： （1）实例化Bean：对于BeanFactory容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。对于ApplicationContext容器，当容器启动结束后，通过获取BeanDefinition对象中的信息，实例化所有的bean。 （2）设置对象属性（依赖注入）：实例化后的对象被封装在BeanWrapper对象中，紧接着，Spring根据BeanDefinition中的信息 以及 通过BeanWrapper提供的设置属性的接口完成依赖注入。 （3）处理Aware接口：接着，Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给Bean：①如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String beanId)方法，此处传递的就是Spring配置文件中Bean的id值；②如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory()方法，传递的是Spring工厂自身。③如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文； （4）BeanPostProcessor：如果想对Bean进行一些自定义的处理，那么可以让Bean实现了BeanPostProcessor接口，那将会调用postProcessBeforeInitialization(Object obj, String s)方法。 （5）InitializingBean 与 init-method：如果Bean在Spring配置文件中配置了 init-method 属性，则会自动调用其配置的初始化方法。 （6）如果这个Bean实现了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Objectobj, String s)方法；由于这个方法是在Bean初始化结束时调用的，所以可以被应用于内存或缓存技术以上几个步骤完成后，Bean就已经被正确创建了，之后就可以使用这个Bean了 （7）DisposableBean：当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法； （8）destroy-method：最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法 环境Java Spring BeanTest MyBeanProcessor MyInstantiationAwareBeanPostProcessor MyBeanConfig 结果 SpringBoot启动流程平时开发springboot项目的时候，一个SpringBootApplication注解加一个main方法就可以启动服务器运行起来（默认tomcat） 主要流程如下 0.启动main方法开始 1.初始化配置：通过类加载器，（loadFactories）读取classpath下所有的spring.factories配置文件，创建一些初始配置对象；通知监听者应用程序启动开始，创建环境对象environment，用于读取环境配置 如 application.yml 2.创建应用程序上下文-createApplicationContext，创建 bean工厂对象 3.刷新上下文（启动核心）3.1 配置工厂对象，包括上下文类加载器，对象发布处理器，beanFactoryPostProcessor3.2 注册并实例化bean工厂发布处理器，并且调用这些处理器，对包扫描解析(主要是class文件)3.3 注册并实例化bean发布处理器 beanPostProcessor3.4 初始化一些与上下文有特别关系的bean对象（创建tomcat服务器）3.5 实例化所有bean工厂缓存的bean对象（剩下的）3.6 发布通知-通知上下文刷新完成（启动tomcat服务器） 4.通知监听者-启动程序完成 启动中，大部分对象都是BeanFactory对象通过反射创建"},{"title":"EXPLAIN","date":"2021-04-26T16:00:00.000Z","url":"/2021/04/27/2021/04/271029/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":" EXPLAIN Output Columns 列名 说明 id 执行编号，标识select所属的行。如果在语句中没子查询或关联查询，只有唯一的select，每行都将显示1。否则，内层的select语句一般会顺序编号，对应于其在原始语句中的位置 select_type 显示本行是简单或复杂select。如果查询有任何复杂的子查询，则最外层标记为PRIMARY（DERIVED、UNION、UNION RESUlT） table 访问引用哪个表（引用某个查询，如“derived3”） type 数据访问&#x2F;读取操作类型（ALL、index、range、ref、eq_ref、const&#x2F;system、NULL） possible_keys 揭示哪一些索引可能有利于高效的查找 key 显示mysql决定采用哪个索引来优化查询 key_len 显示mysql在索引里使用的字节数 ref 显示了之前的表在key列记录的索引中查找值所用的列或常量 rows 为了找到所需的行而需要读取的行数，估算值，不精确。通过把所有rows列值相乘，可粗略估算整个查询会检查的行数 Extra 额外信息，如using index、filesort等 idid是用来顺序标识整个查询中SELELCT 语句的，在嵌套查询中id越大的语句越先执行。该值可能为NULL，如果这一行用来说明的是其他行的联合结果。 select_type表示查询的类型 类型 说明 simple 简单子查询，不包含子查询和union primary 包含union或者子查询，最外层的部分标记为primary subquery 一般子查询中的子查询被标记为subquery，也就是位于select列表中的查询 derived 派生表——该临时表是从子查询派生出来的，位于form中的子查询 union 位于union中第二个及其以后的子查询被标记为union，第一个就被标记为primary如果是union位于from中则标记为derived union result 用来从匿名临时表里检索结果的select被标记为union result dependent union 顾名思义，首先需要满足UNION的条件，及UNION中第二个以及后面的SELECT语句，同时该语句依赖外部的查询 subquery 子查询中第一个SELECT语句 dependent subquery 和DEPENDENT UNION相对UNION一样 table对应行正在访问哪一个表，表名或者别名 关联优化器会为查询选择关联顺序，左侧深度优先 当from中有子查询的时候，表名是derivedN的形式，N指向子查询，也就是explain结果中的下一列 当有union result的时候，表名是union 1,2等的形式，1,2表示参与union的query id 注意：MySQL对待这些表和普通表一样，但是这些“临时表”是没有任何索引的。 typetype显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是：system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL ，一般来说，得保证查询至少达到range级别，最好能达到ref。 类型 说明 All 最坏的情况,全表扫描 index 和全表扫描一样。只是扫描表的时候按照索引次序进行而不是行。主要优点就是避免了排序, 但是开销仍然非常大。如在Extra列看到Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要小很多 range 范围扫描，一个有限制的索引扫描。key 列显示使用了哪个索引。当使用&#x3D;、 &lt;&gt;、&gt;、&gt;&#x3D;、&lt;、&lt;&#x3D;、IS NULL、&lt;&#x3D;&gt;、BETWEEN 或者 IN 操作符,用常量比较关键字列时,可以使用 range ref 一种索引访问，它返回所有匹配某个单个值的行。此类索引访问只有当使用非唯一性索引或唯一性索引非唯一性前缀时才会发生。这个类型跟eq_ref不同的是，它用在关联操作只使用了索引的最左前缀，或者索引不是UNIQUE和PRIMARY KEY。ref可以用于使用&#x3D;或&lt;&#x3D;&gt;操作符的带索引的列。 eq_ref 最多只返回一条符合条件的记录。使用唯一性索引或主键查找时会发生 （高效） const 当确定最多只会有一行匹配的时候，MySQL优化器会在查询前读取它而且只读取一次，因此非常快。当主键放入where子句时，mysql把这个查询转为一个常量（高效） system 这是const连接类型的一种特例，表仅有一行满足条件。 Null 意味说mysql能在优化阶段分解查询语句，在执行阶段甚至用不到访问表或索引（高效） possible_keys显示查询使用了哪些索引，表示该索引可以进行高效地查找，但是列出来的索引对于后续优化过程可能是没有用的 keykey列显示MySQL实际决定使用的键（索引）。如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 key_lenkey_len列显示MySQL决定使用的键长度。如果键是NULL，则长度为NULL。使用的索引的长度。在不损失精确性的情况下，长度越短越好 。 refref列显示使用哪个列或常数与key一起从表中选择行。 rowsrows列显示MySQL认为它执行查询时必须检查的行数。注意这是一个预估值。 ExtraExtra是EXPLAIN输出中另外一个很重要的列，该列显示MySQL在查询过程中的一些详细信息，MySQL查询优化器执行查询的过程中对查询计划的重要补充信息。 类型 说明 Using filesort MySQL有两种方式可以生成有序的结果，通过排序操作或者使用索引，当Extra中出现了Using filesort 说明MySQL使用了后者，但注意虽然叫filesort但并不是说明就是用了文件来进行排序，只要可能排序都是在内存里完成的。大部分情况下利用索引排序更快，所以一般这时也要考虑优化查询了。使用文件完成排序操作，这是可能是ordery by，group by语句的结果，这可能是一个CPU密集型的过程，可以通过选择合适的索引来改进性能，用索引来为查询结果排序。 Using temporary 用临时表保存中间结果，常用于GROUP BY 和 ORDER BY操作中，一般看到它说明查询需要优化了，就算避免不了临时表的使用也要尽量避免硬盘临时表的使用。 Not exists MYSQL优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行， 就不再搜索了。 Using index 说明查询是覆盖了索引的，不需要读取数据文件，从索引树（索引文件）中即可获得信息。如果同时出现using where，表明索引被用来执行索引键值的查找，没有using where，表明索引用来读取数据而非执行查找动作。这是MySQL服务层完成的，但无需再回表查询记录。 Using index condition 这是MySQL 5.6出来的新特性，叫做“索引条件推送”。简单说一点就是MySQL原来在索引上是不能执行如like这样的操作的，但是现在可以了，这样减少了不必要的IO操作，但是只能用在二级索引上。 Using where 使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。注意：Extra列出现Using where表示MySQL服务器将存储引擎返回服务层以后再应用WHERE条件过滤。 Using join buffer 使用了连接缓存：Block Nested Loop，连接算法是块嵌套循环连接;Batched Key Access，连接算法是批量索引连接 impossible where where子句的值总是false，不能用来获取任何元组 select tables optimized away 在没有GROUP BY子句的情况下，基于索引优化MIN&#x2F;MAX操作，或者对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。 distinct 优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作 使用解释提高查询性能Using Explain to Improve Query Performance 参考资料高性能mysql第三版MySQL 8.0 参考手册"},{"title":"Spring事务管理原理","date":"2021-04-26T16:00:00.000Z","url":"/2021/04/27/2021/04/272139/","tags":[["Spring","/tags/Spring/"]],"categories":[["work","/categories/work/"]],"content":" 原生的事务管理在没有Spring存在的时候，事务就已经诞生了。其实框架依赖的还是底层提供的能力，只不过它对这一过程的抽象和复用。这里我们用底层的API来了解下事务管理的过程(JDBC为例)： 上面是一个原生操作事务的一个例子，这些过程也是Spring事务逃不开的，只不过在为了编程的效率让这一过程自动化或是透明化的你无法感知罢了。而我们之后做的就是逐步还原这一自动化的过程。 Spring提供的事务APISpring提供了很多关于事务的API。但是最为基本的就是PlatformTransactionManager、TransactionDefintion和TransactionStatus。 事务管理器——PlatformTransactionManagerPlatformTransactionManager是事务管理器的顶层接口。事务的管理是受限于具体的数据源的(例如，JDBC对应的事务管理器就是DatasourceTransactionManager)，因此PlatformTransactionManager只规定了事务的基本操作:创建事务，提交事物和回滚事务。 同时为了简化事务管理器的实现，Spring提供了一个抽象类AbstractPlatformTransactionManager，规定了事务管理器的基本框架，仅将依赖于具体平台的特性作为抽象方法留给子类实现。 事务状态——TransactionStatus事务状态是我对TransactionStatus这个类的直译。其实我觉得这个类可以直接当作事务的超集来看(包含了事务对象，并且存储了事务的状态)。PlatformTransactionManager.getTransaction()时创建的也正是这个对象。这个对象的方法都和事务状态相关: 事务属性的定义——TransactionDefinitionTransactionDefinition表示一个事务的定义，将根据它规定的特性去开启事务。事务的传播等级和隔离级别的常量同样定义在这个接口中。 编程式使用Spring事务有了上述这些API，就已经可以通过编程的方式实现Spring的事务控制了。但是Spring官方建议不要直接使用PlatformTransactionManager这一偏低层的API来编程，而是使用TransactionTemplate和TransactionCallback这两个偏向用户层的接口。示例代码如下: 以上就是Spring事务最基本的原理。但是为什么这些过程对我们似乎都不可见呢？那是因为这些过程都通过AOP的方式被织入了我们的业务逻辑中。所以，像要深入了解Spring事务原理，还需要了解AOP的原理。 参考资料"},{"title":"Portainer-docker可视化管理工具","date":"2021-04-24T16:00:00.000Z","url":"/2021/04/25/2021/04/251546/","tags":[["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 部署 创建数据卷 启动 官方文档portainer"},{"title":"redo/undo log","date":"2021-04-24T16:00:00.000Z","url":"/2021/04/25/2021/04/251621/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 开始而事务的ACID是通过InnoDB日志和锁来保证。事务的隔离性是通过数据库锁的机制实现的，持久性通过redo log（重做日志）来实现，原子性和一致性通过Undo log来实现。UndoLog的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为UndoLog）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 和Undo Log相反，RedoLog记录的是新数据的备份。在事务提交前，只要将RedoLog持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。 把需要事务的微服务聚合成一个单机服务，使用数据库的本地事务。 redo&#x2F;undo log undo log和redo logo都是InnoDB的功能，都是事务日志 undo log是逻辑日志，记录是操作记录日志，用来回滚行记录到某个版本。根据每行记录进行记录。 redo log是物理日志，记录的是新数据 undo log是为了保证事务原子性而设计的， redo log是为了保证事务持久性设置的。 undo log在InnoDB中用来实现多版本控制，执行rollback操作时，undo log可以作为事务回滚的快照读参考，而- redo log是备份的最新数据位置，系统宕机时，只要重启mysql服务，就可以将未持久保存的数据持久到磁盘 在事务提交前，只要将 Redo Log 持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是 Redo Log 已经持久化。系统可以根据 Redo Log 的内容，将所有数据恢复到最新的状态。 刷新脏页"},{"title":"多文件读取去重后保存至数据库","date":"2021-04-18T16:00:00.000Z","url":"/2021/04/19/2021/04/191533/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 之前的写法也可以完成任务，但是效率很慢。 新的方案采用多线程读取文件，之后到redis的set集合去重，然后再将数据保存至数据库中。 SpringBoot&amp;MyBatis文件读取保存至数据库(一) 线程池 getFileList readFile Redis set去重 CompletableFuture另外一种方案就是使用CompletableFuture进行改进，可以等待所有文件读取完毕后，进行保存的操作 "},{"title":"CompletableFuture","date":"2021-04-18T16:00:00.000Z","url":"/2021/04/19/2021/04/191557/","tags":[["CompletableFuture","/tags/CompletableFuture/"]],"categories":[["Java","/categories/Java/"]],"content":" 多文件读取去重后保存 CompletableFuture另外一种方案就是使用CompletableFuture进行改进，可以等待所有文件读取完毕后，进行保存的操作 API 方法详细信息runAsync 和 supplyAsync方法没有指定Executor的方法会使用ForkJoinPool.commonPool() 作为它的线程池执行异步代码。如果指定线程池，则使用指定的线程池运行。以下所有的方法都类同。 whenComplete：是执行当前任务的线程执行继续执行 whenComplete 的任务。 whenCompleteAsync：是执行把 whenCompleteAsync 这个任务继续提交给线程池来进行执行。 thenApply当一个线程依赖另一个线程时，可以使用 thenApply 方法来把这两个线程串行化。 demo: handle thenAccept接收任务的处理结果，并消费处理，无返回结果 thenRun返回一个新的完成阶段，当此阶段正常完成时，执行给定操作。不会把计算的结果传给 thenRun 方法 thenCombine返回一个新的完成阶段，当此阶段和另一个给定阶段都正常完成时，使用两个结果作为对所提供函数的参数执行。 demo:两个随机数相加 thenAcceptBoth当两个CompletionStage都执行完成后，把结果一块交给thenAcceptBoth来进行消耗 applyToEither两个CompletionStage，谁执行返回的结果快，我就用那个CompletionStage的结果进行下一步的转化操作。 其他API可查询参考资料等。 参考资料Java8 API"},{"title":"SpringBoot&MyBatis文件读取保存至数据库(一)","date":"2021-04-16T16:00:00.000Z","url":"/2021/04/17/2021/04/171159/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 需求大量的句子经过分词以及标记后，需要将对应的词和词性保存到数据库中。 文件样式 python方案 Java方案pom application.yml 统一返回格式 全局异常处理 Mapper 处理方法 启动类"},{"title":"排序","date":"2021-04-11T16:00:00.000Z","url":"/2021/04/12/2021/04/121535/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 常见排序列表 交换方法 选择排序（select） 冒泡 插入排序 希尔排序 归并排序 快速排序 计数排序"},{"title":"常用正则表达式","date":"2021-04-09T16:00:00.000Z","url":"/2021/04/10/2021/04/101430/","tags":[["re","/tags/re/"]],"categories":[["work","/categories/work/"]],"content":" 一、校验数字的表达式 数字：^[0-9]*$ n位的数字：^\\d{n}$ 至少n位的数字**：^\\d{n,}$** m-n位的数字：^\\d{m,n}$ 零和非零开头的数字：^(0|[1-9][0-9]*)$ 非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(.[0-9]{1,2})?$ 带1-2位小数的正数或负数：^(-)?\\d+(.\\d{1,2})$ 正数、负数、和小数：^(-|+)?\\d+(.\\d+)?$ 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$ 有1~3位小数的正实数：^[0-9]+(.[0-9]{1,3})?$ 非零的正整数：^[1-9]\\d*$ 或 ^([1-9][0-9]*){1,3}$ 或 ^+?[1-9][0-9]*$ 非零的负整数：^-[1-9][]0-9”*$ 或 ^-[1-9]\\d*$ 非负整数：^\\d+$ 或 ^[1-9]\\d*|0$ 非正整数：^-[1-9]\\d*|0$ 或 ^((-\\d+)|(0+))$ 非负浮点数：^\\d+(.\\d+)?$ 或 ^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0$ 非正浮点数：^((-\\d+(.\\d+)?)|(0+(.0+)?))$ 或 ^(-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*))|0?.0+|0$ 正浮点数：^[1-9]\\d*.\\d*|0.\\d*[1-9]\\d*$ 或 ^(([0-9]+.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*.[0-9]+)|([0-9]*[1-9][0-9]*))$ 负浮点数：^-([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*)$ 或 ^(-(([0-9]+.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*.[0-9]+)|([0-9]*[1-9][0-9]*)))$ 浮点数：^(-?\\d+)(.\\d+)?$ 或 ^-?([1-9]\\d*.\\d*|0.\\d*[1-9]\\d*|0?.0+|0)$ 二、校验字符的表达式 汉字：^[\\u4e00-\\u9fa5]{0,}$ 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]{4,40}$ 长度为3-20的所有字符：^.{3,20}$ 由26个英文字母组成的字符串：^[A-Za-z]+$ 由26个大写英文字母组成的字符串：^[A-Z]+$ 由26个小写英文字母组成的字符串：^[a-z]+$ 由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$ 由数字、26个英文字母或者下划线组成的字符串：^\\w+$ 或 ^\\w{3,20}$ 中文、英文、数字包括下划线：^[\\u4E00-\\u9FA5A-Za-z0-9_]+$ 中文、英文、数字但不包括下划线等符号：^[\\u4E00-\\u9FA5A-Za-z0-9]+$ 或 ^[\\u4E00-\\u9FA5A-Za-z0-9]{2,20}$ 可以输入含有^%&amp;’,;&#x3D;?$&quot;等字符：[^%&amp;’,;&#x3D;?$\\x22]+ 禁止输入含有的字符：**[^\\x22]+** 三、特殊需求表达式 Email地址：^\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*.\\w+([-.]\\w+)*$ 域名：[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+.? InternetURL：[a-zA-z]+:&#x2F;&#x2F;[^\\s]* 或 ^http:&#x2F;&#x2F;([\\w-]+.)+[\\w-]+(&#x2F;[\\w-.&#x2F;?%&amp;&#x3D;]*)?$ 手机号码：^(13[0-9]|14[5|7]|15[0|1|2|3|4|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\d{8}$ 电话号码(“XXX-XXXXXXX”、”XXXX-XXXXXXXX”、”XXX-XXXXXXX”、”XXX-XXXXXXXX”、”XXXXXXX”和”XXXXXXXX)：^((\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$ 国内电话号码(0511-4405222、021-87888822)：\\d{3}-\\d{8}|\\d{4}-\\d{7} 电话号码正则表达式（支持手机号码，3-4位区号，7-8位直播号码，1－4位分机号）: ((\\d{11})|^((\\d{7,8})|(\\d{4}|\\d{3})-(\\d{7,8})|(\\d{4}|\\d{3})-(\\d{7,8})-(\\d{4}|\\d{3}|\\d{2}|\\d{1})|(\\d{7,8})-(\\d{4}|\\d{3}|\\d{2}|\\d{1}))$) 身份证号(15位、18位数字)，最后一位是校验位，可能为数字或字符X：(^\\d{15}$)|(^\\d{18}$)|(^\\d{17}(\\d|X|x)$) 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\\w{5,17}$ 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在 8-10 之间)：^(?&#x3D;.*\\d)(?&#x3D;.*[a-z])(?&#x3D;.*[A-Z])[a-zA-Z0-9]{8,10}$ 强密码(必须包含大小写字母和数字的组合，可以使用特殊字符，长度在8-10之间)：^(?&#x3D;.*\\d)(?&#x3D;.*[a-z])(?&#x3D;.*[A-Z]).{8,10}$ 日期格式：^\\d{4}-\\d{1,2}-\\d{1,2} 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$ 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$ 钱的输入格式： 有四种钱的表示形式我们可以接受:”10000.00” 和 “10,000.00”, 和没有 “分” 的 “10000” 和 “10,000”：^[1-9][0-9]*$ 这表示任意一个不以0开头的数字,但是,这也意味着一个字符”0”不通过,所以我们采用下面的形式：^(0|[1-9][0-9]*)$ 一个0或者一个不以0开头的数字.我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$ 这表示一个0或者一个可能为负的开头不为0的数字.让用户以0开头好了.把负号的也去掉,因为钱总不能是负的吧。下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$ 必须说明的是,小数点后面至少应该有1位数,所以”10.”是不通过的,但是 “10” 和 “10.2” 是通过的：^[0-9]+(.[0-9]{2})?$ 这样我们规定小数点后面必须有两位,如果你认为太苛刻了,可以这样：^[0-9]+(.[0-9]{1,2})?$ 这样就允许用户只写一位小数.下面我们该考虑数字中的逗号了,我们可以这样：^[0-9]{1,3}(,[0-9]{3})*(.[0-9]{1,2})?$ 1到3个数字,后面跟着任意个 逗号+3个数字,逗号成为可选,而不是必须：^([0-9]+|[0-9]{1,3}(,[0-9]{3})*)(.[0-9]{1,2})?$ 备注：这就是最终结果了,别忘了”+”可以用”*”替代如果你觉得空字符串也可以接受的话(奇怪,为什么?)最后,别忘了在用函数时去掉去掉那个反斜杠,一般的错误都在这里 xml文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\.[x|X][m|M][l|L]$ 中文字符的正则表达式：[\\u4e00-\\u9fa5] 双字节字符：[^\\x00-\\xff] (包括汉字在内，可以用来计算字符串的长度(一个双字节字符长度计2，ASCII字符计1)) 空白行的正则表达式：\\n\\s*\\r (可以用来删除空白行) HTML标记的正则表达式：&lt;(\\S*?)[^&gt;]*&gt;.*?|&lt;.*? &#x2F;&gt; ( 首尾空白字符的正则表达式：^\\s*|\\s*$或(^\\s*)|(\\s*$) (可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式) 腾讯QQ号：[1-9][0-9]{4,} (腾讯QQ号从10000开始) 中国邮政编码：[1-9]\\d{5}(?!\\d) (中国邮政编码为6位数字) IPv4地址：((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})(.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})){3} 参考菜鸟教程-正则表达式"},{"title":"InnoDB解决幻读","date":"2021-04-09T16:00:00.000Z","url":"/2021/04/10/2021/04/101449/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["Java","/categories/Java/"]],"content":" 事务隔离级别 READ UNCOMMITTED（未提交读） 在READ UNCOMMITTED级别，事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读（Dirty Read）。这个级别会导致很多问题，从性能上来说，READ UNCOMMITTED不会比其他的级别好太多，但却缺乏其他级别的很多好处，除非真的有非常必要的理由，在实际应用中一般很少使用。 READ COMMITTED（提交读） 大多数数据库系统的默认隔离级别都是READ COMMITTED（但MySQL不是）。READ COMMITTED满足前面提到的隔离性的简单定义：一个事务开始时，只能“看见”已经提交的事务所做的修改。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别有时候也叫做不可重复读（nonrepeatable read），因为两次执行同样的查询，可能会得到不一样的结果。 REPEATABLE READ（可重复读） REPEATABLE READ解决了脏读的问题。该级别保证了在同一个事务中多次读取同样记录的结果是一致的。但是理论上，可重复读隔离级别还是无法解决另外一个幻读（Phantom Read）的问题。所谓幻读，指的是当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行（Phantom Row）。InnoDB和XtraDB存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）解决了幻读的问题。本章稍后会做进一步的讨论。 可重复读是MySQL的默认事务隔离级别。 SERIALIZABLE（可串行化） SERIALIZABLE是最高的隔离级别。它通过强制事务串行执行，避免了前面说的幻读的问题。简单来说，SERIALIZABLE会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用的问题。实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。 多版本并发控制MySQL的大多数事务型存储引擎实现的都不是简单的行级锁。基于提升并发性能的考虑，它们一般都同时实现了多版本并发控制（MVCC）。不仅是MySQL，包括Oracle、PostgreSQL等其他数据库系统也都实现了MVCC，但各自的实现机制不尽相同，因为MVCC没有一个统一的实现标准。 可以认为MVCC是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 MVCC的实现，**是通过保存数据在某个时间点的快照来实现的。**也就是说，不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。如果之前没有这方面的概念，这句话听起来就有点迷惑。熟悉了以后会发现，这句话其实还是很容易理解的。 前面说到不同存储引擎的MVCC实现是不同的，典型的有乐观（optimistic）并发控制和悲观（pessimistic）并发控制。下面我们通过InnoDB的简化版行为来说明MVCC是如何工作的。 InnoDB的MVCC，是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的并不是实际的时间值，而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。下面看一下在REPEATABLE READ隔离级别下，MVCC具体是如何操作的。 SELECT InnoDB会根据以下两个条件检查每行记录： InnoDB只查找版本早于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。行的删除版本要么未定义，要么大于当前事务版本号。这可以确保事务读取到的行，在事务开始之前未被删除。只有符合上述两个条件的记录，才能返回作为查询结果。 INSERT InnoDB为新插入的每一行保存当前系统版本号作为行版本号。 DELETE InnoDB为删除的每一行保存当前系统版本号作为行删除标识。 UPDATE InnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。 保存这两个额外系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。 MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作。其他两个隔离级别都和MVCC不兼容，因为READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行。而SERIALIZABLE则会对所有读取的行都加锁。 解决PhantomProblem在默认的事务隔离级别下，即REPEATABLE READE下，InnoDB存储引擎采用Next-Key Locking机制来避免Phantom Problem(幻读)问题。其他数据库可能需要在SERIALIZABLE的事务隔离级别下才能解决幻读问题。 幻读是指在同一事务下，连续执行两次同样的SQL可能会导致不同的结果，第二次返回之前不存在的行。 对于上面的SQL语句，其锁住的不是5这单个值，而是对（2,+∞）这个范围加了X锁，因此任何对于这个范围的插入都是不被允许的，从而避免了幻读。 参考 《高性能MySQL：第三版》 《MySQL技术内幕InnoDB存储引擎》 "},{"title":"一条SQL执行很慢的原因","date":"2021-04-07T16:00:00.000Z","url":"/2021/04/08/2021/04/081525/","tags":[["sql","/tags/sql/"]],"categories":[["Java","/categories/Java/"]],"content":" 分类讨论一条 SQL 语句执行的很慢，那是每次执行都很慢呢？还是大多数情况下是正常的，偶尔出现很慢呢？所以我觉得，我们还得分以下两种情况来讨论。 1、大多数情况是正常的，只是偶尔会出现很慢的情况。 2、在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。 针对这两种情况，我们来分析下可能是哪些原因导致的。 二、针对偶尔很慢的情况一条 SQL 大多数情况正常，偶尔才能出现很慢的情况，针对这种情况，我觉得这条SQL语句的书写本身是没什么问题的，而是其他原因导致的，那会是什么原因呢？ 1、数据库在刷新脏页（flush）我也无奈啊当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在内存中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 刷脏页有下面4种场景（后两种不用太关注“性能”问题）： **redolog写满了：**redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，就会导致我们平时正常的SQL语句突然执行的很慢，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。 **内存不够用了：**如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。 **MySQL 认为系统“空闲”的时候：**这时系统没什么压力。 **MySQL 正常关闭的时候：**这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 2、拿不到锁这个就比较容易想到了，我们要执行的这条语句，刚好这条语句涉及到的表，别人在用，并且加锁了，我们拿不到锁，只能慢慢等待别人释放锁了。或者，表没有加锁，但要使用到的某个一行被加锁了，这个时候，我也没办法啊。 如果要判断是否真的在等待锁，我们可以用 show processlist这个命令来查看当前的状态哦，这里我要提醒一下，有些命令最好记录一下，反正，我被问了好几个命令，都不知道怎么写，呵呵。 诊断间歇性问题间歇性数据库性能问题的实际案例： 应用通过curl从一个运行得很慢的外部服务来获取汇率报价的数据。 memcached缓存中的一些重要条目过期，导致大量请求落到MySQL以重新生成缓存条目。 DNS查询偶尔会有超时现象。 可能是由于互斥锁争用，或者内部删除查询缓存的算法效率太低的缘故，MySQL的查询缓存有时候会导致服务有短暂的停顿。 当并发度超过某个阈值时，InnoDB的扩展性限制导致查询计划的优化需要很长的时间。 使用SHOW PROCESSLIST这个方法是通过不停地捕获SHOW PROCESSLIST的输出，来观察是否有大量线程处于不正常的状态或者有其他不正常的特征。例如查询很少会长时间处于“statistics”状态，这个状态一般是指服务器在查询优化阶段如何确定表关联的顺序——通常都是非常快的。另外，也很少会见到大量线程报告当前连接用户是“未经验证的用户（Unauthenticated user）”，这只是在连接握手的中间过程中的状态，当客户端等待输入用于登录的用户信息的时候才会出现。 使用SHOW PROCESSLIST命令时，在尾部加上\\G可以垂直的方式输出结果，这很有用，因为这样会将每一行记录的每一列都单独输出为一行，这样可以方便地使用sort|uniq|sort一类的命令来计算某个列值出现的次数： SHOW STATUS和SHOW PROCESSLIST。 使用查询日志如果要通过查询日志发现问题，需要开启慢查询日志并在全局级别设置long_query_time为0，并且要确认所有的连接都采用了新的设置。这可能需要重置所有连接以使新的全局设置生效；或者使用Percona Server的一个特性，可以在不断开现有连接的情况下动态地使设置强制生效。 如果因为某些原因，不能设置慢查询日志记录所有的查询，也可以通过tcpdump和pt-query-digest工具来模拟替代。 下来我们来访分析下第二种情况，我觉得第二种情况的分析才是最重要的 三、针对一直都这么慢的情况如果在数据量一样大的情况下，这条 SQL 语句每次都执行的这么慢，那就就要好好考虑下你的 SQL 书写了，下面我们来分析下哪些原因会导致我们的 SQL 语句执行的很不理想。 我们先来假设我们有一个表，表里有下面两个字段,分别是主键 id，和两个普通字段 c 和 d。 1、扎心了，没用到索引没有用上索引，我觉得这个原因是很多人都能想到的，例如你要查询这条语句 （1）、字段没有索引 刚好你的 c 字段上没有索引，那么抱歉，只能走全表扫描了，你就体验不会索引带来的乐趣了，所以，这回导致这条查询语句很慢。 （2）、字段有索引，但却没有用索引 好吧，这个时候你给 c 这个字段加上了索引，然后又查询了一条语句 我想问大家一个问题，这样子在查询的时候会用索引查询吗？ 答是不会，如果我们在字段的左边做了运算，那么很抱歉，在查询的时候，就不会用上索引了，所以呢，大家要注意这种字段上有索引，但由于自己的疏忽，导致系统没有使用索引的情况了。 正确的查询应该如下 有人可能会说，右边有运算就能用上索引？难道数据库就不会自动帮我们优化一下，自动把 c - 1&#x3D;1000 自动转换为 c &#x3D; 1000+1。 不好意思，确实不会帮你，所以，你要注意了。 （3）、函数操作导致没有用上索引 如果我们在查询的时候，对字段进行了函数操作，也是会导致没有用上索引的，例如 这里我只是做一个例子，假设函数 pow 是求 c 的 n 次方，实际上可能并没有 pow(c,2)这个函数。其实这个和上面在左边做运算也是很类似的。 所以呢，一条语句执行都很慢的时候，可能是该语句没有用上索引了，不过具体是啥原因导致没有用上索引的呢，你就要会分析了，我上面列举的三个原因，应该是出现的比较多的吧。 2、呵呵，数据库自己选错索引了我们在进行查询操作的时候，例如 我们知道，主键索引和非主键索引是有区别的，主键索引存放的值是整行字段的数据，而非主键索引上存放的值不是整行字段的数据，而且存放主键字段的值。不大懂的可以看这篇文章： 【思维导图-索引篇】搞定数据库索引就是这么简单 里面有说到主键索引和非主键索引的区别 也就是说，我们如果走 c 这个字段的索引的话，最后会查询到对应主键的值，然后，再根据主键的值走主键索引，查询到整行数据返回。 好吧扯了这么多，其实我就是想告诉你，就算你在 c 字段上有索引，系统也并不一定会走 c 这个字段上的索引，而是有可能会直接扫描扫描全表，找出所有符合 100 &lt; c and c &lt; 100000 的数据。 为什么会这样呢？ 其实是这样的，系统在执行这条语句的时候，会进行预测：究竟是走 c 索引扫描的行数少，还是直接扫描全表扫描的行数少呢？显然，扫描行数越少当然越好了，因为扫描行数越少，意味着I&#x2F;O操作的次数越少。 如果是扫描全表的话，那么扫描的次数就是这个表的总行数了，假设为 n；而如果走索引 c 的话，我们通过索引 c 找到主键之后，还得再通过主键索引来找我们整行的数据，也就是说，需要走两次索引。而且，我们也不知道符合 100 c &lt; and c &lt; 10000 这个条件的数据有多少行，万一这个表是全部数据都符合呢？这个时候意味着，走 c 索引不仅扫描的行数是 n，同时还得每行数据走两次索引。 所以呢，系统是有可能走全表扫描而不走索引的。那系统是怎么判断呢？ 判断来源于系统的预测，也就是说，如果要走 c 字段索引的话，系统会预测走 c 字段索引大概需要扫描多少行。如果预测到要扫描的行数很多，它可能就不走索引而直接扫描全表了。 那么问题来了，**系统是怎么预测判断的呢？**这里我给你讲下系统是怎么判断的吧，虽然这个时候我已经写到脖子有点酸了。 系统是通过索引的区分度来判断的，一个索引上不同的值越多，意味着出现相同数值的索引越少，意味着索引的区分度越高。我们也把区分度称之为基数，即区分度越高，基数越大。所以呢，基数越大，意味着符合 100 &lt; c and c &lt; 10000 这个条件的行数越少。 所以呢，一个索引的基数越大，意味着走索引查询越有优势。 那么问题来了，怎么知道这个索引的基数呢？ 系统当然是不会遍历全部来获得一个索引的基数的，代价太大了，索引系统是通过遍历部分数据，也就是通过采样的方式，来预测索引的基数的。 扯了这么多，重点的来了，居然是采样，那就有可能出现失误的情况，也就是说，c 这个索引的基数实际上是很大的，但是采样的时候，却很不幸，把这个索引的基数预测成很小。例如你采样的那一部分数据刚好基数很小，然后就误以为索引的基数很小。然后就呵呵，系统就不走 c 索引了，直接走全部扫描了。 所以呢，说了这么多，得出结论：由于统计的失误，导致系统没有走索引，而是走了全表扫描，而这，也是导致我们 SQL 语句执行的很慢的原因。 这里我声明一下，系统判断是否走索引，扫描行数的预测其实只是原因之一，这条查询语句是否需要使用使用临时表、是否需要排序等也是会影响系统的选择的。 不过呢，我们有时候也可以通过强制走索引的方式来查询，例如 我们也可以通过 来查询索引的基数和实际是否符合，如果和实际很不符合的话，我们可以重新来统计索引的基数，可以用这条命令 来重新统计分析。 既然会预测错索引的基数，这也意味着，当我们的查询语句有多个索引的时候，系统有可能也会选错索引哦，这也可能是 SQL 执行的很慢的一个原因。 是否向数据库请求了不需要的数据 查询不需要的记录 多表关联时返回全部列 总是取出全部列 重复查询相同的数据 好吧，就先扯这么多了，你到时候能扯出这么多，我觉得已经很棒了，下面做一个总结。 总结以上是我的总结与理解，最后一个部分，我怕很多人不大懂数据库居然会选错索引，所以我详细解释了一下，下面我对以上做一个总结。 一个 SQL 执行的很慢，我们要分两种情况讨论： 1、大多数情况下很正常，偶尔很慢，则有如下原因 (1)、数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。 (2)、执行的时候，遇到锁，如表锁、行锁。 2、这条 SQL 语句一直执行的很慢，则有如下原因。 (1)、没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。 (2)、数据库选错了索引。 原文地址：一条SQL语句执行得很慢的原因"},{"title":"CAP&BASE","date":"2021-04-01T16:00:00.000Z","url":"/2021/04/02/2021/04/021412/","tags":[["cap","/tags/cap/"]],"categories":[["Java","/categories/Java/"]],"content":" CAP 理论CAP 理论指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性，等同于所有节点访问同一份最新的数据副本。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。 **可用性：**每次请求都能获取到正确的响应，但是不保证获取的数据为最新数据。 **分区容错性：**分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 在这三个基本需求中，最多只能同时满足其中的两项，P 是必须的，因此只能在 CP 和 AP 中选择，zookeeper 保证的是 CP，对比 spring cloud 系统中的注册中心 eruka 实现的是 AP。 BASE 理论BASE 是 Basically Available(基本可用)、Soft-state(软状态) 和 Eventually Consistent(最终一致性) 三个短语的缩写。 **基本可用：**在分布式系统出现故障，允许损失部分可用性（服务降级、页面降级）。 **软状态：**允许分布式系统出现中间状态。而且中间状态不影响系统的可用性。这里的中间状态是指不同的 data replication（数据备份节点）之间的数据更新可以出现延时的最终一致性。 **最终一致性：**data replications 经过一段时间达到一致性。 BASE 理论是对 CAP 中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。 ZooKeeperzookeeper保证了cp(一致性、分区容错性)，但是作为服务注册中心，我们可以容忍注册中心返回的是几分钟以前的注册信息。但是服务中心却必须保证可用性，即服务注册中心对于高可用性的需求高于一致性。对于可用性，zookeeper有一个leader选举方案。当master主节点宕机与其他节点失去联系时，其他节点会重新进行Leader选举，选出新的master节点。然而选举耗时过长，一般为30~120S，并且整个选举期间，整个zookeeper集群是无法使用的。 Eurekaeureka保证了ap(可用性、分区容错性)，eureka每一个节点都是平等的，几个节点宕机不会影响正常节点的工作。剩余的正常节点依旧可以提供服务注册和查询。并且，当客户端向某节点注册服务时，注册失败或者超时，则会自动切换到其他节点。只要有一台eureka节点还正常工作，就能保证注册服务的可用。但是对于服务信息的同步则不能保证一致性(不能保证强一致性，但是最终一致)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内85%的节点都没有正常心跳(不可用)那么Eureka就认为客户端与注册中心之间出现了网络故障，此时会出现以下几种情况：1、Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务2、Eureka仍然能够接收新服务的注册和查询请求，但是不会被同步到其他节点上(保证当前节点的可用性)3、当网络稳定后，当前实例新注册的服务会被同步到其他节点 因此,Eureka能够保证注册中心的高可用性，而不会像zookeeper一样直接集群瘫痪"},{"title":"Java并发","date":"2021-03-31T16:00:00.000Z","url":"/2021/04/01/2021/04/011413/","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 线程三种方法 实现Runnable接口; 实现Callable接口； 继承Thread类。 实现Runnable和Callable接口的类只能当做是一个可以在线程中运行的任务，不是真正意义上的线程，还是需要Thread类来调用，执行run()方法是作为一个普通的方法执行，而不是启动线程执行，start()方法是线程就绪，获得cpu时间片就可以执行run()方法。 与 Runnable 相比，Callable 可以有返回值，返回值通过 FutureTask 进行封装。 实现接口会更好一些，因为： Java 不支持多重继承，因此继承了 Thread 类就无法继承其它类，但是可以实现多个接口； 类可能只要求可执行就行，继承整个 Thread 类开销过大。 线程状态 状态 说明 NEW（新建） 创建后未启动，没有调用start()方法 RUNNABLE(运行) 可以被运行，具体看操作系统的资源调度 BLOCKED(阻塞) 表示线程阻塞于锁 WAITING（等待） 等待其他线程显式唤醒 TIMED_WAITING(超时等待) 时间到了之后自动被系统唤醒 TERMINATED（终止） 任务结束之后结束或出现异常而结束 锁synchronized底层是有两个指令monitorenter和monitorexit来实现同步的，JDK1.6之前，monitor的实现依赖于操作系统底层Mutex Lock互斥锁。 操作系统实现线程之间的切换需要从用户态切换到内核态&#x2F;核心态，这个成本非常高，状态之间的转换是消耗资源的，需要相对比较长的时间，所以此时的同步操作是一个重量级的操作，性能很低。 JDK1.6带来了新变化，有三种实现方式，偏向锁，轻量级锁（自旋锁）和重量级锁，由偏向到轻量级再到重量级，就是锁升级 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是synchronized括号里配置的对象。 同步代码块 synchronized (this) 同步方法 同步一个类 同步一个静态方法 ReentrantLockReentrantLock 是 java.util.concurrent（J.U.C）包中的锁 比较1. 锁的实现 synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的。 synchronized不需要手动释放锁，ReentrantLock需要手动释放。 2. 性能 新版本 Java 对 synchronized 进行了很多优化，例如自旋锁等，synchronized 与 ReentrantLock 大致相同。 3. 等待可中断 当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。 ReentrantLock 可中断，而 synchronized 不行。 4. 公平锁 公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁。 synchronized 中的锁是非公平的，ReentrantLock 默认情况下也是非公平的，但是也可以是公平的。 5. 锁绑定多个条件 一个 ReentrantLock 可以同时绑定多个 Condition 对象。 使用选择除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。这是因为 synchronized 是 JVM 实现的一种锁机制，JVM 原生地支持它，而 ReentrantLock 不是所有的 JDK 版本都支持。并且使用 synchronized 不用担心没有释放锁而导致死锁问题，因为 JVM 会确保锁的释放。 死锁线程t1和线程t2互相等待对方释放锁。 避免死锁 避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock（timeout）来替代使用内部锁机制。 J.U.CAQSCAS+CLH AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个CLH 锁队列的一个结点（Node）来实现锁的分配。 CLH同步队列的结构图 NonfairSync非公平锁 acquire addWaiter 这里使用了for(;;)自旋操作， 首先判断node的前辈节点，是不是head,如果是，说明它是下一个可以获得锁的线程，则调用tryAcquire()尝试获取锁，如果获取到，则将链表关系重新维护下（node设置为head,之前的head从链表移出），然后返回。线程逐级返回，加锁过程结束。 如果node的前辈节点不是head,或者获取锁失败，再判断其前辈节点的waitState,是不是SIGNAL,如果是，则当前线程调用park,进入阻塞状态，如果不是 &#x3D;&#x3D;0,则设置为SIGNAL; 0(&#x3D;&#x3D;1),则表示前辈节点已经被取消了，将取消的节点从队列移出，重新维护排队链表的关系。 然后再次进入for循环，执行上面的逻辑。 acquireQueued unlock 同步队列（CLH）遵循FIFO，首节点是获取同步状态的节点，首节点的线程释放同步状态后，将会唤醒它的后继节点（next），而后继节点将会在获取同步状态成功时将自己设置为首节点，这个过程非常简单。如下图 设置首节点是通过获取同步状态成功的线程来完成的（获取同步状态是通过CAS来完成），只能有一个线程能够获取到同步状态，因此设置头节点的操作并不需要CAS来保证，只需要将首节点设置为其原首节点的后继节点并断开原首节点的next（等待GC回收）应用即可。 入列操作也就是利用CAS(保证线程安全)来设置尾节点，出列就很简单了直接将head指向新头节点并断开老头节点联系就可以了。 CountDownLatch用来控制一个或者多个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方法而在等待的线程就会被唤醒。 结果 CyclicBarrier用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。 CyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。 CyclicBarrier 有两个构造函数，其中 parties 指示计数器的初始值，barrierAction 在所有线程都到达屏障的时候会执行一次。 SemaphoreSemaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 LockSupportAPI 用于创建锁和其他同步类的基本线程阻塞原语。 该类与使用它的每个线程关联一个许可证（在Semaphore类的意义上）。 如果许可证可用，将立即返回park ，并在此过程中消费; 否则可能会阻止。 如果尚未提供许可，则致电unpark获得许可。 （与Semaphores不同，许可证不会累积。最多只有一个。）可靠的使用需要使用volatile（或原子）变量来控制何时停放或取消停放。 对于易失性变量访问保持对这些方法的调用的顺序，但不一定是非易失性变量访问。 线程阻塞&amp;唤醒的方法 使用Object中的wait()让线程等待，notify()/notifyAll()方法唤醒 使用JUC中的Condition的await()让线程等待，使用signal()/signalAll()唤醒 LockSupport类的park()/unpark()可以阻塞当前线程以及唤醒指定被阻塞的线程 注意 wait()和notify()&#x2F;notifyAll()都必须在synchronized内部执行,成对出现使用，先wait()再notify() Condition需要lock()和unlock()方法配合使用，先lock()再unlock() LockSupport可以先唤醒后再阻塞线程 unpark获得许可凭证，之后再调用park可以凭证消费，故不会阻塞，但是不能阻塞两次再唤醒，凭证的数量最多为1,连续调用两次unpark()并不能增加凭证的数量，证不够不会放行，会继续阻塞。 线程池 拒绝策略 AbortPolicy：直接抛出异常。 CallerRunsPolicy：只用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 当然，也可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录 日志或持久化存储不能处理的任务。 ThreadLocal 为每一个线程创建一个副本 实现线程的上下文传递对象 如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用get\u0004\u0005和set\u0004\u0005方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。 原理 ThreadLocal内部维护的是一个类似Map的ThreadLocalMap数据结构，key为当前对象的Thread对象，值为 Object 对象。 比如我们在同一个线程中声明了两个ThreadLocal对象的话，会使用Thread内部都是使用仅有那个ThreadLocalMap存放数据的，ThreadLocalMap的 key 就是ThreadLocal对象，value 就是ThreadLocal对象调用set方法设置的值。 ThreadLocal 内存泄露问题ThreadLocalMap中使用的 key 为ThreadLocal的弱引用,而 value 是强引用。所以，如果ThreadLocal没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用set()、get()、remove()方法的时候，会清理掉 key 为 null的记录。使用完ThreadLocal方法后最好手动调用remove()方法 线程池的使用也会导致内存泄露问题 CASVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止. 三大问题 ABA问题,AtomicStampedReference来解决ABA 循环时间长开销大 只能保证一个共享变量的原子操作。 良好实践"},{"title":"nginx[warn]could not build optimal proxy_headers_hash","date":"2021-03-30T16:00:00.000Z","url":"/2021/03/31/2021/03/312003/","tags":[["nginx","/tags/nginx/"]],"categories":[["work","/categories/work/"]],"content":" 问题 修改配置文件 检查配置文件 热加载"},{"title":"Integer缓存池","date":"2021-03-22T16:00:00.000Z","url":"/2021/03/23/2021/03/231432/","tags":[["Java","/tags/Java/"]],"categories":[["Java","/categories/Java/"]],"content":" 说明Integer 缓存是 Java 5 中引入的一个有助于节省内存、提高性能的特性。 Integer中有个静态内部类IntegerCache，里面有个cache[],也就是Integer常量池，常量池的大小为一个字节（-128~127）。JDK源码如下（摘自JDK11源码）： IntegerCache是Integer的一个静态内部类： 如果设置了java.lang.IntegerCache.high，就使用这个值，如果没有设置就使用127，java.lang.IntegerCache.high这个值是通过JVM参数改变的，在java程序执行的时候加上 -XX:AutoBoxCacheMax=&lt;size&gt; 的参数即可。 使用了断言assert； 其实所谓的在内存中取值，就是在数组中，可以看出，Integer的值是被存在了cache数组中的。 参考资料Java Integer的缓存策略"},{"title":"SpringMVC工作流程","date":"2021-03-16T16:00:00.000Z","url":"/2021/03/17/2021/03/170953/","tags":[["SpringMVC","/tags/SpringMVC/"]],"categories":[["Java","/categories/Java/"]],"content":" 步骤​ 1、用户发送请求至前端控制器DispatcherServlet。 ​ 2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。 ​ 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。 ​ 4、DispatcherServlet调用HandlerAdapter处理器适配器。 ​ 5、 HandlerAdapter经过适配调用具体的处理器(Controller，也叫后端控制器)。 ​ 6、Controller执行完成返回ModelAndView。 ​ 7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet。 ​ 8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器。 ​ 9、 ViewReslover解析后返回具体View，这个view不是完整的，仅仅是一个页面（视图）名字，且没有后缀名。 ​ 10、DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。 ​ 11、 DispatcherServlet响应用户。 作者：打杂匠链接：来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"},{"title":"TCP/IP","date":"2021-03-16T16:00:00.000Z","url":"/2021/03/17/2021/03/171054/","tags":[["tcp/ip","/tags/tcp-ip/"]],"categories":[["Java","/categories/Java/"]],"content":" 访问百度 抓包 三次握手 四次挥手TCP连接是一种双工的通信模式 “ TCP 连接与 Web 服务器进行交互”摘录来自HTTP权威指南 (图灵程序设计丛书)[美]David Gourley Brian Totty Marjorie Sayer Sailu Reddy Aushu Aggarwal此材料可能受版权保护。 "},{"title":"Redis-Cluster集群:docker-compose方式","date":"2021-03-10T16:00:00.000Z","url":"/2021/03/11/2021/03/112053/","tags":[["Redis","/tags/Redis/"]],"categories":[["Java","/categories/Java/"]],"content":" 环境 默认系统已安装过docker&amp;docker-compose 步骤 下载redis镜像 编写redis配置文件 编写docker-compose.yml模板文件 创建并启动容器 创建集群 集群相关命令redis-cli 配置文件 模板文件：redis-cluster.tmpl 文件说明： port：节点端口； requirepass：添加访问认证； masterauth：如果主节点开启了访问认证，从节点访问主节点需要认证； protected-mode：保护模式，默认值 yes，即开启。开启保护模式以后，需配置 bind ip 或者设置访问密码；关闭保护模式，外部网络可以直接访问； daemonize：是否以守护线程的方式启动（后台启动），默认 no； appendonly：是否开启 AOF 持久化模式，默认 no； cluster-enabled：是否开启集群模式，默认 no； cluster-config-file：集群节点信息文件； cluster-node-timeout：集群节点连接超时时间； cluster-announce-ip：集群节点 IP，填写宿主机的 IP； cluster-announce-port：集群节点映射端口； cluster-announce-bus-port：集群节点总线端口。 生成配置文件及目录在192.168.9.100执行 在192.168.9.151执行 以上命令可以保存为shell脚本，便于以后使用，sh config.sh 执行之后会发现在当前目录下生成对应的以端口为目录名称及配置文件 redis.conf docker-compose.yml 启动 集群命令 注意，这里使用的redis版本， Redis Cluster 在5.0之后取消了ruby脚本redis-trib.rb的支持（手动命令行添加集群的方式不变），集合到redis-cli里，避免了再安装ruby的相关环境。直接使用redis-clit的参数--cluster 来取代。为方便自己后面查询就说明下如何使用该命令进行Cluster的创建和管理。 redis-cli 添加一个节点到集群 Redis-Cluster集群redis的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台redis服务器都存储相同的数据，很浪费内存，所以在redis3.0上加入了cluster模式，实现的redis的分布式存储，也就是说每台redis节点上存储不同的内容。 Redis-Cluster采用无中心结构,它的特点如下： 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。节点的fail是通过集群中超过半数的节点检测失效时才生效。客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。工作方式： 在redis的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是cluster，可以理解为是一个集群管理的插件。当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。 为了保证高可用，redis-cluster集群引入了主从模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点A1都宕机了，那么该集群就无法再提供服务了。 Redis集群方案基于分而治之的思想。Redis中数据都是以Key-Value形式存储的，而不同Key的数据之间是相互独立的。因此可以将Key按照某种规则划分成多个分区，将不同分区的数据存放在不同的节点上。这个方案类似数据结构中哈希表的结构。在Redis集群的实现中，使用哈希算法（公式是CRC16(Key) mod 16383）将Key映射到016383范围的整数。这样每个整数对应存储了若干个Key-Value数据，这样一个整数对应的抽象存储称为一个槽（slot）。每个Redis Cluster的节点——准确讲是master节点——负责一定范围的槽，所有节点组成的集群覆盖了016383整个范围的槽。 Slave上面的方案只是解决了性能扩展的问题，集群的故障容错能力并没有提升。提高容错能力的方法一般为使用某种备份&#x2F;冗余手段。负责一定数量的槽的节点被称为master节点。为了增加集群稳定性，每个master节点可以配置若干个备份节点——称为slave节点。Slave节点一般作为冷备份保存master节点的数据，在master节点宕机时替换master节点。在一些数据访问压力比较大的情况下，slave节点也可以提供读取数据的功能，不过slave节点的数据实时性会略差一下。而写数据的操作则只能通过master节点进行。 节点通信尽管不同节点存储的数据相互独立，这些节点仍然需要相互通信以同步节点状态信息。Redis集群采用P2P的Gossip协议，节点之间不断地通信交换信息，最终所有节点的状态都会达成一致。常用的Gossip消息有下面几种：ping消息：每个节点不断地向其他节点发起ping消息，用于检测节点是否在线和交换节点状态信息。pong消息：收到ping、meet消息时的响应消息。meet消息：新节点加入消息。fail消息：节点下线消息。forget消息：忘记节点消息，使一个节点下线。这个命令必须在60秒内在所有节点执行，否则超过60秒后该节点重新参与消息交换。实践中不建议直接使用forget命令来操作节点下线。 节点下线当某个节点出现问题时，需要一定的传播时间让多数master节点认为该节点确实不可用，才能标记标记该节点真正下线。Redis集群的节点下线包括两个环节：主观下线（pfail）和客观下线（fail）。主观下线：当节点A在cluster-node-timeout时间内和节点B通信（ping-pong消息）一直失败，则节点A认为节点B不可用，标记为主观下线，并将状态消息传播给其他节点。客观下线：当一个节点被集群内多数master节点标记为主观下线后，则触发客观下线流程，标记该节点真正下线。 故障恢复一个持有槽的master节点客观下线后，集群会从slave节点中选出一个提升为master节点来替换它。Redis集群使用选举-投票的算法来挑选slave节点。一个slave节点必须获得包括故障的master节点在内的多数master节点的投票后才能被提升为master节点。假设集群规模为3主3从，则必须至少有2个主节点存活才能执行故障恢复。如果部署时将2个主节点部署到同一台服务器上，则该服务器不幸宕机后集群无法执行故障恢复。默认情况下，Redis集群如果有master节点不可用，即有一些槽没有负责的节点，则整个集群不可用。也就是说当一个master节点故障，到故障恢复的这段时间，整个集群都处于不可用的状态。这对于一些业务来说是不可忍受的。可以在配置中将cluster-require-full-coverage配置为no，那么master节点故障时只会影响访问它负责的相关槽的数据，不影响对其他节点的访问。 持久化一、redis持久化—-两种方式1、redis提供了两种持久化的方式，分别是RDB（Redis DataBase）和AOF（Append Only File）。 2、RDB，简而言之，就是在不同的时间点，将redis存储的数据生成快照并存储到磁盘等介质上； 3、AOF，则是换了一个角度来实现持久化，那就是将redis执行过的所有写指令记录下来，在下次redis重新启动时，只要把这些写指令从前到后再重复执行一遍，就可以实现数据恢复了。 4、其实RDB和AOF两种方式也可以同时使用，在这种情况下，如果redis重启的话，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式的数据恢复完整度更高。 5、如果你没有数据持久化的需求，也完全可以关闭RDB和AOF方式，这样的话，redis将变成一个纯内存数据库，就像memcache一样。 二、redis持久化—-RDB1、RDB方式，是将redis某一时刻的数据持久化到磁盘中，是一种快照式的持久化方法。 2、redis在进行数据持久化的过程中，会先将数据写入到一个临时文件中，待持久化过程都结束了，才会用这个临时文件替换上次持久化好的文件。正是这种特性，让我们可以随时来进行备份，因为快照文件总是完整可用的。 3、对于RDB方式，redis会单独创建（fork）一个子进程来进行持久化，而主进程是不会进行任何IO操作的，这样就确保了redis极高的性能。 4、如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。 5、虽然RDB有不少优点，但它的缺点也是不容忽视的。如果你对数据的完整性非常敏感，那么RDB方式就不太适合你，因为即使你每5分钟都持久化一次，当redis故障时，仍然会有近5分钟的数据丢失。所以，redis还提供了另一种持久化方式，那就是AOF。 三、redis持久化—-AOF1、AOF，英文是Append Only File，即只允许追加不允许改写的文件。 2、如前面介绍的，AOF方式是将执行过的写指令记录下来，在数据恢复时按照从前到后的顺序再将指令都执行一遍，就这么简单。 3、我们通过配置redis.conf中的appendonly yes就可以打开AOF功能。如果有写操作（如SET等），redis就会被追加到AOF文件的末尾。 4、默认的AOF持久化策略是每秒钟fsync一次（fsync是指把缓存中的写指令记录到磁盘中），因为在这种情况下，redis仍然可以保持很好的处理性能，即使redis故障，也只会丢失最近1秒钟的数据。 5如果在追加日志时，恰好遇到磁盘空间满、inode满或断电等情况导致日志写入不完整，也没有关系，redis提供了redis-check-aof工具，可以用来进行日志修复。 6、因为采用了追加方式，如果不做任何处理的话，AOF文件会变得越来越大，为此，redis提供了AOF文件重写（rewrite）机制，即当AOF文件的大小超过所设定的阈值时，redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集。举个例子或许更形象，假如我们调用了100次INCR指令，在AOF文件中就要存储100条指令，但这明显是很低效的，完全可以把这100条指令合并成一条SET指令，这就是重写机制的原理。 7、在进行AOF重写时，仍然是采用先写临时文件，全部完成后再替换的流程，所以断电、磁盘满等问题都不会影响AOF文件的可用性，这点大家可以放心。 8、AOF方式的另一个好处，我们通过一个“场景再现”来说明。某同学在操作redis时，不小心执行了FLUSHALL，导致redis内存中的数据全部被清空了，这是很悲剧的事情。不过这也不是世界末日，只要redis配置了AOF持久化方式，且AOF文件还没有被重写（rewrite），我们就可以用最快的速度暂停redis并编辑AOF文件，将最后一行的FLUSHALL命令删除，然后重启redis，就可以恢复redis的所有数据到FLUSHALL之前的状态了。是不是很神奇，这就是AOF持久化方式的好处之一。但是如果AOF文件已经被重写了，那就无法通过这种方法来恢复数据了。 9、虽然优点多多，但AOF方式也同样存在缺陷，比如在同样数据规模的情况下，AOF文件要比RDB文件的体积大。而且，AOF方式的恢复速度也要慢于RDB方式。 如果你直接执行BGREWRITEAOF命令，那么redis会生成一个全新的AOF文件，其中便包括了可以恢复现有数据的最少的命令集。 10、如果运气比较差，AOF文件出现了被写坏的情况，也不必过分担忧，redis并不会贸然加载这个有问题的AOF文件，而是报错退出。这时可以通过以下步骤来修复出错的文件： 1.备份被写坏的AOF文件2.运行redis-check-aof –fix进行修复3.用diff -u来看下两个文件的差异，确认问题点4.重启redis，加载修复后的AOF文件 四、redis持久化—-AOF重写1、AOF重写的内部运行原理，我们有必要了解一下。 2、在重写即将开始之际，redis会创建（fork）一个“重写子进程”，这个子进程会首先读取现有的AOF文件，并将其包含的指令进行分析压缩并写入到一个临时文件中。 3、与此同时，主工作进程会将新接收到的写指令一边累积到内存缓冲区中，一边继续写入到原有的AOF文件中，这样做是保证原有的AOF文件的可用性，避免在重写过程中出现意外。 4、当“重写子进程”完成重写工作后，它会给父进程发一个信号，父进程收到信号后就会将内存中缓存的写指令追加到新AOF文件中。 5、当追加结束后，redis就会用新AOF文件来代替旧AOF文件，之后再有新的写指令，就都会追加到新的AOF文件中了。 五、redis持久化—-如何选择RDB和AOF1、对于我们应该选择RDB还是AOF，官方的建议是两个同时使用。这样可以提供更可靠的持久化方案。 2、redis的备份和还原，可以借助第三方的工具redis-dump。 六、Redis的两种持久化方式也有明显的缺点1、RDB需要定时持久化，风险是可能会丢两次持久之间的数据，量可能很大。 2、AOF每秒fsync一次指令硬盘，如果硬盘IO慢，会阻塞父进程；风险是会丢失1秒多的数据；在Rewrite过程中，主进程把指令存到mem-buffer中，最后写盘时会阻塞主进程。 持久化相关内容："},{"title":"yarn","date":"2021-03-08T16:00:00.000Z","url":"/2021/03/09/2021/03/092045/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 安装 更新 用法显示命令列表 初始化一个新项目 安装所有依赖项 添加依赖项 将依赖项添加到不同的依赖类别中 更新依赖项 删除依赖项 更新 Yarn 本体 安装之后无法执行问题 解决办法PowerShell管理员权限执行 缓存问题 清理缓存 yarn No such file or directory 参考文章：  "},{"title":"curl模拟请求","date":"2021-03-08T16:00:00.000Z","url":"/2021/03/09/2021/07/141752/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" curl GET curl POST curl POST 上传文件 帮助"},{"title":"CAS原理","date":"2021-03-04T16:00:00.000Z","url":"/2021/03/05/2021/03/051530/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 1.什么是CAS，如何保证线程安全？CAS是英文单词CompareAndSwap的缩写，中文意思是：比较并替换。CAS需要有3个操作数**：内存地址V，旧的预期值A，即将要更新的目标值B。**它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子操作。 CAS指令执行时，当且仅当内存地址V的值与预期值A相等时，将内存地址V的值修改为B，否则不做替换操作。当使用CAS替换新值不成功时，自旋，重新获得原值和新值再试一次直到成功为止。 CAS并发原语体现在JAVA语言中就是sun.misc.Unsafe类中的各个方法，调用Unsafe类中的CAS方法，JVM会帮我们实现出CAS汇编指令。这是一种完全依赖于硬件的功能，通过它实现了原子操作。再次强调，由于CAS是一种系统原语，原语属于操作系统用语范畴，是由若干条指令组成的，用于完成某个功能的一个过程，并且原语的执行必须是连续的，在执行过程中不允许被中断，也就是说CAS是一条CPU的原子指令，不会造成所谓的数据不一致问题。（简单说，原语就是连续执行的，可以保证线程安全） 2.getAndIncrement()底层原理？ this指当前对象，valueOffset代表内存偏移量，表示该变量值在内存中的偏移地址，因为Unsafe就是根据内存偏移地址获取数据的。 var1是AtomicInteger对象本身，var2是该对象值的引用地址，var4需要变动的数量，var5是用过var1和var2找出的主内存中真实值。 用该对象当前的值和var5比较，如果相同，更新var5+var4并且返回true，如果不同，继续取值然后再比较，直到更新完成。 缺点：循环时间长，开销大，因为如果CAS失败，会一直进行尝试，如果CAS长时间一直不成功，可能会给CPU带来很大的开销。 3.Unsafe类Unsafe是CAS的核心类，由于Java方法无法直接访问底层系统，需要通过本地(native)方法来访问，Unsafe相当于一个后门，基于该类可以直接操作特定内存的数据。Unsafe类存在于sun.misc包中，其内部方法操作可以像C指针一样直接操作内存，因为Java中CAS操作的执行依赖于Unsafe类的方法。 注意：Unsafe类中的所有方法都是native修饰的，也就是说Unsafe类中的方法都直接调用操作系统底层资源执行相应任务。 4.CAS缺点1.循环时间长，开销大，例如getAndAddInt方法执行时，如果CAS失败，会一直进行尝试。如果CAS长时间一直不成功，可能会给CPU带来很大的开销； 2.只能保证一个共享变量的原子性，对于多个共享变量的操作，循环CAS无法保证操作的原子性，这个时候就需要加锁来保证原子性； 3.产生ABA问题。 5.什么是ABA问题？ 当线程1从内存位置V中取出A，这个时候线程2也从内存中取出A 线程2进行了一些操作将值改为了B，然后线程2又将V位置的数据改回为A。 而线程1的执行时间比线程2长，当线程1完成CAS操作后准备写回内存地址V时，发现内存中仍然是A，然后线程1操作成功。 尽管线程1的CAS操作成功，但是不代表这个过程是没有问题的。 只管头和尾，不管中间操作。如果业务场景不在意ABA问题，也就无所谓了。但是要在意ABA问题，就不行。 举一个场景： 一家火锅店为了生意推出了一个特别活动，凡是在五一期间的老用户凡是卡里余额小于20的，赠送10元，但是这种活动每人只可享受一次。如果用CAS技术，先去用户卡里的余额，然后包装成AtomicInteger，写一个判断，开启10个线程，然后判断小于20的，一律加10。假设有个线程A去判断账户里的钱此时是15，满足条件，直接+10，这时候卡里余额是25.但是此时不巧，正好在连锁店里，这个客人正在消费，又消费了10，此时卡里余额又为15，线程B去执行扫描账户的时候，发现它又小于20，又用cas给它加了10，这样的话就相当于加了两次！ 6.如何解决ABA问题？增加版本号来标记，改一次，版本号增加1。可以使用AtomicStampedReference的compareAndSet()。 ABA问题代码显示： ABA问题解决： 首先把我们上面CAS操作的int，变成CAS操作对象Pair，原理是一样。 加了个版本号stamp，只有版本号不一样时，CAS才操作成功。 上面代码流程： 如果元素值和版本号都没有变化，并且和新的也相同，返回true；如果元素值和版本号都没有变化，并且和新的不完全相同，就构造一个新的Pair对象并执行CAS更新pair。 LongAdder大致原理我们知道，AtomicLong中有个内部变量value保存着实际的long值，所有的操作都是针对该变量进行。也就是说，高并发环境下，value变量其实是一个热点数据，也就是N个线程竞争一个热点。 它们都是在一个死循环内，不断尝试修改目标值，直到修改成功。如果竞争不激烈，那么修改成功的概率就很高，否则，修改失败的概率就很高。在大量修改失败时，这些原子操作就会进行多次循环尝试，因此性能就会受到影响。 一种基本方案就是可以使用热点分离，将竞争的数据进行分解，基于这个思路，大家应该可以想到一种对传统AtomicInteger等原子类的改进方法。虽然在CAS操作中没有锁，但是像减小锁粒度这种分离热点的思想依然可以使用。一种可行的方案就是仿造ConcurrentHashMap，将热点数据分离。比如，可以将AtomicInteger的内部核心数据value分离成一个数组，每个线程访问时，通过哈希等算法映射到其中一个数字进行计数，而最终的计数结果，则为这个数组的求和累加，热点数据value被分离成多个单元cell，每个cell独自维护内部的值，当前对象的实际值由所有的cell累计合成，这样，热点就进行了有效的分离，提高了并行度。LongAdder正是使用了这种思想。 LongAdder的基本思路就是分散热点，将value值的新增操作分散到一个数组中，不同线程会命中到数组的不同槽中，各个线程只对自己槽中的那个value值进行CAS操作，这样热点就被分散了，冲突的概率就小很多。 LongAdder有一个全局变量volatile long base值，当并发不高的情况下都是通过CAS来直接操作base值，如果CAS失败，则针对LongAdder中的Cell[]数组中的Cell进行CAS操作，减少失败的概率。 在实际的操作中，LongAdder并不会一开始就动用数组进行处理，而是将所有数据都先记录在一个称为base的变量中。如果在多线程条件下，大家修改base都没有冲突，那么也没有必要扩展为cell数组。但是，一旦base修改发生冲突，就会初始化cell数组，使用新的策略。如果使用cell数组更新后，发现在某一个cell上的更新依然发生冲突，那么系统就会尝试创建新的cell，或者将cell的数量加倍，以减少冲突的可能。 例如当前类中base &#x3D; 5，有三个线程进行CAS原子性的+1操作，线程一执行成功，此时base&#x3D;11，线程二、线程三执行失败后开始针对于Cell[]数组中的Cell元素进行+1操作，同样也是CAS操作，此时数组index&#x3D;1和index&#x3D;2中Cell的value都被设置为了1.执行完成后，统计累加数据：sum &#x3D; 5 + 1 + 1 &#x3D; 7 increment()方法（该方法会将LongAdder自增1）的内部实现 由于longAccumulate()方法比较复杂，这里不再展开讨论，其大致内容是根据需要创建新的cell或者对cell数组进行扩容，以减少冲突。"},{"title":"Excel TREND 函数","date":"2021-02-28T16:00:00.000Z","url":"/2021/03/01/2021/03/011419/","tags":[["excel","/tags/excel/"]],"categories":[["work","/categories/work/"]],"content":" TREND 函数TREND函数沿线性趋势返回值。 它使用 (最小二) 数组的和known_y的known_x拟合直线。 TREND 返回指定值数组沿该行new_x的 y 值。 其中，x 为独立变量，y 为因变量，b 为常数，m 为斜率。 语法 TREND 函数语法具有下列参数 参数 说明 Known_y’s 必需 关系 y &#x3D; mx + b 中已了解的 y 值集- 如果数组 known_y’s 在单独一列中，则 known_x’s 的每一列被视为一个独立的变量。 - 如果数组 known_y’s 在单独一行中，则 known_x’s 的每一行被视为一个独立的变量。 Known_x’s 可选 关系 y &#x3D; mx + b 中可能已经知道的一组可选 x 值- 数组 known_x’s 可以包含一组或多组变量。 如果仅使用一个变量，那么只要 known_x’s 和 known_y’s 具有相同的维数，则它们可以是任何形状的区域。 如果用到多个变量，则 known_y’s 必须为向量（即必须为一行或一列）。 - 如果省略 known_x’s，则假设该数组为 {1,2,3,…}，其大小与 known_y’s 相同。 new_x的 可选 希望 TREND 返回对应 y 值的新 x 值- New_x’s 与 known_x’s 一样，对每个自变量必须包括单独的一列（或一行）。 因此，如果 known_y’s 是单列的，known_x’s 和 new_x’s 应该有同样的列数。 如果 known_y’s 是单行的，known_x’s 和 new_x’s 应该有同样的行数。 - 如果省略 new_x’s，将假设它和 known_x’s 一样。 - 如果 known_x’s 和 new_x’s 都省略，将假设它们为数组 {1,2,3,…}，大小与 known_y’s 相同。 const 可选 一个逻辑值，指定是否强制常量 b 等于 0- 如果 const 为 TRUE 或省略，b 将按正常计算。 - 如果 const 为 FALSE，b 将被设为 0（零），m 将被调整以使 y &#x3D; mx。 备注 可以使用 TREND 函数计算同一变量的不同乘方的回归值来拟合多项式曲线。 例如，假设 A 列包含 y 值，B 列含有 x 值。 可以在 C 列中输入 x^2，在 D 列中输入 x^3，等等，然后根据 A 列，对 B 列到 D 列进行回归计算。 返回数组的公式必须使用Ctrl+Shift+Enter作为数组公式输入，除非拥有Microsoft 365的当前版本，然后只需按Enter。 当为参数（如 known_x’s）输入数组常量时，应当使用逗号分隔同一行中的数据，用分号分隔不同行中的数据。 TREND 函数"},{"title":"jQuery - 设置内容和属性","date":"2021-02-24T16:00:00.000Z","url":"/2021/02/25/2021/02/251359/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 设置内容 - text()、html() 以及 val()我们将使用前一章中的三个相同的方法来设置内容： text() - 设置或返回所选元素的文本内容 html() - 设置或返回所选元素的内容（包括 HTML 标记） val() - 设置或返回表单字段的值 text()、html() 以及 val() 的回调函数上面的三个 jQuery 方法：text()、html() 以及 val()，同样拥有回调函数。回调函数由两个参数：被选元素列表中当前元素的下标，以及原始（旧的）值。然后以函数新值返回您希望使用的字符串。 下面的例子演示带有回调函数的 text() 和 html()： 设置属性 - attr()jQuery attr() 方法也用于设置&#x2F;改变属性值。 attr() 方法也允许您同时设置多个属性。 下面的例子演示如何改变（设置）链接中 href 属性的值： "},{"title":"EasyUI给input 赋值","date":"2021-02-24T16:00:00.000Z","url":"/2021/02/25/2021/02/251411/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" html内容 赋值方式 取值 当我们使用了easyui-xxxx class这样的样式,后面的xxxx作为设置或取值的方法（） 例如： "},{"title":"删除一个表中和另一个表相同的记录","date":"2021-02-21T16:00:00.000Z","url":"/2021/02/22/2021/02/221008/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":" "},{"title":"mysql分组取最大(最小、最新、前N条)条记录","date":"2021-02-21T16:00:00.000Z","url":"/2021/02/22/2021/02/221932/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":" 在数据库开发过程中，我们要为每种类型的数据取出前几条记录，或者是取最新、最小、最大等等，这个该如何实现呢，本文章向大家介绍如何实现mysql分组取最大(最小、最新、前N条)条记录。需要的可以参考一下。 先看一下本示例中需要使用到的数据 创建表并插入数据： 数据表如下： name val memo a 2 a2 a 1 a1 a 3 a3 b 1 b1 b 3 b3 b 2 b2 b 4 b4 b 5 b5 按name分组取val最大的值所在行的数据方法一： 方法二： 方法三： 方法四： 方法五： 以上五种方法运行的结果均为如下所示： name val memo a 3 a3 b 5 b5 小编推荐使用第一、第三、第四钟方法,结果显示第1,3,4种方法效率相同，第2，5种方法效率差些。 按name分组取val最小的值所在行的数据方法一： 方法二： 方法三： 方法四： 方法五： 以上五种方法运行的结果均为如下所示： name val memo a 1 a1 b 1 b1 按name分组取第一次出现的行所在的数据sql如下： 结果如下： name val memo a 2 a2 b 1 b1 按name分组随机取一条数据sql如下： 结果如下： name val memo a 1 a1 b 3 b3 按name分组取最小的两个(N个)val第一种方法： 第二种方法： 第三种方法： 结果如下： name val memo a 1 a1 a 2 a2 b 1 b1 b 2 b2 按name分组取最大的两个(N个)val第一种方法： 第二种方法： 第三种方法： 结果如下： name val memo a 3 a3 a 2 a2 b 5 b5 b 4 b4 原文地址："},{"title":"datagrid中load,reload,loadData方法","date":"2021-02-21T16:00:00.000Z","url":"/2021/02/22/2021/02/251118/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 一.load方法 参数为一个json对象，里面写的是你要传输的参数的键值对，调用这个方法来加载数据的时候，它传给后台的分页信息是从第一页开始的 二.reload方法: 它跟load一样有加载数据功能，也一样的传参数，但它传给后台的分布信息是当前的页码，就是实现刷新当前页的功能 三.loadData方法 首先它加载的本地数据，就是不会跟后台什么的有交互,比较灵活可以设置分页信息，total是行数，也可以设置它的当前页，而它那个rows属性设的就是你所要加载的行的集合"},{"title":"vim命令","date":"2021-01-29T07:32:00.000Z","url":"/2021/01/29/2021/01/291027/","tags":[["vim","/tags/vim/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 整行操作 单行复制 在“命令”模式下，将光标移动到将要复制的行处，按“yy”进行复制 多行复制 在“命令”模式下，将光标移动到将要复制的首行处，按“nyy”复制n行 粘贴 在“命令”模式下，将光标移动到将要粘贴的行处，按“p”进行粘贴 选择操作 按字符选择 在“命令”行模式下，按“v”（小写），进入按字符选择模式，移动光标选择要进行复制的字符 选择完成后按“y”进行复制，按“p”进行粘贴 按行选择 在“命令”行模式下，按“V”（大写），进入行选择模式，移动光标选择要进行复制的行选择完成后按“y”进行复制，按“p”进行粘贴 按块选择 在“命令”行模式下，按组合键Ctrl+v进入到按块选择模式，移动光标选择要进行复制的块选择完成后按“y”进行复制，按“p”进行粘贴 查找1.首先通过 / 查找，如：/dir2.然后用n查找下一个，用N查找上一个 替换 查看编码格式 指定编码格式打开文件 转换编码格式 iconv转换编码格式 vim退出"},{"title":"sql数据中时间的月份、年份","date":"2021-01-11T16:00:00.000Z","url":"/2021/01/12/2021/01/121517/","tags":[["SQL","/tags/SQL/"]],"categories":[["work","/categories/work/"]],"content":" 执行sql语句,获取月份 获取年份 data_format"},{"title":"MySQL查找正在运行的事务并结束掉该事务","date":"2021-01-11T16:00:00.000Z","url":"/2021/01/12/2021/01/121534/","tags":[["SQL","/tags/SQL/"]],"categories":[["work","/categories/work/"]],"content":" 查找正在运行的事务并结束掉该事务 "},{"title":"Spring事务不生效问题","date":"2021-01-03T16:00:00.000Z","url":"/2021/01/04/2021/01/041657/","tags":[["spring","/tags/spring/"],["java","/tags/java/"]],"categories":[["work","/categories/work/"]],"content":" 问题说明基于xml方式的配置 配置文件目录 错误信息 配置文件MVC配置文件这里已经声明只扫描controller了，所以不需要添加排除service等 Spring主容器配置加载环境配置以及数据源等 事务配置如果需要使用声明式事务，需要添加&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 解决方案如果是重复扫描的问题 需要在配置文件中排除相关的注解扫描,网上多给出的是这种解决方案 使用类似上面的包扫描配置，不需要另外排除，经检查，是由于切点的表达式书写有问题，导致匹配不到service. 日志"},{"title":"生日快乐","date":"2020-12-05T01:39:00.000Z","url":"/2020/12/05/2020/12/050939/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 生日快乐"},{"title":"Ubuntu关闭swap","date":"2020-12-03T01:28:00.000Z","url":"/2020/12/03/2020/12/030927/","tags":[["Ubuntu","/tags/Ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 查看交换分区 total used free shared buff&#x2F;cache available Mem: 62G 58G 507M 2.7M 3.4G 3.2G Swap: 4.0G 14M 4.0G 命令"},{"title":"Ubuntu报错`flAbsPath on /var/lib/dpkg/status failed`","date":"2020-12-01T07:54:00.000Z","url":"/2020/12/01/2020/12/011349/","tags":[["Ubuntu","/tags/Ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 错误 解决"},{"title":"MySQL遍历","date":"2020-11-30T08:11:00.000Z","url":"/2020/11/30/2020/11/301611/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":" 取样 表1 表2 需求说明将表二中城市和一相同的，根据exam数量取样 mysql sql 中 limit 接收变量方法"},{"title":"MyBatis-Plus的Entity文件排除非表中字段","date":"2020-11-26T03:11:00.000Z","url":"/2020/11/26/2020/11/261110/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 以下三种方式选择一种即可： 使用 transient 修饰 使用 static 修饰 使用 TableField 注解"},{"title":"Win10轻松使用之放大文本","date":"2020-11-24T08:27:00.000Z","url":"/2020/11/24/2020/11/241027/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 需求说明在屏幕较小的显示器上显示文本看起来会比较小，通过全局的缩放，图标和文字就会显得有一些不协调，这里介绍单独的文本放大的方式。 解决办法设置&#x3D;》轻松使用&#x3D;》显示：放大文本。 "},{"title":"在MySQL中的VARCHAR和TEXT之间的选择","date":"2020-11-12T16:00:00.000Z","url":"/2020/11/13/2020/11/131342/","tags":[["mysql","/tags/mysql/"]],"categories":[["work","/categories/work/"]],"content":" 2020 年 2 月 19 日，由 Robert Gravelle 撰写 MySQL 5.0.3版的一项更改包括将VARCHAR字段的最大长度从255个字符增加到65,535个字符。这使得VARCHAR类型比以往任何时候都更类似于TEXT。对于那些设计数据库表的人来说，在VARCHAR和TEXT之间进行选择现在变得更加困难。在今天的博客中，我们将概述两者之间的主要区别，并确定在决定使用哪种数据类型时要考虑的因素。 VARCHAR和TEXT之间的一些区别两种数据类型共享的最大长度为65,535个字符，但仍然存在一些差异： VARCHAR中的VAR表示您可以将最大大小设置为1到65,535之间的任何值。 TEXT字段的最大固定大小为65,535个字符。 VARCHAR可以是索引的一部分，而TEXT字段要求您指定前缀长度，该长度可以是索引的一部分。 VARCHAR与表内联存储（至少对于MyISAM存储引擎而言），因此在大小合理时可能会更快。当然，快得多少取决于您的数据和硬件。同时，TEXT存储在表外，该表具有指向实际存储位置的指针。 排序使用TEXT列将需要使用基于磁盘的临时表作为MEMORY（HEAP）存储引擎。 TEXT类型如果您需要TEXT类型，请知道实际上有三种口味；除了TEXT，还有MEDIUMTEXT或LONGTEXT变体。后两个用于存储长度超过65,535个字符的文本内容。 MEDIUMTEXT最多可存储16 MB的字符串，而LONGTEXT最多可存储4 GB的字符串！不用说，除非您有大量的存储空间，否则应避免使用这些较大的类型。 在Navicat中选择VARCHAR和TEXT类型在Navicat for MySQL和Navicat Premium中，对象设计器都允许您创建和维护各种数据库对象，包括表，视图，函数，索引以及列。在“类型”标题下，您只需从下拉菜单中选择列的数据类型即可。如您所见，它包含text，mediumtext和longtext类型： 对于VARCHAR类型，也可以从“类型”下拉列表中选择它，但是如果要使用非255（默认值）的值，则应编辑“长度”值。 提示：由于TEXT字段可能会很长，因此Navicat拥有表单视图，为它们提供了更多空间： 总结我们可以从所有这些中得出的结论是，如果可能的话，应该对255至65k个字符的列使用VARCHAR字段而不是TEXT。 这将可能导致更少的磁盘读取和更少的写入。"},{"title":"dayjs格式化时间","date":"2020-11-10T03:41:00.000Z","url":"/2020/11/10/2020/11/101141/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" 引入dayjs 格式化 中文文档：  类似的还有一个momentjs,但是已经停止维护了。"},{"title":"win7笔记本电脑设置WiFi热点","date":"2020-11-08T16:00:00.000Z","url":"/2020/11/09/2020/11/091651/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" win + R，输入cmd 回车，在命令行输入 "},{"title":"Excel合并多个Sheet","date":"2020-11-05T16:00:00.000Z","url":"/2020/11/06/2020/11/061119/","tags":[["excel","/tags/excel/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 合并表"},{"title":"Excel自动将多个Sheet拆分成多个文件","date":"2020-10-26T16:00:00.000Z","url":"/2020/10/27/2020/10/272023/","tags":[["excel","/tags/excel/"]],"categories":[["wuw","/categories/wuw/"]],"content":"宏 excel格式枚举： XlFileFormat 总结：解决问题的过程中可以获得什么，而不是解决完成之后取得的。"},{"title":"MySQL存储过程批量更新表字段","date":"2020-10-11T16:00:00.000Z","url":"/2020/10/12/2020/10/121144/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":" 添加字段 需要更新的字段 存储过程-更新字段"},{"title":"minio","date":"2020-09-28T16:00:00.000Z","url":"/2020/09/29/2020/09/291042/","tags":[["minio","/tags/minio/"]],"categories":[["work","/categories/work/"]],"content":"官方文档 docker安装 使用客户端设置永久下载链接minio client 操作容器 连接服务端 设置权限"},{"title":"知识点","date":"2020-09-28T16:00:00.000Z","url":"/2020/09/29/2020/09/291458/","tags":[["spring","/tags/spring/"]],"categories":[["work","/categories/work/"]],"content":"Spring Boot自定义配置Hebernate 不需要再次检查实体的变化。这是非常高效的。79、发布 Spring Boot 用户应用程序自定义配置的最好方法是什么？@Value 的问题在于，您可以通过应用程序分配你配置值。更好的操作是采取集中的方法。你可以使用 @ConfigurationProperties 定义一个配置组件。 你可以在 application.properties 中配置参数。 配置文件的需求是什么？企业应用程序的开发是复杂的，你需要混合的环境： Dev QA Stage Production 在每个环境中，你想要不同的应用程序配置。 配置文件有助于在不同的环境中进行不同的应用程序配置. Spring 和 Spring Boot 提供了你可以制定的功能。不同配置文件中，不同环境的配置是什么？为一个制定的环境设置活动的配置文件。Spring Boot 将会根据特定环境中设置的活动配置文件来选择应用程序的配置。 "},{"title":"诗和远方曾来过","date":"2020-09-26T16:00:00.000Z","url":"/2020/09/27/2020/09/251618/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"诗和远方曾来过，钟意你眸中的清澈。"},{"title":"Python批量转换xlsx到csv","date":"2020-09-16T16:00:00.000Z","url":"/2020/09/17/2020/09/171436/","tags":[["pandas","/tags/pandas/"]],"categories":[["Python","/categories/Python/"]],"content":"代码 "},{"title":"Linux下文件的拆分以及随机取样","date":"2020-09-06T16:00:00.000Z","url":"/2020/09/07/2020/09/071334/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"需求200W的文本文件，需要删除以某些字符开始的行，然后在剩下的文件中取4个10W的样本。样本不重复。 一开始觉得使用MySQL导入到数据库，发现速度太慢了。 所以使用先拆分后取样的方式，保证不重复即可，概率上可能会有所偏差，不影响结果。 sed命令删除不需要的行删除以100开头的行 动作说明： a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s&#x2F; old&#x2F;new&#x2F;g 就是啦！ split拆分文件 根据行拆分每3行拆分成一个文件，拆分后的文件名以name开头，以数字作为后缀后缀长度为1 shuf随机取样shuf命令可以随机提取一个文本文件的不同行，输出到文件或者标准输出中。 "},{"title":"MySQL分解关联查询","date":"2020-09-04T16:00:00.000Z","url":"/2020/09/05/2020/09/051432/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":"很多高性能的应用都会对关联查询进行分解。简单地，可以对每一个表进行一次单表查询，然后将结果在应用程序中进行关联。例如，下面这个查询： 可以分解成下面这些查询来代替： 到底为什么要这样做？乍一看，这样做并没有什么好处，原本一条查询，这里却变成多条查询，返回的结果又是一模一样的。事实上，用分解关联查询的方式重构查询有如下的优势： 让缓存的效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象。例如，上面查询中的tag已经被缓存了，那么应用就可以跳过第一个查询。再例如，应用中已经缓存了ID为123、567、9098的内容，那么第三个查询的IN()中就可以少几个ID。另外，对MySQL的查询缓存来说(6)，如果关联中的某个表发生了变化，那么就无法使用查询缓存了，而拆分后，如果某个表很少改变，那么基于该表的查询就可以重复利用查询缓存结果了。 将查询分解后，执行单个查询可以减少锁的竞争。 在应用层做关联，可以更容易对数据库进行拆分，更容易做到高性能和可扩展。 查询本身效率也可能会有所提升。这个例子中，使用IN()代替关联查询，可以让MySQL按照ID顺序进行查询，这可能比随机的关联要更高效。我们后续将详细介绍这点。 可以减少冗余记录的查询。在应用层做关联查询，意味着对于某条记录应用只需要查询一次，而在数据库中做关联查询，则可能需要重复地访问一部分数据。从这点看，这样的重构还可能会减少网络和内存的消耗。 更进一步，这样做相当于在应用中实现了哈希关联，而不是使用MySQL的嵌套循环关联。某些场景哈希关联的效率要高很多（本章后续我们将讨论这点）。 在很多场景下，通过重构查询将关联放到应用程序中将会更加高效，这样的场景有很多，比如：当应用能够方便地缓存单个查询的结果的时候、当可以将数据分布到不同的MySQL服务器上的时候、当能够使用IN()的方式代替关联查询的时候、当查询中使用同一个数据表的时候。 内容来自《高性能MySQL》:第三版 "},{"title":"Kibana server is not ready yet","date":"2020-09-01T16:00:00.000Z","url":"/2020/09/02/2020/09/021520/","tags":[["elasticsearch","/tags/elasticsearch/"],["kibana","/tags/kibana/"]],"categories":[["work","/categories/work/"]],"content":"查看错误信息 日志 删除index 重启服务"},{"title":"input输入框输入数据查询","date":"2020-08-27T16:00:00.000Z","url":"/2020/08/28/2020/08/281356/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":" data.json 原文地址："},{"title":"ArchLinux下Anaconda安装TensorFlow-gpu==2.3.0","date":"2020-08-20T16:00:00.000Z","url":"/2020/08/21/2020/08/211548/","tags":[["Arch Linux","/tags/Arch-Linux/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["work","/categories/work/"]],"content":"系统信息 系统已安装Anaconda3并且配置了国内的源。 创建虚拟环境 安装CUDA&amp;CUDNN 安装TensorFlow 验证 查看当前环境下的所有包"},{"title":"Linux 查看内存/CPU信息","date":"2020-08-18T16:00:00.000Z","url":"/2020/08/19/2020/08/190947/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"系统信息 查看CPU信息 内存信息 查看内存型号、频率 支持的内存大小 内核版本 Windows"},{"title":"杭州之行","date":"2020-08-17T16:00:00.000Z","url":"/2020/08/18/2020/08/151616/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"实际行程2020-08-22"},{"title":"volatile","date":"2020-08-13T16:00:00.000Z","url":"/2020/08/14/2020/08/141055/","tags":[["volatile","/tags/volatile/"]],"categories":[["Java","/categories/Java/"]],"content":"Java内存模型随着计算机的CPU的飞速发展，CPU的运算能力已经远远超出了从主内存（运行内存）中读取的数据的能力，为了解决这个问题，CPU厂商设计出了CPU内置高速缓存区。高速缓存区的加入使得CPU在运算的过程中直接从高速缓存区读取数据，在一定程度上解决了性能的问题。但也引起了另外一个问题，在CPU多核的情况下，每个处理器都有自己的缓存区，数据如何保持一致性。为了保证多核处理器的数据一致性，引入多处理器的数据一致性的协议，这些协议包括MOSI、Synapse、Firely、DragonProtocol等。 JVM在执行多线程任务时，共享数据保存在主内存中，每一个线程（执行再不同的处理器）有自己的高速缓存，线程对共享数据进行修改的时候，首先是从主内存拷贝到线程的高速缓存，修改之后，然后从高速缓存再拷贝到主内存。当有多个线程执行这样的操作的时候，会导致共享数据出现不可预期的错误。 举个例子： i++;&#x2F;&#x2F;操作 这个i++操作，线程首先从主内存读取i的值，比如i&#x3D;0，然后复制到自己的高速缓存区，进行i++操作，最后将操作后的结果从高速缓存区复制到主内存中。如果是两个线程通过操作i++,预期的结果是2。这时结果真的为2吗？答案是否定的。线程1读取主内存的i&#x3D;0,复制到自己的高速缓存区，这时线程2也读取i&#x3D;0,复制到自己的高速缓存区，进行i++操作，怎么最终得到的结构为1，而不是2。 为了解决缓存不一致的问题，有两种解决方案： 在总线加锁，即同时只有一个线程能执行i++操作（包括读取、修改等）。 通过缓存一致性协议 第一种方式就没什么好说的，就是同步代码块或者同步方法。也就只能一个线程能进行对共享数据的读取和修改，其他线程处于线程阻塞状态。 第二种方式就是缓存一致性协议，比如Intel 的MESI协议，它的核心思想就是当某个处理器写变量的数据，如果其他处理器也存在这个变量，会发出信号量通知该处理器高速缓存的数据设置为无效状态。当其他处理需要读取该变量的时候，会让其重新从主内存中读，然后再复制到高速缓存区。 并发编程的概念并发编程的有三个概念，包括原子性、可见性、有序性。 原子性原子性是指，操作为原子性的，要么成功，要么失败，不存在第三种情况。比如： 这个复杂操作是原子性的。再比如： i&#x3D;0这是一个赋值操作，这一步是原子性操作；那么i++是原子性操作吗？当然不是，首先它需要读取i&#x3D;0，然后需要执行运算，写入i的新值1，它包含了读取和写入两个步骤，所以不是原子性操作。 可见性可见性是指共享数据的时候，一个线程修改了数据，其他线程知道数据被修改，会重新读取最新的主存的数据。 举个例子： 线程1修改了i值，但是没有将i值复制到主内存中，线程2读取i的值，并将i的值赋值给j,我们期望j&#x3D;1,但是由于线程1修改了，没有来得及复制到主内存中，线程2读取了i,并赋值给j，这时j的值为0。 也就是线程i值被修改，其他线程并不知道。 有序性是指代码执行的有序性，因为代码有可能发生指令重排序（Instruction Reorder）。 Java 语言提供了 volatile 和 synchronized 两个关键字来线程代码操作的有序性，volatile 是因为其本身包含“禁止指令重排序”的语义，synchronized 在单线程中执行代码，无论指令是否重排，最终的执行结果是一致的。 volatile详解volatile关键字作用被volatile关键字修饰变量，起到了2个作用： 1.某个线程修改了被volatile关键字修饰变量是，根据数据一致性的协议，通过信号量，更改其他线程的高速缓存中volatile关键字修饰变量状态为无效状态，其他线程如果需要重写读取该变量会再次从主内存中读取，而不是读取自己的高速缓存中的。 2.被volatile关键字修饰变量不会指令重排序。 volatile能够保证可见性和防止指令重排在Java并发编程实战一书中有这样 在上述代码中，有可能（概率非常小，但是有这种可能性）永远不会打印a的值，因为线程ReadThread读取了主内存的ready为false,主线程虽然更新了ready，但是ReadThread的高速缓存中并没有更新。 另外： a &#x3D; 32; ready &#x3D; true; 这两行代码有可能发生指令重排。也就是可以打印出a的值为0。 如果在变量加上volatile关键字，可以防止上述两种不正常的情况的发生。 volatile不能保证原子性首先用一段代码测试下，开起了10个线程，这10个线程共享一个变量inc（被volatile修饰），并在每个线程循环1000次对inc进行inc++操作。我们预期的结果是10000. 多次运行main函数，你会发现结果永远都不会为10000，都是小于10000。可能有这样的疑问，volatile保证了共享数据的可见性，线程1修改了inc变量线程2会重新从主内存中重新读，这样就能保证inc++的正确性了啊，可为什么没有得到我们预期的结果呢？ 在之前已经讲述过inc++这样的操作不是一个原子性操作，它分为读、加加、写。一种情况，当线程1读取了inc的值，还没有修改，线程2也读取了，线程1修改完了，通知线程2将线程的缓存的 inc的值无效需要重读，可这时它不需要读取inc ，它仍执行写操作，然后赋值给主线程，这时数据就会出现问题。 所以volatile不能保证原子性 。这时需要用锁来保证,在increase方法加上synchronized，重新运行打印的结果为10000 。 volatile的使用场景状态标记volatile最常见的使用场景是状态标记，如下： 防止指令重排"},{"title":"ArchLinux开机挂载磁盘","date":"2020-08-11T16:00:00.000Z","url":"/2020/08/12/2020/08/121527/","tags":[["Arch Linux","/tags/Arch-Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"查看磁盘信息lsblk 查看UUID 编辑/etc/fstab 挂载"},{"title":"单Tomcat多项目部署session刷新问题","date":"2020-08-09T16:00:00.000Z","url":"/2020/08/10/2020/08/100903/","tags":[["tomcat","/tags/tomcat/"]],"categories":[["work","/categories/work/"]],"content":"问题描述将登录后的用户信息保存到session中，做登录校验以及日志的记录需要的相关参数。 本地的测试中没有问题，可以正常存储到前端以及后台的获取。 部署到服务器的tomcat中发现失效，排查之后发现，浏览器不能正常存储SESSIONID,每次请求，服务器都会生产新的SESSIONID. 解决方案 为每一个项目做单独的tomcat配置。，项目太多，不考虑。 修改tomcat的配置文件。指定SESSION的生成路径等。 sessionCookieDomain 用于为此上下文创建的所有会话cookie的域。如果设置，则它将覆盖Web应用程序设置的任何域。如果未设置，则将使用Web应用程序指定的值（如果有）。 sessionCookieName 用于为此上下文创建的所有会话cookie的名称。如果设置，它将覆盖Web应用程序设置的任何名称。如果未设置，则将使用Web应用程序指定的值（如果有），JSESSIONID如果Web应用程序未明确设置一个，则使用名称。 sessionCookiePath 用于为此上下文创建的所有会话cookie的路径。如果设置，它将覆盖Web应用程序设置的任何路径。如果未设置，则将使用Web应用程序指定的值，如果Web应用程序未明确设置一个，则使用上下文路径。要将所有Web应用程序配置为使用空路径（这对于portlet规范实现可能很有用），请&#x2F;在全局CATALINA_BASE&#x2F;conf&#x2F;context.xml 文件中将此属性设置为 。注意：使用的一个Web应用程序 sessionCookiePath&#x3D;”&#x2F;“获得会话后，同一主机中也配置了该会话的其他Web应用程序的所有后续会话sessionCookiePath&#x3D;”&#x2F;“将始终使用相同的会话ID。即使会话无效并创建了一个新会话，该设置仍然成立。这使得会话固定保护更加困难，并且需要定制的特定于Tomcat的代码来更改多个应用程序共享的会话ID。 context 官方文档tomcat-context"},{"title":"elasticsearch7.5&spring boot2.3.1/jpa","date":"2020-08-02T16:00:00.000Z","url":"/2020/08/03/2020/08/031941/","tags":[["elasticsearch","/tags/elasticsearch/"]],"categories":[["work","/categories/work/"]],"content":"需要注意：不同版本之间API差距较大。 项目依赖 配置文件 查询结果高亮controller service 页面效果采用thymeleaf模板渲染 "},{"title":"docker-compose安装calibre-web","date":"2020-07-31T16:00:00.000Z","url":"/2020/08/01/2020/08/011128/","tags":[["docker","/tags/docker/"],["calibre","/tags/calibre/"]],"categories":[["work","/categories/work/"]],"content":"docker-compose.yml 上传metqdata.db,使用本地的calibre生成一个空数据库文件，上传到./books所在的文件中，在配置页面添加书库为‘&#x2F;books’,然后就可以进去入页面了。 默认的帐号密码：admin/admin123 配置界面的语言设置：点击用户头像，语言下拉选择中文（简体，中国） Send to Kindle:头像设置里面添加kindle邮箱 书籍上传：基本配置-特性配置里面勾选启用上传 主题：UI配置里面主题可选择黑色主题 修改文件上传大小比较复杂，进入docker容器 修改文件/app/calibre-web/cps/server.py中的max_buffer_size=512000000,默认是200M.这里修改为500M. 保存之后退出，重启容器docker restart calibre-web。"},{"title":"第一章","date":"2020-07-29T16:00:00.000Z","url":"/2020/07/30/2020/07/302324/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"第一章"},{"title":"Spring 4 通过@Scheduled注解创建定时任务","date":"2020-07-12T16:00:00.000Z","url":"/2020/07/13/2020/07/130958/","tags":[["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":"pom首先我们需要引入spring-context-support 在需要的任务上加@Scheduled注解 corn在线生成corn在线生成"},{"title":"Fcitx5的安装","date":"2020-06-30T16:00:00.000Z","url":"/2020/07/01/2020/07/011019/","tags":[["fcitx5","/tags/fcitx5/"]],"categories":[["Linux","/categories/Linux/"]],"content":"安装 fcitx5为主体，fcitx5-chinese-addons中文输入方式支持 fcitx5-qt：对 Qt5 程序的支持 fcitx5-gtk：对 GTK 程序的支持 fcitx5-qt4-gitAUR：对 Qt4 程序的支持 kcm-fcitx5是KDE下的配置工具，不过在gnome下也可以正常使用。 提示： 一般情况下，只安装 fcitx5-qt 和 fcitx5-gtk就行了 配置工具fcitx5 的配置文件位于 ~/.local/share/fcitx5，尽管您可以使用文本编辑器编辑配置文件，但是使用 GUI 配置显然更方便： kcm-fcitx5：集成到 KCM 中的配置工具，专为 KDE 而生 fcitx5-config-qt-git AUR：Qt 前端的 fcitx5 配置工具，与 kcm-fcitx5 相冲突 注意： Fcitx5 开发人员明确表示不会考虑基于 GTK 的配置工具，对于非 KDE 界面，可以使用 fcitx5-config-qt-gitAUR。该软件包与 kcm-fcitx5 相冲突，你需要手动卸载它 环境变量 自启动ArchWiki 中文输入需要注意的是： fcitx5 只是提供了基本框架，基本框架只对英文提供了输入支持，如果需要输入其他语言，则需要安装相应的输入法引擎： 中文 fcitx5-chinese-addons 包含了大量中文输入方式：拼音、双拼、五笔拼音、自然码、仓颉、冰蟾全息、二笔等 fcitx5-rime, 对经典的 Rime IME 输入法的包装，内置了繁体中文和简体中文的支持。 fcitx5-chewing 对注音输入法 libchewing 的包装 打开fcitx5配置 找到简体中文，选择拼音 主题 GitHub:Fcitx5-Material-Color 设置单行模式在拼音输入法的设置中，启用“ *在程序中显示预编辑文本* ”即可启用单行模式 搜狗词库 Fcitx5-ArchWiki"},{"title":"docker安装elastcisearch&kibana","date":"2020-06-30T16:00:00.000Z","url":"/2020/07/01/2020/07/011615/","tags":[["elasticsearch","/tags/elasticsearch/"],["kibana","/tags/kibana/"]],"categories":[["work","/categories/work/"]],"content":"vm.max_map_count docker-compose.yml IK分词器 注意替换自己的对应版本"},{"title":"Komorebi壁纸分享","date":"2020-06-29T16:00:00.000Z","url":"/2020/06/30/2020/06/300957/","tags":[["wallpaper","/tags/wallpaper/"]],"categories":[["wuw","/categories/wuw/"]],"content":"Nier-2B预览图 网址 Nier-2B time-wallpaper&#x2F;樱花效果壁纸 地址： time-wallpaper compass[假装预览图片] Compass"},{"title":"HotSpot虚拟机对象","date":"2020-06-28T16:00:00.000Z","url":"/2020/06/29/2020/06/291045/","tags":[["java","/tags/java/"]],"categories":[["Java","/categories/Java/"]],"content":" 环境HotSpot虚拟机 new关键字 对象的创建 判断对像对应的类是否加载、链接、初始化。 为对象分配内存 处理并发 CAS&#x2F;TLAB –通过-XX:+UseTLAB参数来设置（8默认启用） 初始化分配到的空间—所有属性设置默认值，保证对象实例字段在不赋值时可以直接使用 设置对象的对象头 执行init方法进行初始化 当Java虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 在类加载检查通过之后，接下来虚拟机将为新生对象分配内存。 指针碰撞（Bump the Pointer）：假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离。 空闲列表（Free List）：如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理（Compact）功能决定。因此，在使用Serial、ParNew等带压缩整理过程的收集器时，系统采用的分配算法是指针碰撞，既简单又高效。而使用CMS这种基于清除（Sweep）算法的收集器时，理论上通常采用空闲列表。（设计的有分配缓冲区，可以在里面使用指针碰撞） 虚拟机采用CAS配上失败重试的方式保证更新操作的原子性。 另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）。哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。 虚拟机要对对象进行必要的设置，头信息。 构造函数，一般来说（Java编译器会在new同时生成new和invokespecial两条字节码指令，如果通过其他方式产生的则不一定如此），new指令之后会接着执行&lt;init&gt;()方法。 以上内容来自《深入理解Java虚拟机第三版》 对象的内存布局 对象的访问定位"},{"title":"Komorebi--Linux下动态壁纸","date":"2020-06-17T16:00:00.000Z","url":"/2020/06/18/2020/06/181429/","tags":[["Wallpaper","/tags/Wallpaper/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Manjaro&#x2F;Arch安装 创建壁纸wallpaper creator 将创建好的壁纸文件夹移动到Komorebi下 web壁纸由于komorebi没有自带的web服务器，无法部署静态页面，这里采用一个第三方的方案gitee pages 创建git仓库,上传静态页面，开启gitee pages 在创建壁纸中选择web,然后下一步即可，最后复制文件到壁纸目录。 示例： Nier-2B Variety"},{"title":"苏州之行","date":"2020-06-15T16:00:00.000Z","url":"/2020/06/16/2020/06/161156/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"天气雷暴天，接到通知，要求去苏州一趟，参加一个AI会议，早上出门的时候，特意看了天气，由于忘记带身份证到公司，就回去拿身份证，直接从家里出发。 出门的时候已经是大雨倾盆了，整个小区的路上都是积水，伞的作用有限，鞋子还是湿透了。打车到虹桥火车站，进车收伞的一瞬间，雨侵入了我的衣服。有的地方开始滴水了。 挨到了车站，下车进入候车室的时候，要穿过一条马路，前面的人没有经过 我以为有车，回头看了一眼，并没有车要经过，撑起伞走过去，一脚下雨，内心崩溃，“这么深的水”？好吧，鞋子完全透了。 整个世界都是潮湿的。 苏州的雨下的不大，断断续续，有江南梅雨的味道了。 旅程疫情的原因，进出站都是需要检查的。上海到苏州的高铁，速度很快,苏州园林居然也有高铁站了，不过在地下，信号不太好，还需要申请苏城码。导航也差点意思，先出站，然后找到公交车站候车。 到新的环境，方向感是不怎么好的，但我真不是路痴呀。 会议就是进出比较尴尬，真的不好找啊。虽然这是来此的主要目的，也不想过多描述。 食物没成想，在中国著名连锁点–沙县小吃，吃黄悶鸡米饭。找饭的途中，经过一座桥，沿着河边小道寻觅的时候，发现了一只水鸟，大抵也是在觅食吧。 风景细雨绵绵，弱柳扶风。 "},{"title":"redis：cant resolve localhost address错误","date":"2020-06-12T16:00:00.000Z","url":"/2020/06/13/2020/06/131417/","tags":[["spring","/tags/spring/"],["linux","/tags/linux/"],["redis","/tags/redis/"]],"categories":[["Linux","/categories/Linux/"]],"content":"配置文件 错误信息 解决方案查看Linux系统的主机名 修改/etc/hosts 添加127.0.0.1 hostname "},{"title":"Linux查看端口占用","date":"2020-06-11T16:00:00.000Z","url":"/2020/06/12/2020/06/121603/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"lsof netstat如果没有则安装 ArchLinux Centos 参数说明 Tip: LISTEN和LISTENING的状态只有用-a或者-l才能看到  "},{"title":"Ubuntu18.0.4　start-up script","date":"2020-06-08T16:00:00.000Z","url":"/2020/06/09/2020/06/091006/","tags":[["Ubuntu","/tags/Ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":"系统信息 方法一：crontab 方法二:&#x2F;etc&#x2F;rc.local建立rc-local.service文件 文件内容 创建文件rc.local 给rc.local加上权限,启用服务 启动服务并检查状态 方法三：systemctl *.service创建服务 /etc/systemd/system/ethtool.service 脚本 添加执行权限 开机启动 参考(原文访问较慢)askubuntu superuser"},{"title":"关于我","date":"2020-06-06T01:55:00.000Z","url":"/2020/06/06/2020/06/060955/","tags":[["daily","/tags/daily/"]],"categories":[["congco","/categories/congco/"]],"content":"专业技能主业 Java 开发 也兼职前端，UI Python爬虫等 联系方式congco@foxmail.com 图片来源网络"},{"title":"Windows 终端","date":"2020-06-02T16:00:00.000Z","url":"/2020/06/03/2020/06/032233/","tags":[["windows terminal","/tags/windows-terminal/"]],"categories":[["work","/categories/work/"]],"content":"什么是 Windows 终端？Windows 终端是一个面向命令行工具和 shell（如命令提示符、PowerShell 和适用于 Linux 的 Windows 子系统 (WSL)）用户的新式终端应用程序。 它的主要功能包括多个选项卡、窗格、Unicode 和 UTF-8 字符支持、GPU 加速文本呈现引擎，还可以用于创建你自己的主题并自定义文本、颜色、背景和快捷键绑定。 文档 配置文件设置配置文件设置"},{"title":"镜像站-mirror","date":"2020-05-30T16:00:00.000Z","url":"/2020/05/31/2020/05/311341/","tags":[["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"win系统镜像itellyou 开源镜像站MirrorZ南京大学镜像站腾讯开源镜像站华为开源镜像站阿里开源镜像站：网易开源镜像站：搜狐开源镜像站：北京交通大学开源镜像站：兰州大学开源软件镜像站：上海交通大学开源软件镜像站：清华大学开源软件镜像站：开源软件镜像站：北京外国语大学开源软件镜像站:中国科学技术大学开源软件镜像站：东北大学开源软件镜像站：东软信息学院开源软件镜像站：浙江大学开源软件镜像站：北京理工大学开源软件镜像站：华中科技大学开源软件镜像站：大连理工大学开源软件镜像站： 感谢以上网站为为国内开源生态作出的伟大贡献"},{"title":"Git设置user.name&user.email","date":"2020-05-28T16:00:00.000Z","url":"/2020/05/29/2020/05/291051/","tags":[["git","/tags/git/"]],"categories":[["Linux","/categories/Linux/"]],"content":" ssh key git全局设置 可以查看配置的一些东西。可以看到user.name 和user.email 分别是什么。 如果你没有初始化过。那么直接： 从Git版本控制中删除文件如果你想把一个文件从版本控制中移除，并且保留本地的文件，首先需要把这个文件加入到gitignore文件中。然后执行以下命令就可以了。 以上命令将file_path所代表的文件从版本控制中删除，并保留本地文件，此外还要进行commit操作才能将服务器端的文件删掉。如果想把一个文件夹从版本控制中删除并保留本地的文件，只需在上述命令的基础上加上-r参数，即 从版本控制中删除如果想把所有gitignore中的文件从版本控制中删除的话，需要执行以下两个命令，即先移除所有文件，再执行添加所有文件（这次会忽略gitignore中的文件）。 Git 全局设置 创建一个新仓库 推送现有文件夹 推送现有的 Git 仓库 提交模板建议"},{"title":"Docker镜像加速器","date":"2020-05-28T16:00:00.000Z","url":"/2020/05/29/2020/05/291058/","tags":[["docker","/tags/docker/"]],"categories":[["work","/categories/work/"]],"content":"Ubuntu&#x2F;ArchLinux针对Docker客户端版本大于 1.10.0 的用户 您可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器 Centos 安装／升级Docker客户端推荐安装1.10.0以上版本的Docker客户端，参考文档 docker-ce 配置镜像加速器针对Docker客户端版本大于 1.10.0 的用户 您可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器 镜像加速站"},{"title":"nginx配置http到https","date":"2020-05-28T16:00:00.000Z","url":"/2020/05/29/2020/05/291956/","tags":[["nginx","/tags/nginx/"]],"categories":[["work","/categories/work/"]],"content":"nginx配置"},{"title":"新冠疫情数据爬取","date":"2020-05-24T16:00:00.000Z","url":"/2020/05/25/2020/05/251006/","tags":[["python","/tags/python/"]],"categories":[["undefined",""]],"content":"仓库地址 页面解析选取腾讯新闻的接口 模拟请求头 数据请求 百度疫情新闻爬取"},{"title":"Spring Aop&IOC","date":"2020-05-22T16:00:00.000Z","url":"/2020/05/23/2020/05/231107/","tags":[["Spring","/tags/Spring/"]],"categories":[["Java","/categories/Java/"]],"content":"Spring的AOP理解OOP面向对象，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。 AOP，一般称为面向切面，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理。 AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。 （1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。 （2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理 ： ①JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。 ②如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的 3）静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。 Spring的IOC理解1）IOC就是控制反转，是指创建对象的控制权的转移，以前创建对象的主动权和时机是由自己把控的，而现在这种权力转移到Spring容器中，并由容器根据配置文件去创建实例和管理各个实例之间的依赖关系，对象与对象之间松散耦合，也利于功能的复用。DI依赖注入，和控制反转是同一个概念的不同角度的描述，即 应用程序在运行时依赖IoC容器来动态注入对象需要的外部资源。 （2）最直观的表达就是，IOC让对象的创建不用去new了，可以由spring自动生产，使用java的反射机制，根据配置文件在运行时动态的去创建对象以及管理对象，并调用对象的方法的。 （3）Spring的IOC有三种注入方式 ：构造器注入、setter方法注入、根据注解注入。 IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件 AOP AOP的全称是Aspect Orient Programming，即面向切面编程。是对OOP（Object Orient Programming）的一种补充，战门用于处理一些具有横切性质的服务。常常用于日志输出、安全控制等。 上面说到是对OOP的一种补充，具体补充的是什么呢？考虑一种情况，如果我们需要在所有方法执行前打印一句日志，按照OOP的处理思想，我们需要在每个业务方法开始时加入一些语句，但是我们辛辛苦苦加完之后，如果又要求在这句日志打印后再打印一句，那是不是又要加一遍？这时候你一定会想到，在某个类中编写一个日志打印方法，该方法执行这些日志打印操作，然后在每个业务方法之前加入这句方法调用，这就是面向对象编程思想。但是如果要求我们在业务方法结束时再打印一些日志呢，是不是还要去每个业务方法结束时加一遍？这样始终不是办法，而且我们总是在改业务方法，在业务方法里面掺杂了太多的其他操作，侵入性太高。 这时候AOP就起到作用了，我们可以编写一个切面类（Aspect），在其中的方法中来编写横切逻辑（如打印日志），然后通过配置或者注解的方式来声明该横切逻辑起作用的位置。 实现AOP（这里的AOP指的是面向切面编程思想，而不是Spring AOP）主要的的实现技术主要有Spring AOP和AspectJ。 1、AspectJ的底层技术。 AspectJ的底层技术是静态代理，即用一种AspectJ支持的特定语言编写切面，通过一个命令来编译，生成一个新的代理类，该代理类增强了业务类，这是在编译时增强，相对于下面说的运行时增强，编译时增强的性能更好。 2、Spring AOP Spring AOP采用的是动态代理，在运行期间对业务方法进行增强，所以不会生成新类，对于动态代理技术，Spring AOP提供了对JDK动态代理的支持以及CGLib的支持。 JDK动态代理只能为接口创建动态代理实例，而不能对类创建动态代理。需要获得被目标类的接口信息（应用Java的反射技术），生成一个实现了代理接口的动态代理类（字节码），再通过反射机制获得动态代理类的构造函数，利用构造函数生成动态代理类的实例对象，在调用具体方法前调用invokeHandler方法来处理。 CGLib动态代理需要依赖asm包，把被代理对象类的class文件加载进来，修改其字节码生成子类。 但是Spring AOP基于注解配置的情况下，需要依赖于AspectJ包的标准注解，但是不需要额外的编译以及AspectJ的织入器，而基于XML配置不需要 IOC:控制反转也叫依赖注入IOC利用java反射机制，AOP利用代理模式。所谓控制反转是指，本来被调用者的实例是由调用者来创建的，这样的缺点是耦合性太强，IOC则是统一交给spring来管理创建，将对象交给容器管理，你只需要在spring配置文件总配置相应的bean，以及设置相关的属性，让spring容器来生成类的实例对象以及管理对象。在spring容器启动的时候，spring会把你在配置文件中配置的bean都初始化好，然后在你需要调用的时候，就把它已经初始化好的那些bean分配给你需要调用这些bean的类。 AOP：面向切面编程。（Aspect-Oriented Programming)AOP可以说是对OOP的补充和完善。OOP引入封装、继承和多态性等概念来建立一种对象层次结构，用以模拟公共行为的一个集合。实现AOP的技术，主要分为两大类：一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码，属于静态代理. AOP 思想： 基于代理思想，对原来目标对象，创建代理对象，在不修改原对象代码情况下，通过代理对象，调用增强功能的代码，从而对原有业务方法进行增强 ！ 1、JDK动态代理依赖 接口 实现 动态代理 2、Cglib动态代理在实际开发中，可能需要对没有实现接口的类增强，用JDK动态代理的方式就没法实现。采用Cglib动态代理可以对没有实现接口的类产生代理，实际上是生成了目标类的子类来增强。 首先，需要导入Cglib所需的jar包。提示：spring已经集成了cglib，我们已经导入了spring包，所以不需要再导入其它包了。 接口 动态代理 "},{"title":"MyBatis","date":"2020-05-21T16:00:00.000Z","url":"/2020/05/22/2020/05/221949/","tags":[["mybatis","/tags/mybatis/"]],"categories":[["Java","/categories/Java/"]],"content":"简介MyBatis 是一款优秀的持久层框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 字符串替换默认情况下，使用 #{} 参数语法时，MyBatis 会创建 PreparedStatement 参数占位符，并通过占位符安全地设置参数（就像使用 ? 一样）。 这样做更安全，更迅速，通常也是首选做法，不过有时你就是想直接在 SQL 语句中直接插入一个不转义的字符串。 比如 ORDER BY 子句，这时候你可以： 这样，MyBatis 就不会修改或转义该字符串了。 当 SQL 语句中的元数据（如表名或列名）是动态生成的时候，字符串替换将会非常有用。 举个例子，如果你想 select 一个表任意一列的数据时，不需要这样写： &#x2F;&#x2F; 其它的 “findByXxx” 方法而是可以只写这样一个方法： 其中 $&#123;column&#125; 会被直接替换，而 #&#123;value&#125; 会使用 ? 预处理。 这样，就能完成同样的任务： 这种方式也同样适用于替换表名的情况。 提示 用这种方式接受用户的输入，并用作语句参数是不安全的，会导致潜在的 SQL 注入攻击。因此，要么不允许用户输入这些字段，要么自行转义并检验这些参 结果映射resultMap 元素是 MyBatis 中最重要最强大的元素。它可以让你从 90% 的 JDBC ResultSets 数据提取代码中解放出来，并在一些情形下允许你进行一些 JDBC 不支持的操作。实际上，在为一些比如连接的复杂语句编写映射代码的时候，一份 resultMap 能够代替实现同等功能的数千行代码。ResultMap 的设计思想是，对简单的语句做到零配置，对于复杂一点的语句，只需要描述语句之间的关系就行了。 之前你已经见过简单映射语句的示例，它们没有显式指定 resultMap。比如： 上述语句只是简单地将所有的列映射到 HashMap 的键上，这由 resultType 属性指定。虽然在大部分情况下都够用，但是 HashMap 并不是一个很好的领域模型。你的程序更可能会使用 JavaBean 或 POJO（Plain Old Java Objects，普通老式 Java 对象）作为领域模型。MyBatis 对两者都提供了支持。看看下面这个 JavaBean： 基于 JavaBean 的规范，上面这个类有 3 个属性：id，username 和 hashedPassword。这些属性会对应到 select 语句中的列名。 这样的一个 JavaBean 可以被映射到 ResultSet，就像映射到 HashMap 一样简单。 类型别名是你的好帮手。使用它们，你就可以不用输入类的全限定名了。比如： 在这些情况下，MyBatis 会在幕后自动创建一个 ResultMap，再根据属性名来映射列到 JavaBean 的属性上。如果列名和属性名不能匹配上，可以在 SELECT 语句中设置列别名（这是一个基本的 SQL 特性）来完成匹配。比如： 在学习了上面的知识后，你会发现上面的例子没有一个需要显式配置 ResultMap，这就是 ResultMap 的优秀之处——你完全可以不用显式地配置它们。 虽然上面的例子不用显式配置 ResultMap。 但为了讲解，我们来看看如果在刚刚的示例中，显式使用外部的 resultMap 会怎样，这也是解决列名不匹配的另外一种方式。 然后在引用它的语句中设置 resultMap 属性就行了（注意我们去掉了 resultType 属性）。比如: 如果这个世界总是这么简单就好了。 映射器注解设计初期的 MyBatis 是一个 XML 驱动的框架。配置信息是基于 XML 的，映射语句也是定义在 XML 中的。而在 MyBatis 3 中，我们提供了其它的配置方式。MyBatis 3 构建在全面且强大的基于 Java 语言的配置 API 之上。它是 XML 和注解配置的基础。注解提供了一种简单且低成本的方式来实现简单的映射语句。 提示 不幸的是，Java 注解的表达能力和灵活性十分有限。尽管我们花了很多时间在调查、设计和试验上，但最强大的 MyBatis 映射并不能用注解来构建——我们真没开玩笑。而 C# 属性就没有这些限制，因此 MyBatis.NET 的配置会比 XML 有更大的选择余地。虽说如此，基于 Java 注解的配置还是有它的好处的。 注解 使用对象 XML 等价形式 描述 @CacheNamespace 类 &lt;cache&gt; 为给定的命名空间（比如类）配置缓存。属性：implemetation、eviction、flushInterval、size、readWrite、blocking、properties。 @Property N&#x2F;A &lt;property&gt; 指定参数值或占位符（placeholder）（该占位符能被 mybatis-config.xml 内的配置属性替换）。属性：name、value。（仅在 MyBatis 3.4.2 以上可用） @CacheNamespaceRef 类 &lt;cacheRef&gt; 引用另外一个命名空间的缓存以供使用。注意，即使共享相同的全限定类名，在 XML 映射文件中声明的缓存仍被识别为一个独立的命名空间。属性：value、name。如果你使用了这个注解，你应设置 value 或者 name 属性的其中一个。value 属性用于指定能够表示该命名空间的 Java 类型（命名空间名就是该 Java 类型的全限定类名），name 属性（这个属性仅在 MyBatis 3.4.2 以上可用）则直接指定了命名空间的名字。 @ConstructorArgs 方法 &lt;constructor&gt; 收集一组结果以传递给一个结果对象的构造方法。属性：value，它是一个 Arg 数组。 @Arg N/A - &lt;arg&gt; -&lt;idArg&gt; ConstructorArgs 集合的一部分，代表一个构造方法参数。属性：id、column、javaType、jdbcType、typeHandler、select、resultMap。id 属性和 XML 元素 &lt;idArg&gt; 相似，它是一个布尔值，表示该属性是否用于唯一标识和比较对象。从版本 3.5.4 开始，该注解变为可重复注解。 @TypeDiscriminator 方法 &lt;discriminator&gt; 决定使用何种结果映射的一组取值（case）。属性：column、javaType、jdbcType、typeHandler、cases。cases 属性是一个 Case 的数组。 @Case N&#x2F;A &lt;case&gt; 表示某个值的一个取值以及该取值对应的映射。属性：value、type、results。results 属性是一个 Results 的数组，因此这个注解实际上和 ResultMap 很相似，由下面的 Results 注解指定。 @Results 方法 &lt;resultMap&gt; 一组结果映射，指定了对某个特定结果列，映射到某个属性或字段的方式。属性：value、id。value 属性是一个 Result 注解的数组。而 id 属性则是结果映射的名称。从版本 3.5.4 开始，该注解变为可重复注解。 @Result N&#x2F;A - &lt;result&gt;- &lt;id&gt; 在列和属性或字段之间的单个结果映射。属性：id、column、javaType、jdbcType、typeHandler、one、many。id 属性和 XML 元素 &lt;id&gt; 相似，它是一个布尔值，表示该属性是否用于唯一标识和比较对象。one 属性是一个关联，和 &lt;association&gt; 类似，而 many 属性则是集合关联，和 &lt;collection&gt; 类似。这样命名是为了避免产生名称冲突。 @One N&#x2F;A &lt;association&gt; 复杂类型的单个属性映射。属性：select，指定可加载合适类型实例的映射语句（也就是映射器方法）全限定名；fetchType，指定在该映射中覆盖全局配置参数 lazyLoadingEnabled。提示 注解 API 不支持联合映射。这是由于 Java 注解不允许产生循环引用。 @Many N&#x2F;A &lt;collection&gt; 复杂类型的集合属性映射。属性：select，指定可加载合适类型实例集合的映射语句（也就是映射器方法）全限定名；fetchType，指定在该映射中覆盖全局配置参数 lazyLoadingEnabled。提示 注解 API 不支持联合映射。这是由于 Java 注解不允许产生循环引用。 @MapKey 方法 供返回值为 Map 的方法使用的注解。它使用对象的某个属性作为 key，将对象 List 转化为 Map。属性：value，指定作为 Map 的 key 值的对象属性名。 @Options 方法 映射语句的属性 该注解允许你指定大部分开关和配置选项，它们通常在映射语句上作为属性出现。与在注解上提供大量的属性相比，Options 注解提供了一致、清晰的方式来指定选项。属性：useCache&#x3D;true、flushCache&#x3D;FlushCachePolicy.DEFAULT、resultSetType&#x3D;DEFAULT、statementType&#x3D;PREPARED、fetchSize&#x3D;-1、timeout&#x3D;-1、useGeneratedKeys&#x3D;false、keyProperty&#x3D;””、keyColumn&#x3D;””、resultSets&#x3D;””。注意，Java 注解无法指定 null 值。因此，一旦你使用了 Options 注解，你的语句就会被上述属性的默认值所影响。要注意避免默认值带来的非预期行为。 注意：keyColumn 属性只在某些数据库中有效（如 Oracle、PostgreSQL 等）。要了解更多关于 keyColumn 和 keyProperty 可选值信息，请查看“insert, update 和 delete”一节。 - @Insert- @Update- @Delete- @Select 方法 - &lt;insert&gt;- &lt;update&gt;- &lt;delete&gt;- &lt;select&gt; 每个注解分别代表将会被执行的 SQL 语句。它们用字符串数组（或单个字符串）作为参数。如果传递的是字符串数组，字符串数组会被连接成单个完整的字符串，每个字符串之间加入一个空格。这有效地避免了用 Java 代码构建 SQL 语句时产生的“丢失空格”问题。当然，你也可以提前手动连接好字符串。属性：value，指定用来组成单个 SQL 语句的字符串数组。 - @InsertProvider- @UpdateProvider- @DeleteProvider- @SelectProvider 方法 - &lt;insert&gt;- &lt;update&gt;- &lt;delete&gt;- &lt;select&gt; 允许构建动态 SQL。这些备选的 SQL 注解允许你指定返回 SQL 语句的类和方法，以供运行时执行。（从 MyBatis 3.4.6 开始，可以使用 CharSequence 代替 String 来作为返回类型）。当执行映射语句时，MyBatis 会实例化注解指定的类，并调用注解指定的方法。你可以通过 ProviderContext 传递映射方法接收到的参数、”Mapper interface type” 和 “Mapper method”（仅在 MyBatis 3.4.5 以上支持）作为参数。（MyBatis 3.4 以上支持传入多个参数）属性：type、method。type 属性用于指定类名。method 用于指定该类的方法名（从版本 3.5.1 开始，可以省略 method 属性，MyBatis 将会使用 ProviderMethodResolver 接口解析方法的具体实现。如果解析失败，MyBatis 将会使用名为 provideSql 的降级实现）。提示 接下来的“SQL 语句构建器”一章将会讨论该话题，以帮助你以更清晰、更便于阅读的方式构建动态 SQL。 @Param 参数 N&#x2F;A 如果你的映射方法接受多个参数，就可以使用这个注解自定义每个参数的名字。否则在默认情况下，除 RowBounds 以外的参数会以 “param” 加参数位置被命名。例如 #{param1}, #{param2}。如果使用了 @Param(“person”)，参数就会被命名为 #{person}。 @SelectKey 方法 &lt;selectKey&gt; 这个注解的功能与 &lt;selectKey&gt; 标签完全一致。该注解只能在 @Insert 或 @InsertProvider 或 @Update 或 @UpdateProvider 标注的方法上使用，否则将会被忽略。如果标注了 @SelectKey 注解，MyBatis 将会忽略掉由 @Options 注解所设置的生成主键或设置（configuration）属性。属性：statement 以字符串数组形式指定将会被执行的 SQL 语句，keyProperty 指定作为参数传入的对象对应属性的名称，该属性将会更新成新的值，before 可以指定为 true 或 false 以指明 SQL 语句应被在插入语句的之前还是之后执行。resultType 则指定 keyProperty 的 Java 类型。statementType 则用于选择语句类型，可以选择 STATEMENT、PREPARED 或 CALLABLE 之一，它们分别对应于 Statement、PreparedStatement 和 CallableStatement。默认值是 PREPARED。 @ResultMap 方法 N&#x2F;A 这个注解为 @Select 或者 @SelectProvider 注解指定 XML 映射中 &lt;resultMap&gt; 元素的 id。这使得注解的 select 可以复用已在 XML 中定义的 ResultMap。如果标注的 select 注解中存在 @Results 或者 @ConstructorArgs 注解，这两个注解将被此注解覆盖。 @ResultType 方法 N&#x2F;A 在使用了结果处理器的情况下，需要使用此注解。由于此时的返回类型为 void，所以 Mybatis 需要有一种方法来判断每一行返回的对象类型。如果在 XML 有对应的结果映射，请使用 @ResultMap 注解。如果结果类型在 XML 的 &lt;select&gt; 元素中指定了，就不需要使用其它注解了。否则就需要使用此注解。比如，如果一个标注了 @Select 的方法想要使用结果处理器，那么它的返回类型必须是 void，并且必须使用这个注解（或者 @ResultMap）。这个注解仅在方法返回类型是 void 的情况下生效。 @Flush 方法 N&#x2F;A 如果使用了这个注解，定义在 Mapper 接口中的方法就能够调用 SqlSession#flushStatements() 方法。（Mybatis 3.3 以上可用） 映射注解示例这个例子展示了如何使用 @SelectKey 注解来在插入前读取数据库序列的值： 这个例子展示了如何使用 @Flush 注解来调用 SqlSession#flushStatements()： 增删改查"},{"title":"Manjaro gnome桌面美化","date":"2020-05-17T16:00:00.000Z","url":"/2020/05/18/2020/05/182018/","tags":[["manjaro","/tags/manjaro/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 网站地址： 主题 图标 壁纸 shell 安装OCS-InstallArch Linux "},{"title":"如何用calibre给txt等格式的文本添加目录","date":"2020-05-16T16:00:00.000Z","url":"/2020/05/17/2020/05/172123/","tags":[["calibre","/tags/calibre/"]],"categories":[["work","/categories/work/"]],"content":"正则表达式 步骤 "},{"title":"mysql数据导入以及去重","date":"2020-05-12T16:00:00.000Z","url":"/2020/05/13/2020/05/131832/","tags":[["mysql","/tags/mysql/"]],"categories":[["work","/categories/work/"]],"content":" 需求分析将一个数据库中的表字段值导入到另外一个数据库的表中 sql 获取需要导入的表MySQL中 如何查询表名中包含某字段的表 去重 验证 使用python处理sql文件"},{"title":"Linux下安装&更新gitlab-ce","date":"2020-05-05T16:00:00.000Z","url":"/2020/05/06/2020/05/061623/","tags":[["gitlab","/tags/gitlab/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Centos系统信息 安装docker-compose 安装vim 编辑docker-compose.ymlvim docker-compose.yml写入以下内容 启动在yml文件所在路径下执行 ubuntu18.04LTS官方网站教程： 安装国内可能会遇到访问速度的问题，可以使用清华源 1.安装和配置必要的依赖项 接下来，安装Postfix发送通知电子邮件。如果要使用其他解决方案发送电子邮件，请跳过此步骤并 在安装GitLab之后配置外部SMTP服务器。 在Postfix安装过程中，可能会出现一个配置屏幕。选择“ Internet网站”，然后按Enter。使用服务器的外部DNS作为“邮件名”，然后按Enter。如果出现其他屏幕，请继续按Enter接受默认设置。 2.添加GitLab软件包存储库并安装软件包添加GitLab软件包存储库。 对于，GitLab将使用Let&#39;s Encrypt自动请求证书，这需要入站HTTP访问和有效的主机名。您也可以使用自己的证书或仅使用http：&#x2F;&#x2F;。 注意这里安装的是企业版，区别相关说明参照 3.浏览到主机名并登录首次访问时，您将被重定向到密码重置屏幕。提供初始管理员帐户的密码，您将被重定向回登录屏幕。使用默认帐户的用户名root登录。 更新社区版 (企业版为gitlab-ee) 1.进行备份（可选）如果您想在更新前进行备份，则/var/opt/gitlab/backups 默认情况下，以下命令将备份数据 。 2.更新GitLab更新到最新版本的GitLab。 使用docker-compose安装GitLab官方文档： 使用Docker compose，您可以轻松配置，安装和升级基于Docker的GitLab安装。 安装 Docker Compose 创建一个docker-compose.yml文件（或下载一个示例）： 确保您docker-compose.yml与docker-compose up -dGitLab 位于同一目录并运行 以启动阅读“预配置Docker容器”以了解GITLAB_OMNIBUS_CONFIG变量的工作方式。 下面是docker-compose.yml在自定义HTTP和SSH端口上运行GitLab的另一个示例。注意GITLAB_OMNIBUS_CONFIG变量如何匹配该 ports部分： 这与使用相同--publish 8929:8929 --publish 2224:22。 使用Docker compose更新GitLab假设您使用docker-compose安装了GitLab，则只需运行docker-compose pull并docker-compose up -d下载新版本并升级GitLab实例即可。"},{"title":"Manjaro下修改grub启动背景","date":"2020-05-04T16:00:00.000Z","url":"/2020/05/05/2020/05/051700/","tags":[["Manjaro","/tags/Manjaro/"],["grub","/tags/grub/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 方法一安装工具grub customizer 设置外观 方法二首先，找一张自己喜欢的图片，png格式manjaro使用的主题位置/usr/share/grub/themes/manjaro 复制图片到指定位置 更新配置 重启系统"},{"title":"五月","date":"2020-05-02T16:00:00.000Z","url":"/2020/05/03/2020/05/032254/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"2020-05-03昨天从家里出发，今天凌晨四点到的上海，上海今天很热 "},{"title":"Arch Linux 清理包","date":"2020-04-25T16:00:00.000Z","url":"/2020/04/26/2020/04/arch_linux_clean/","tags":[["Arch Linux","/tags/Arch-Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"删除孤立软件包(注意可能会清理一些还在使用的包，例如yarn) 如果没有孤立软件包，将显示错误 error: no targets specified. 这个是正常的，因为 pacman -Rns 没有收到任何参数. 清除已下载的安装包 pacman (简体中文) - ArchWiki 删除孤立软件包"},{"title":"四月是你的谎言","date":"2020-04-19T16:00:00.000Z","url":"/2020/04/20/2020/04/daily/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"不知道决定开始不在用日期和时间来命名了，虽然看起来很规范，但是查找实在是不方便啊。还是根据内容来命名吧。 由于工作的需要，最近又看了一下hexo来做博客的东西。不得不说，发展的很好，主题什么的也是很完善的，但是因为一开始就选用了这个，所以修改的可能性不是很大，暂时就是这样吧。 2020-04-27五一假期，可能要回家一次。 带上旧电脑，家里有网了。 过年期间，因为疫情，不能按时回来上班，有一个办法就是远程办公，但是家里没网，和同事说，都没人相信，怎么可能会没有网呢。现在这个时期，没有网络真的挺难的。 所以有计划把旧的电脑带回去，然后开启远程桌面 ，想看啥都可以远程操作了。"},{"title":"三月拾遗","date":"2020-03-08T16:00:00.000Z","url":"/2020/03/09/2020/03/091511/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"一 漫漫归程不得不说，今年是很特殊的一年。 回去的时候，还没赶上公司放假，搭了老同学的顺风车回家。每年的春运都是这个样子，很是拥挤。到家的时候已经是凌晨了。 爸爸妈妈一直在等我回去，还没有入睡，表哥在后面的公路上接我回家，为了不两次叫开门。也是很辛苦了，毕竟老家的冬天不是那么的暖和。和家人聊天说了会话，夜比较深了，于是就都睡了。 表哥今年本打算在家过年的，姑父回去了，然后又一起回了闽，不得不说是很巧合了。 二 梦魇疫情开始的时候，家里人都没有关注，也没有重视，开始的几天，我很紧张，也挺害怕的，途中还去参加了同学的婚礼，后面还有两个都取消了。这样紧张的情绪，导致了我后来在夜里感觉自己呼吸困难，上气不接下气，十分的危险了，自己吓自己真的要不得。拍醒了沉睡中的大哥， “我好像不行了，得了那个病了~” “怎么可能，起来喝点水，没事” 好吧，去了下洗手间，然后回来倒了开水，喝了点，找到了体温计，测量了一下，夜里体温是比较低的，没有发烧，深呼吸，渐渐的没有了呼吸难受的感觉。之前真的是觉得什么症状都在身体上上演了一次。 不得不说，大哥好淡定。 还有一件事就是不想让家里人出去串门，我都准备把门锁上了。 三 家庭生活家里有五口人，凑一桌牌还能剩下一个可以替换的，如果是斗地主就有两个闲人了。 家里喜欢打一种两副牌的“双扣”。 然后是做饭，蒸煮煎炸。 做饭&emsp;&emsp;在家一开始还有些新奇，毕竟很多年没有在家待这么久了。渐渐的也没了些意思。想着做一些之前没有做过的东西，网上找了教程，准备做懒人蛋饺。在煤气灶上将铜勺烧热，涂上一点油，然后将打好的鸡蛋液放进去，缓慢摇动勺子，让蛋液凝固成型，然后放上馅，包好即可从出勺了。 &emsp;&emsp;后来又准备开发一下烙饼，也是我下厨这么多年来认真的一次擀面皮，和面擀皮的事情，我不是很擅长，从来都是我妈做好的。烫熟了玉米面，将面皮擀好，撒上了葱花，加入了玉米面，从头饧面，然后再擀好就可以下锅了。 &emsp;&emsp;后来啊，又做了煎馍，还有个烙馍没学会。 &emsp;&emsp;慢慢的会会议起更多的事情，所以这个记录会显得有些杂乱。最近又想起来某一天做火锅的事情。家里一共五口人，四个人在打牌，我看了一会，刚好到了中午，就提议他们继续打，今天的午饭我来负责。做什么最简单呢，当然是火锅了，把馒头热好，准备火锅食材，调制好火锅汤底，也不是什么难的嘛。妈妈建议我哥或者爸爸也来这么做一次，后面也没有实现。 挖地 劈柴 喂鹅 种树 剪枝妈妈说，应该是爷爷还在的时候，很喜欢吃桃子，每次吃完之后的桃核便会用力使其飞跃屋脊，落到厨房后面的一片空地上。现如今，已经长出了三棵桃树了，看起来还小，距离结果应该还差些年月，枝丫长得很是疯狂，刚好，如今有些时间，便找来工具，修剪了起来，桃树需要将头剪掉，让其不至于疯长，发叉，可以结出更多的桃子来。 后来没有等到开花就离开了家，电话里据说桃花开的很绚烂。 2020-03-21&emsp;&emsp;隔离期终于结束了，因为昨天不满24小时，所以还不行。今天来公司上班了。进行正常的工作。换了新的出入证，进出小区测量了体温，进园区也要测量一次，园区的超市开门了，可以去买早餐。日常还会关注一下疫情发展，国外正在流行爆发，也就导致了很多人往国内跑的情况。希望早日战胜病毒。 2020-03-27&emsp;&emsp;开始工作的一周，因为在家休息的时间过久，现在真的是整个人变得懒散了许多，做了一些计划安排，但是还没有执行， 工作计划 分析需求 搭建框架 设计表结构 编写代码 测试 "},{"title":"Spring事务配置","date":"2020-03-08T16:00:00.000Z","url":"/2020/03/09/2020/04/101911/","tags":[["spring","/tags/spring/"]],"categories":[["work","/categories/work/"]],"content":" Spring 事务的隔离性 Spring事务的传播行为 事务配置 applicationContext-transaction.xml mapper service 单元测试 结果 数据库表没有插入新的数据 参考博客 Spring使用注解方式进行事务管理 Spring 事物机制总结 Spring AOP 中@Pointcut的用法 Spring注解事务 "},{"title":"SpringMVC AOP","date":"2020-03-08T16:00:00.000Z","url":"/2020/03/09/2020/04/161633/","tags":[["spring","/tags/spring/"]],"categories":[["work","/categories/work/"]],"content":"spingmvc配置AOP有两种方式，一种是利用注解的方式配置，另一种是XML配置实现。 注解方式依赖环境 mvc配置文件 代码 在类上加上注解Aspect声明这个类是一个切面，加上component注解加入IOC容器中。Order注解是优先级，如果只有一个切面类可以不用。注解pointcut是声明注解的作用范围。execution(public * com.example.controller.Controller.(..))我这里用的execution表达式，代表作用com.example.controller包下所有以Controller结尾类的所有Public方法。@pointcut所描述的方法里面通常不需要内容。切面通知包含： 前置通知（@Before,在执行方法之前，参数为JoinPoint） 后置通知(@After,无论方法抛不抛异常都会执行，所以获取不到方法的返回值。参数为JoinPoint) 返回通知（@AfterReturning,在方法正常结束后执行，可以获取到方法的返回值。参数为JoinPoint和result(Object)） 异常通知（@AfterThrowing,在方法抛出异常时执行，可以获取到异常对象，且可以指定在出现特定异常时执行的代码，参数为JoinPoint何Exception） 环绕通知（@Around,环绕通知需要携带的类型为ProceedingJoinPoint类型的参数, 环绕通知包含前置、后置、返回、异常通知；ProceedingJoinPoin 类型的参数可以决定是否执行目标方法,且环绕通知必须有返回值，返回值即目标方法的返回值) 应用XML方式配置 可以看到，这个类除了没有@Aspect以及相关切面的注解外，跟正常的AOP切面类没有差别。如果声明其让其为一个切面。需要我们在xml文件中手动配置。 "},{"title":"二月日记","date":"2020-02-26T16:00:00.000Z","url":"/2020/02/27/2020/02/211127/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"02-21武汉爆发了新的肺炎疫情，正逢春运，导致了全国各地相继爆发，甚至蔓延国外。 为了控制疫情，防止进一步蔓延，交通封闭，一直在家，未有返工。 信阳，距离武汉很近，感染人数是河南之最，今天，其他的市区都已恢复交通，唯有信阳、南阳、郑州等待进一步观察，新增的两例患者也是信阳和郑州的。 返回上海，也是可以的，但是需要居家隔离观察14天，吃喝多少都是问题，而且公司所在的园区也没有允许复工，所以还是老老实实的呆在家中。"},{"title":"流云与风","date":"2020-01-15T16:00:00.000Z","url":"/2020/01/16/2020/01/160951/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"序章 短命的杀手 师父曾经说过，当杀手就不能嗜酒。 果然，死在酒上了。 少年成名，无论是帝国皇帝，还是路边乞丐，给钱，统统帮你搞定。 不久前刚完成了一单，一如往常那样顺利。 高兴之余，去了那家酒馆，是什么让我放松了警惕呢，杀人后的轻松愉悦？ 零距離城門不遠処的道路旁，开着镇上唯一的一家客栈—雲來客棧。原本門可羅雀的客棧由於下雨，多了許多躲雨的客人，一下子变得热闹起来了，身为店里唯一的跑堂的，突然间的忙碌，还是很令人心情愉悦的，老板看到这么多人，说不定一开心，这个月能多几钱银子呢。 “小二，来二两熟牛肉，再来二斤烧刀子。” “得嘞，客官您稍等，马上就来。” 。。。 一店小二只是我的爱好，体验生活的，顺带可以融入环境和收集情报。 没错，杀手，才是本职工作，嗯，今天接了一单生意，要去京城一趟才行，其实我是不大愿意去的，捕快那么多。 当然，以我的本领，自然是不带怕的，但我这个人就是怕麻烦，追来追去的，害的人家都瘦了。 也不能太胖，影响轻功的发挥。 起源于那天听到的一个故事。 下雨那天，听店里客人闲聊，说是隔壁县城，李员外家的小姐忧思成疾。李员外遍请十里八乡的名医也没与办法，说是相思成疾,细问之下，才知爱上了一位前几年路过的进京赶考的书生。 哎，什么狗血剧情都被我遇上了。 本不打算继续听下去的，但是看大家都这么有兴趣，也没人叫茶水了。 我要是有这么好的运气，也不至于沦落至此。娶媳妇太费钱了。 二又开始下雨了。 所以，根本不可能会有月亮。漆黑的屋子里，什么也看不见。 床上也空无一人，没有上床睡觉的习惯。 从老家出门的时候，就想着能有好的发展，一开始感觉县城应该不错，机会多，也容易挣钱。 在一位老乡的介绍下，加入了组织。可是一直都没有任务轮到我。 等待了很久，终于等到了。因为别的人都不愿意接，从来没开张的我，已经经济困难了。无论是什么，我都想做一笔。 “糖葫芦哦，一文钱一串”，一个小孩子在管家的带领下准备去买一串，真的是十分的单纯可爱。 “来福，他是干什么的呀，怎么脏兮兮的，好像很久没有吃饭样子啊”，小女孩指着我对身边的管家说到。 “乞丐吧”"},{"title":"通过反射获取类的属性以及注解名称","date":"2020-01-12T16:00:00.000Z","url":"/2020/01/13/2020/01/132012/","tags":[["reflect","/tags/reflect/"]],"categories":[["Java","/categories/Java/"]],"content":"需求分析大佬突发奇想，需要做一个根据当前类的所有字段进行选择然后匹配自的查询，简单点就是多几个搜索框，每一个对应一个字段，还可以联合查询，大佬只是想要根据一个字段就可以了，而且需要在很多地方复用，还要求下拉框选择，前端附身，搞好了下拉列表。 服务端通过反射获取类的字段以及注解，封装成下拉列表需要的格式返回即可。 这里主要使用了lang3 代码注解 Entity类 下拉框数据格式 主要方法 页面部分"},{"title":"二零年一月份记录","date":"2020-01-05T16:00:00.000Z","url":"/2020/01/06/2020/01/061035/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"2020-01-06周一 晴 小寒 早上起床，问过天猫精灵，发现今天气温有点不正常，小寒没有排面。 估计过两天之后就会有一个大降温了，周末接到老哥的电话，郑州已经大雪了，山西也在下雪，今年姑姑应该不会回家过年了，见不到我卡哇伊的妹妹了。 表哥会回去，但是和我时间不一致，回来倒是可能一起，今年的火车票还是那么的难买，话说阜阳到上海已经开通了高铁，希望可以体验一把，毕竟之前十个多小时的路程，现在只需要三个小时多就可以到达了，免得旅途太长，体验太差。 又是一年过去了，在家待的时间也很短，说缺少年味的，可能很大原因就是这里吧，物质缺乏早已不像小时候了。没有说过年才吃肉，父辈也越来越年纪大了，有的人也永远的离开了我们。 2020-01-08周三 多云 买到了回家的火车票了，阜阳刚开通了高铁，第一次，还没去过那边的高铁站，不知道怎么到达，不过，肯定会有配套的交通。印象深刻的是阜阳的公交车，一路超速到火车站的，惊心动魄。 2020-01-13周一 多云 回家过年前的最后一周？NO,还不是，下周还要上一天班。又开始了，五福，虽然每年的红包并没有多少，过程却是挺开心的。加油！ 可能要买一些年货回去。 你为什么要紧张你难道没有注意到你自己的样子吗你美丽又聪慧我需要使尽浑身解数才能打动你 2020-01-16周四 雨 糟糕透了，越发的不懂，不能够明白。 不理解，也不想去思考。什么也不要想，不要去做。 可能，也许，大概，To be end. 放不下，痛苦，难过，却也无力去做什么了。 始于两天前的一句对话，但种子可能早已种下。"},{"title":"EwoMail","date":"2020-01-02T16:00:00.000Z","url":"/2020/01/03/2020/01/031715/","tags":[["docker","/tags/docker/"],["ewomail","/tags/ewomail/"]],"categories":[["work","/categories/work/"]],"content":"官方文档： 文章原地址： EwoMail 开源企业邮件系统 的docker镜像EwoMail-Admin版本为1.05 rainloop 版本为1.11.1运行docker mail.ewomail.com 换成自己的域名 docker-compose方式 或 可配置参数 自定义证书映射 &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;dovecot.pem，&#x2F;etc&#x2F;ssl&#x2F;private&#x2F;dovecot.pem，&#x2F;ewomail&#x2F;dkim&#x2F;mail.pem运行成功后访问 邮箱管理后台 默认用户: admin 默认密码: ewomail123 Rainloop 管理端  默认用户: admin 默认密码: 12345 Rainloop 用户端  设置域名DNS这里使用万网DNS为参考 将mail.ewomail.cn 改成你的域名 spf记录：v&#x3D;spf1 include:ewomail.cn -all 红色部分请改为你的服务器IP DKIM设置DKIM是电子邮件验证标准，域名密钥识别邮件标准，主要是用来防止被判定为垃圾邮件。 每个域名都需要添加一个dkim的key，EwoMail默认安装后已自动添加主域名dkim，只需要设置好dkim的dns即可。 获取dkim key 执行查看代码 若安装成功会输出以下信息： 整理后，设置DNS 域名 记录类型 主机记录 记录值 ewomail.com TXT dkim._domainkey v&#x3D;DKIM1;p&#x3D;MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC09HcLpwcdgWtzkrZDBRBYfQo5prSRyedA72wiD3vFGXLWHyy0KOXp+uwvkNzaBpvU2DDKNTTPdo1pNWtl&#x2F;LkpRCVq+uRG+LhZBuic0GpDJnD7HckUbwsyGktb&#x2F;6g5ogScNtPWB+pegENFDl8BuFn3zDiDnnGxbpj3emSxDlskzwIDAQAB 等待10分钟后测试是否设置正确。 显示pass则正确。 问题解决 1.ewomail管理后台，账户登录出现失败次数超过五次… 官方邮件给出的解决办法去服务器执行这个命令就可以清除了 自带的安全机制会禁止当前ip登录，所以这里提供一个办法，修改掉默认的用户名称 查看mysql默认的密码 登录mysql 修改用户名 同样适用于忘记密码后修改密码"},{"title":"兼职画师的一天","date":"2019-12-26T16:00:00.000Z","url":"/2019/12/27/2019/12/271633/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"起上班的过程中，突然接到了一个电话。专利公司那边打来的，他们帮忙申请专利，需要一个CAD图纸，我一想，没有啊，但是也不能直接说，就等一下给他呗。于是，赶紧的开始了制作过程。首先，它是这样子的。 然后根据这个，我画了这个示意图 画得不好，让大家见笑了，主要是给我的朋友看明白，然后用cad重新做图，也许是太过抽象了吧，朋友做的图，enmmm… 最后，自己动手 把装置拆卸下来了，然后拍了照片，才画好。 涉及一些技术细节，需要保密，就不放上来了。"},{"title":"一九年十二月记事","date":"2019-12-15T16:00:00.000Z","url":"/2019/12/16/2019/12/160928/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"2019-12-16 晴 周一最后的一个月，总想着记住些什么，殊不知，最难抓住的，便是时光。 最近开始准备学习一下shell脚本。网上找了一本书学习中。 这个月的计划，因为工作的原因，可能还需要学习一下视频剪辑，不想为生活琐事所烦，但是却是有很多的事情。我无法控制，亦不能去改变什么。 2019-12-21没遇到这么不守信的人，做好了应用交付，但是迟迟却不结算钱，几个意思啊。 2019-12-23距离双蛋越来越近了。。。没什么喜悦，每年的这个时候，都免不了一场感怀。昨天冬至，以后就会天长夜短了。 2019-12-31最后一天"},{"title":"页面内容朗读","date":"2019-12-05T16:00:00.000Z","url":"/2019/12/06/2019/12/061011/","tags":[["WEB","/tags/WEB/"]],"categories":[["work","/categories/work/"]],"content":"代码"},{"title":"Java虚拟机运行时数据区","date":"2019-12-02T16:00:00.000Z","url":"/2019/12/03/2019/12/031348/","tags":[["JVM","/tags/JVM/"]],"categories":[["Java","/categories/Java/"]],"content":" 运行时数据区域Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。根据《Java虚拟机规范（Java SE 7版）》的规定，Java虚拟机所管理的内存将会包括以下几个运行时数据区域。 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java 虚拟机桟与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame[1]）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 经常有人把Java内存区分为堆内存（Heap）和栈内存（Stack），这种分法比较粗糙，Java内存区域的划分实际上远比这复杂。这种划分方式的流行只能说明大多数程序员最关注的、与对象内存分配关系最密切的内存区域是这两块。其中所指的“堆”笔者在后面会专门讲述，而所指的“栈”就是现在讲的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。在虚拟机规范中对本地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 Java堆对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有的对象实例以及数组都要在堆上分配[1]，但是随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换[2]优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么“绝对”了。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”（Garbage Collected Heap，幸好国内没翻译成“垃圾堆”）。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer,TLAB）。不过无论如何划分，都与存放内容无关，无论哪个区域，存储的都仍然是对象实例，进一步划分的目的是为了更好地回收内存，或者更快地分配内存。在本章中，我们仅仅针对内存区域的作用进行讨论，Java堆中的上述各个区域的分配、回收等细节将是第3章的主题。 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 方法区方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发、部署程序的开发者来说，很多人都更愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已，这样HotSpot的垃圾收集器可以像管理Java堆一样管理这部分内存，能够省去专门为方法区编写内存管理代码的工作。对于其他虚拟机（如BEA JRockit、IBM J9等）来说是不存在永久代的概念的。原则上，如何实现方法区属于虚拟机实现细节，不受虚拟机规范约束，但使用永久代来实现方法区，现在看来并不是一个好主意，因为这样更容易遇到内存溢出问题（永久代有-XX:MaxPermSize的上限，J9和JRockit只要没有触碰到进程可用内存的上限，例如32位系统中的4GB，就不会出现问题），而且有极少数方法（例如String.intern()）会因这个原因导致不同虚拟机下有不同的表现。因此，对于HotSpot虚拟机，根据官方发布的路线图信息，现在也有放弃永久代并逐步改为采用Native Memory来实现方法区的规划了[1]，在目前已经发布的JDK 1.7的HotSpot中，已经把原本放在永久代的字符串常量池移出。 Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说，这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是必要的。在Sun公司的BUG列表中，曾出现过的若干个严重的BUG就是由于低版本的HotSpot虚拟机对此区域未完全回收而导致内存泄漏。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 Java虚拟机对Class文件每一部分（自然也包括常量池）的格式都有严格规定，每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、装载和执行，但对于运行时常量池，Java虚拟机规范没有做任何细节的要求，不同的提供商实现的虚拟机可以按照自己的需要来实现这个内存区域。不过，一般来说，除了保存Class文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池中[1]。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现，所以我们放到这里一起讲解。 在JDK 1.4中新加入了NIO（New Input&#x2F;Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I&#x2F;O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 显然，本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存（包括RAM以及SWAP区或者分页文件）大小以及处理器寻址空间的限制。服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现OutOfMemoryError异常。 内容来自《深入理解Java虚拟机》第二版"},{"title":"elenmentUI的表格行颜色变换","date":"2019-12-01T16:00:00.000Z","url":"/2019/12/02/2019/12/020347/","tags":[["Vue","/tags/Vue/"],["elenmentUI","/tags/elenmentUI/"]],"categories":[["work","/categories/work/"]],"content":"cell-style "},{"title":"登高","date":"2019-11-25T16:00:00.000Z","url":"/2019/11/26/2019/11/261714/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"2019-12-04人总是会感叹时光的易逝，此时此刻却不懂得去珍惜。 终。 2019-12-12双十二，躲过了双十一，还是买了些东西。 今年都快结束了，没有别的想法，只好努力工作，安心赚钱。有些东西一直想买，心心念的，但是也咩有下手。还是不好说呀。可能天生不是个果断的人吧。 2019-12-13连续的记录，咱也不知道为啥，还是需要好好学习一下。 之前的小书屋网站挂了，于是又去找了一些，但是可用的书籍不是很多，很多也是重复的。搜艘捡捡，还是找到了一些，之前的看的也差不多了，有的书还是值得多看几遍的。 除了编程方面的书籍，还想找一些其他的。增加自己的知识面。"},{"title":"restful方式部署pytorch模型","date":"2019-11-24T16:00:00.000Z","url":"/2019/11/25/2019/11/251100/","tags":[["pytorch","/tags/pytorch/"]],"categories":[["Python","/categories/Python/"]],"content":"需求图片分类，模型训练已经完成，需要提供服务。 环境requirement.txt gunicorn.cong.py 加载模型 图片预处理 接口 参考： 文章搬运：Deploying PyTorch in Python via a REST API with Flask"},{"title":"Deploying PyTorch in Python via a REST API with Flask","date":"2019-11-24T16:00:00.000Z","url":"/2019/11/25/2019/11/251144/","tags":[["PyTorch","/tags/PyTorch/"],["Flask","/tags/Flask/"]],"categories":[["Python","/categories/Python/"]],"content":"原文地址: In this tutorial, we will deploy a PyTorch model using Flask and expose aREST API for model inference. In particular, we will deploy a pretrainedDenseNet 121 model which detects the image. .. tip:: All the code used here is released under MIT license and is available on Github &lt;. This represents the first in a series of tutorials on deploying PyTorch modelsin production. Using Flask in this way is by far the easiest way to startserving your PyTorch models, but it will not work for a use casewith high performance requirements. For that: API DefinitionWe will first define our API endpoints, the request and response types. OurAPI endpoint will be at /predict which takes HTTP POST requests with afile parameter which contains the image. The response will be of JSONresponse containing the prediction: :: DependenciesInstall the required dependenices by running the following command: :: Simple Web ServerFollowing is a simple webserver, taken from Flask’s documentaion Save the above snippet in a file called app.py and you can now run aFlask development server by typing: :: When you visit  in your web browser, you will begreeted with Hello World! text We will make slight changes to the above snippet, so that it suits our APIdefinition. First, we will rename the method to predict. We will updatethe endpoint path to /predict. Since the image files will be sent viaHTTP POST requests, we will update it so that it also accepts only POSTrequests: We will also change the response type, so that it returns a JSON responsecontaining ImageNet class id and name. The updated app.py file willbe now: Inference In the next sections we will focus on writing the inference code. This willinvolve two parts, one where we prepare the image so that it can be fedto DenseNet and next, we will write the code to get the actual predictionfrom the model. Preparing the imageDenseNet model requires the image to be of 3 channel RGB image of size224 x 224. We will also normalise the image tensor with the required meanand standard deviation values. You can read more about ithere &lt;. We will use transforms from torchvision library and build atransform pipeline, which transforms our images as required. Youcan read more about transforms here &lt;. The above method takes image data in bytes, applies the series of transformsand returns a tensor. To test the above method, read an image file inbytes mode (first replacing ../_static/img/sample_file.jpeg with the actualpath to the file on your computer) and see if you get a tensor back: Prediction Now will use a pretrained DenseNet 121 model to predict the image class. Wewill use one from torchvision library, load the model and get aninference. While we’ll be using a pretrained model in this example, you canuse this same approach for your own models. See more about loading yourmodels in this :doc:tutorial &lt;/beginner/saving_loading_models&gt;. The tensor y_hat will contain the index of the predicted class id.However, we need a human readable class name. For that we need a class idto name mapping. Downloadthis file &lt; imagenet_class_index.json and remember where you saved it (or, if youare following the exact steps in this tutorial, save it intutorials/_static). This file contains the mapping of ImageNet class id toImageNet class name. We will load this JSON file and get the class name ofthe predicted index. Before using imagenet_class_index dictionary, first we will converttensor value to a string value, since the keys in theimagenet_class_index dictionary are strings.We will test our above method: You should get a response like this: The first item in array is ImageNet class id and second item is the humanreadable name. .. Note :: Did you notice that model variable is not part of get_prediction method? Or why is model a global variable? Loading a model can be an expensive operation in terms of memory and compute. If we loaded the model in the get_prediction method, then it would get unnecessarily loaded every time the method is called. Since, we are building a web server, there could be thousands of requests per second, we should not waste time redundantly loading the model for every inference. So, we keep the model loaded in memory just once. In production systems, it’s necessary to be efficient about your use of compute to be able to serve requests at scale, so you should generally load your model before serving requests. Integrating the model in our API ServerIn this final part we will add our model to our Flask API server. Sinceour API server is supposed to take an image file, we will update our predictmethod to read files from the requests: The app.py file is now complete. Following is the full version; replacethe paths with the paths where you saved your files and it should run: Let’s test our web server! Run: :: We can use therequests &lt; to send a POST request to our app: .. code-block:: python import requests resp &#x3D; requests.post(““, files&#x3D;{“file”: open(‘&lt;PATH&#x2F;TO&#x2F;.jpg&#x2F;FILE&gt;&#x2F;cat.jpg’,’rb’)}) Printing resp.json() will now show the following: :: Next stepsThe server we wrote is quite trivial and and may not do everythingyou need for your production application. So, here are some things youcan do to make it better: The endpoint /predict assumes that always there will be a image filein the request. This may not hold true for all requests. Our user maysend image with a different parameter or send no images at all. The user may send non-image type files too. Since we are not handlingerrors, this will break our server. Adding an explicit error handingpath that will throw an exception would allow us to better handlethe bad inputs Even though the model can recognize a large number of classes of images,it may not be able to recognize all images. Enhance the implementationto handle cases when the model does not recognize anything in the image. We run the Flask server in the development mode, which is not suitable fordeploying in production. You can check out this tutorial &lt; deploying a Flask server in production. You can also add a UI by creating a page with a form which takes the image anddisplays the prediction. Check out the demo &lt; a similar project and its source code &lt;. In this tutorial, we only showed how to build a service that could return predictions fora single image at a time. We could modify our service to be able to return predictions formultiple images at once. In addition, the service-streamer &lt; automatically queues requests to your service and samples them into mini-batchesthat can be fed into your model. You can check out this tutorial &lt;. Finally, we encourage you to check out our other tutorials on deploying PyTorch modelslinked-to at the top of the page. "},{"title":"PyTorch迁移学习","date":"2019-11-24T16:00:00.000Z","url":"/2019/11/25/2019/11/251352/","tags":[["PyTorch","/tags/PyTorch/"]],"categories":[["Python","/categories/Python/"]],"content":" Transfer Learning for Computer Vision TutorialAuthor: Sasank Chilamkurthy &lt; In this tutorial, you will learn how to train a convolutional neural network forimage classification using transfer learning. You can read more about the transferlearning at cs231n notes &lt; Quoting these notes, These two major transfer learning scenarios look as follows: Finetuning the convnet: Instead of random initializaion, weinitialize the network with a pretrained network, like the one that istrained on imagenet 1000 dataset. Rest of the training looks asusual. ConvNet as fixed feature extractor: Here, we will freeze the weightsfor all of the network except that of the final fully connectedlayer. This last fully connected layer is replaced with a new onewith random weights and only this layer is trained. Load DataWe will use torchvision and torch.utils.data packages for loading thedata. The problem we’re going to solve today is to train a model to classifyants and bees. We have about 120 training images each for ants and bees.There are 75 validation images for each class. Usually, this is a verysmall dataset to generalize upon, if trained from scratch. Since weare using transfer learning, we should be able to generalize reasonablywell. This dataset is a very small subset of imagenet. .. Note :: Download the data from here &lt; and extract it to the current directory. Visualize a few images^^^^^^^^^^^^^^^^^^^^^^Let’s visualize a few training images so as to understand the dataaugmentations. Training the modelNow, let’s write a general function to train a model. Here, we willillustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object fromtorch.optim.lr_scheduler. Visualizing the model predictions^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Generic function to display predictions for a few images Finetuning the convnetLoad a pretrained model and reset final fully connected layer. Train and evaluate^^^^^^^^^^^^^^^^^^ It should take around 15-25 min on CPU. On GPU though, it takes less than aminute. ConvNet as fixed feature extractorHere, we need to freeze all the network except the final layer. We needto set requires_grad == False to freeze the parameters so that thegradients are not computed in backward(). You can read more about this in the documentationhere &lt;. Train and evaluate^^^^^^^^^^^^^^^^^^ On CPU this will take about half the time compared to previous scenario.This is expected as gradients don’t need to be computed for most of thenetwork. However, forward does need to be computed. "},{"title":"JQuery 定时任务请求","date":"2019-11-20T16:00:00.000Z","url":"/2019/11/21/2019/11/211054/","tags":[["js","/tags/js/"]],"categories":[["work","/categories/work/"]],"content":"需求定时检测是否有新的任务需要处理 实现"},{"title":"Future 模式","date":"2019-11-17T16:00:00.000Z","url":"/2019/11/18/2019/11/181827/","tags":[["future","/tags/future/"]],"categories":[["Java","/categories/Java/"]],"content":"FutureFuture模式是多线程开发中非常常见的一种设计模式，它的核心思想是异步调用。当我们需要调用一个函数方法时，如果这个函数执行很慢，那么我们就要进行等待。但有时候，我们可能并不急着要结果。因此，我们可以让被调者立即返回，让它在后台慢慢处理这个请求。对于调用者来说，则可以先处理一些其他任务，在真正需要数据的场合再去尝试获得需要的数据。 对于Future模式来说，虽然它无法立即给出你需要的数据。但是，它会返回给你一个契约，将来，你可以凭借着这个契约去重新获取你需要的信息。 RealData FutureMain 上述代码就是使用Future模式的典型。第10行，构造了FutureTask对象实例，表示这个任务是有返回值的。构造FutureTask时，使用Callable接口，告诉FutureTask我们需要的数据应该如何产生。接着再第15行，将FutureTask提交给线程池。显然，作为一个简单的任务提交，这里必然是立即返回的，因此程序不会阻塞。接下来，我们不用关心数据是如何产生的。可以去做一些额外的事情，然后在需要的时候可以通过Future.get()（第18行）得到实际的数据。 除了基本的功能外，JDK还为Future接口提供了一些简单的控制功能： "},{"title":"Docker的安装与配置","date":"2019-11-10T16:00:00.000Z","url":"/2019/11/11/2019/11/111452/","tags":[["manjaro","/tags/manjaro/"],["docker","/tags/docker/"],["linux","/tags/linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"安装 启动与配置将当前用户加入到docker用户组中 更新用户组 配置镜像加速"},{"title":"NextCloud 安装&升级","date":"2019-11-08T16:00:00.000Z","url":"/2019/11/09/2019/11/091018/","tags":[["docker","/tags/docker/"],["nextcloud","/tags/nextcloud/"]],"categories":[["work","/categories/work/"]],"content":"安装 修改config.php，设置信任域名,文件位置位于配置的数据卷中./data/cloud/config:/var/www/html/config 完整版配置参考 更新重新拉取镜像，执行docker-compose up -d 问题1： Updates between multiple major versions are unsupported 默认不可以跨版本升级，删除配置文件中的版本号 问题2： 维护模式 同样在配置文件中关闭配置即可 "},{"title":"Manjaro安装","date":"2019-11-07T16:00:00.000Z","url":"/2019/11/08/2019/11/082112/","tags":[["manjaro","/tags/manjaro/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 安装与配置下载地址manjaro 推荐一个软件rufus刻录镜像. 生成可用中国镜像站列表 选择速度快的源 刷新 修改pacman.conf使用方法：在 /etc/pacman.conf 文件末尾添加以下两行：sudo vim /etc/pacman.conf 社区仓库镜像 安装 archlinuxcn-keyring 包导入GPG key 使用yay安装软件不需要使用root 常见软件安装JetBrains软件推荐使用Toolbox安装下载地址 中文输入法搜狗 注意Xrong需要修改配置文件 ~/.xprofilesudo vim ~/.xprofile复制下面的内容保存 搜狗输入法异常！请删除.config&#x2F;SogouPY 并重启輸入以下命令 出現錯誤信息 安裝fcitx-qt4，解決上述問題 Gnome On Wayland 用户无法使用 fcitx由于 wayland 无法读取 ~/.xprofile 中的环境变量，所以请在/etc/environment中加入：帮助地址 AUR安装 建议手动安装官网版本 手动安装， 下载tar.gz包 下载地址tar解压 移动到 &#x2F;opt目录下 配置jdk环境变量 修改配置文件 /etc/profile JDK11环境变量配置为了确保一个特定的应用程序能够正常工作，它需要确切地知道如何定位 JVM。有两个主要的变量需要设置：JAVA_HOME 和 PATH。 启用配置 此时就已经配置完毕了。 bash 输入 java -version 系统更新 注意，如果这里出现无法同步软件包数据库，可能是DNS解析错误 请参考Domain name resolution修改DNS配置sudo vim /etc/resolv.conf 刷新DNS oh-my-zsh 系统已经安装了zsh 首先修改默认shell为zsh 安装curl wget gitee安装 ArchLinux-Wiki"},{"title":"Flask 部署 TensorFlow 模型","date":"2019-11-03T16:00:00.000Z","url":"/2019/11/04/2019/11/041948/","tags":[["flask","/tags/flask/"],["keras","/tags/keras/"],["tensorflow","/tags/tensorflow/"]],"categories":[["Python","/categories/Python/"]],"content":"模型转换 模型读取 WEB 服务端 client 测试"},{"title":"MySQL","date":"2019-10-31T16:00:00.000Z","url":"/2019/11/01/2019/11/011844/","tags":[["mysql","/tags/mysql/"]],"categories":[["work","/categories/work/"]],"content":"索引基础在MySQL中，存储引擎用类似的方法使用索引，其先在索引中找到对应值，然后根据匹配的索引记录找到对应的数据行。 索引的类型索引有很多种类型，可以为不同的场景提供更好的性能。在MySQL中，索引是在存储引擎层而不是服务器层实现的。所以，并没有统一的索引标准：不同存储引擎的索引的工作方式并不一样，也不是所有的存储引擎都支持所有类型的索引。即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。 B+Tree1.数据结构B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。 在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 2.操作进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。 插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。 3.与红黑树的比较 红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。 （一）B+ 树有更低的树高 平衡树的树高 O(h)&#x3D;O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。 （二）磁盘访问原理 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I&#x2F;O 就能完全载入一个节点。 如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。 （三）磁盘预读特性 为了减少磁盘 I&#x2F;O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。 补充： 局部性原理，相似的数据会有聚集成群的倾向，磁盘预读4K的整数倍，innodb页大小默认是16kb数据，尽可能多的减少要读取的数据量，减少IO次数，减少IO的量。 MySQL 索引B+Tree 索引是大多数 MySQL 存储引擎的默认索引类型。 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。 InnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。 哈希索引哈希冲突，散列不均匀，过多会成为链表 不支持范围查询 需要大量内存空间 哈希索引（hash index）基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 在MySQL中，只有Memory引擎显式支持哈希索引。这也是Memory引擎表的默认索引类型，Memory引擎同时也支持B-Tree索引。值得一提的是，Memory引擎是支持非唯一哈希索引的，这在数据库世界里面是比较与众不同的。如果多个列的哈希值相同，索引会以链表的方式存放多个记录指针到同一个哈希条目中。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 空间数据索引（R-Tree）MyISAM表支持空间索引，可以用作地理数据存储。和B-Tree索引不同，这类索引无须前缀查询。空间索引会从所有维度来索引数据。查询时，可以有效地使用任意维度来组合查询。必须使用MySQL的GIS相关函数如MBRCONTAINS()等来维护数据。MySQL的GIS支持并不完善，所以大部分人都不会使用这个特性。开源关系数据库系统中对GIS的解决方案做得比较好的是PostgreSQL的PostGIS。 全文索引全文索引是一种特殊类型的索引，它查找的是文本中的关键词，而不是直接比较索引中的值。全文搜索和其他几类索引的匹配方式完全不一样。它有许多需要注意的细节，如停用词、词干和复数、布尔搜索等。全文索引更类似于搜索引擎做的事情，而不是简单的WHERE条件匹配。 在相同的列上同时创建全文索引和基于值的B-Tree索引不会有冲突，全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。 其他索引类别还有很多第三方的存储引擎使用不同类型的数据结构来存储索引。例如TokuDB使用分形树索引（fractal tree index），这是一类较新开发的数据结构，既有B-Tree的很多优点，也避免了B-Tree的一些缺点。如果通读完本章，可以看到很多关于InnoDB的主题，包括聚簇索引、覆盖索引等。多数情况下，针对InnoDB的讨论也都适用于TokuDB。 ScaleDB使用Patricia tries（这个词不是拼写错误），其他一些存储引擎技术如InfiniDB和Infobright则使用了一些特殊的数据结构来优化某些特殊的查询。 索引的优点索引可以让服务器快速地定位到表的指定位置。但是这并不是索引的唯一作用，到目前为止可以看到，根据创建索引的数据结构不同，索引也有一些其他的附加作用。 最常见的B-Tree索引，按照顺序存储数据，所以MySQL可以用来做ORDER BY和GROUP BY操作。因为数据是有序的，所以B-Tree也就会将相关的列值都存储在一起。最后，因为索引中存储了实际的列值，所以某些查询只使用索引就能够完成全部查询。据此特性，总结下来索引有如下三个优点： 索引大大减少了服务器需要扫描的数据量。 索引可以帮助服务器避免排序和临时表。 索引可以将随机I&#x2F;O变为顺序I&#x2F;O。 索引是最好的解决方案吗? 索引并不总是最好的工具。总的来说，只有当索引帮助存储引擎快速查找到记录带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，大部分情况下简单的全表扫描更高效。 对于中到大型的表，索引就非常有效。 但对于特大型的表，建立和使用索引的代价将随之增长。这种情况下，则需要一种技术可以直接区分出查询需要的一组数据，而不是一条记录一条记录地匹配。例如可以使用分区技术。 如果表的数量特别多，可以建立一个元数据信息表，用来查询需要用到的某些特性。例如执行那些需要聚合多个应用分布在多个表的数据的查询，则需要记录“哪个用户的信息存储在哪个表中”的元数据，这样在查询时就可以直接忽略那些不包含指定用户信息的表。对于大型系统，这是一个常用的技巧。事实上，Infobright就是使用类似的实现。对于TB级别的数据，定位单条记录的意义不大，所以经常会使用块级别元数据技术来替代索引。 索引优化1.独立的列 在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。 2.多列索引(联合索引) 在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。 例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。 3.索引列的顺序 最左匹配原则让选择性最强的索引列放在前面。 索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。 组合索引，MySQL优化器会调整执行顺序。 显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。 4.前缀索引对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。 前缀长度的选取需要根据索引选择性来确定。 5.覆盖索引 索引包含所有需要查询的字段的值。 具有以下优点： 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。否则的话需要回表查询。 查询性能优化EXPLAIN id select_type table type possible_keys key key_len ref rows Extra 1 SIMPLE db_1 index PRIMARY 1022 996726 Using index 比较重要的字段 select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 优化数据访问1.减少请求的数据量 只返回必要的列：最好不要使用 SELECT * 语句。 只返回必要的行：使用 LIMIT 语句来限制返回的数据。 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。 2.减少服务器端扫描的行数 最有效的方式是使用索引来覆盖查询。 重构查询方式1.切分大查询一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 2.分解大连接查询将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。减少锁竞争；在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 存储引擎InnoDB是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。 实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。 MyISAM设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 比较 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。 外键：InnoDB 支持外键。 MyISAM非聚簇索引，InnoDB聚簇索引 备份：InnoDB 支持在线热备份。 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 其它特性：MyISAM 支持压缩表和空间数据索引。 参考"},{"title":"一个处理excel文件的工具","date":"2019-10-30T16:00:00.000Z","url":"/2019/10/31/2019/10/311155/","tags":[["pandas","/tags/pandas/"],["tkinter","/tags/tkinter/"]],"categories":[["Python","/categories/Python/"]],"content":"需求分析上传excel文件,读取获得时间,比较时间获取单据是否超时.将判断结果回写到excel中 使用Tkinter作为GUI,pandas读取写入excel,datetime比较时间 环境 代码实现 注意,执行上面代码可能会缺少一些模块,根据提示安装即可,是处理excel的相关模块 打包使用pyinstaller打包成exe文件,需要在Windows环境下打包. 安装 打包 参见pyinstaller的命令相关说明 -F, –onefile 打包一个单个文件，如果你的代码都写在一个.py文件的话，可以用这个，如果是多个.py文件就别用 -D, –onedir 打包多个文件，在dist中生成很多依赖文件，适合以框架形式编写工具代码，我个人比较推荐这样，代码易于维护 -K, –tk 在部署时包含 TCL&#x2F;TK -a, –ascii 不包含编码 . 在支持 Unicode的 python 版本上默认包含所有的编码 . -d, –debug 产生 debug 版本的可执行文件 -w,–windowed,–noconsole 使用 Windows 子系统执行 . 当程序启动的时候不会打开命令行 ( 只对 Windows 有效 ) -c,–nowindowed,–console 使用控制台子系统执行 ( 默认 )( 只对 Windows 有效 )pyinstaller -c xxxx.pypyinstaller xxxx.py –console -s,–strip 可执行文件和共享库将 run through strip. 注意 Cygwin的 strip 往往使普通的 win32 Dll 无法使用 . -X, –upx 如果有 UPX 安装 ( 执行 Configure.py 时检测 ), 会压缩执行文件 (Windows 系统中的 DLL 也会 )( 参见 note) -o DIR, –out&#x3D;DIR 指定 spec 文件的生成目录 , 如果没有指定 , 而且当前目录是 PyInstaller 的根目录 , 会自动创建一个用于输出 (spec 和生成的可执行文件 ) 的目录 . 如果没有指定 , 而当前目录不是 PyInstaller 的根目录 , 则会输出到当前的目录下 . -p DIR, –path&#x3D;DIR 设置导入路径 ( 和使用 PYTHONPATH 效果相似 ). 可以用路径分割符 (Windows 使用分号 ,Linux 使用冒号 ) 分割 , 指定多个目录 . 也可以使用多个 -p 参数来设置多个导入路径，让pyinstaller自己去找程序需要的资源 –icon&#x3D;&lt;FILE.ICO&gt; 将 file.ico 添加为可执行文件的资源 ( 只对 Windows 系统有效 )，改变程序的图标 pyinstaller - i ico路径 xxxxx.py –icon&#x3D;&lt;FILE.EXE,N&gt; 将 file.exe 的第 n 个图标添加为可执行文件的资源 ( 只对 Windows 系统有效 ) -v FILE, –version&#x3D;FILE 将 verfile 作为可执行文件的版本资源 ( 只对 Windows 系统有效 ) -n NAME, –name&#x3D;NAME 可选的项目 ( 产生的 spec 的 ) 名字 . 如果省略 , 第一个脚本的主文件名将作为 spec 的名字  "},{"title":"改变swagger请求的url","date":"2019-10-24T16:00:00.000Z","url":"/2019/10/25/2019/10/251426/","tags":[["swagger","/tags/swagger/"]],"categories":[["Java","/categories/Java/"]],"content":"描述 解决方案"},{"title":"SSL&nginx配置","date":"2019-10-24T16:00:00.000Z","url":"/2019/10/25/2019/10/251858/","tags":[["nginx","/tags/nginx/"],["SSL","/tags/SSL/"]],"categories":[["work","/categories/work/"]],"content":"证书生成第三方购买或免费申请 解压后包含文件 操作步骤 上传上述文件到服务器中（使用“FileZilla”连接服务器并上传文件），路径与nginx.conf保持同一目录。 通过ssh方式登录到服务器。 编辑配置文件/etc/nginx/nginx.conf 修改内容如下 重启 Nginx，即可使用  进行访问。 生成证书要生成虚拟证书，您可以执行以下步骤： 将新证书配置到nginx.conf中： 重新启动Nginx。 现在，所有人都可以使用以下方法进行访问： https：&#x2F;&#x2F; YOUR_DOMAINNAME_HERE HTTP 自动跳转 HTTPS 的安全配置 指令ssl语法： ssl [on | off] 默认值： ssl off 上下文： 主服务器 为服务器启用HTTPS。 ssl_certificate语法： ssl_certificate文件 默认值： ssl_certificate cert.pem 上下文： 主服务器 指示此虚拟服务器带有PEM格式的证书的文件。同一文件可以包含其他证书，也可以包含PEM格式的密钥。从0.6.7版开始，文件路径是相对于nginx配置文件nginx.conf的目录，而不是相对于nginx前缀目录。 ssl_certificate_key语法： ssl_certificate_key文件 默认值： ssl_certificate_key cert.pem 上下文： 主服务器 指示此虚拟服务器具有PEM格式的密钥的文件。从0.6.7版开始，文件名路径是相对于nginx配置文件nginx.conf的目录，而不是相对于nginx前缀目录。 ssl_client_certificate语法： ssl_client_certificate文件 默认值： 无 上下文： 主服务器 表示带有PEM格式证书CA的文件，用于检查客户端证书。 ssl_dhparam语法： ssl_dhparam文件 默认值： 无 上下文： 主服务器 指示具有PEM格式的Diffie-Hellman参数的文件，用于协商TLS会话密钥。 ssl_ciphers语法： ssl_ciphers文件 默认值： ssl_ciphers ALL：！ADH：RC4 + RSA：+ HIGH：+ MEDIUM：+ LOW：+ SSLv2：+ EXP 上下文： 主服务器 指令描述了允许的密码。密码以OpenSSL支持的格式分配，例如： ssl_ciphers ALL：！ADH：！EXPORT56：RC4 + RSA：+ HIGH：+ MEDIUM：+ LOW：+ SSLv2：+ EXP; 可以使用以下命令查看完整列表： openssl密码 ssl_crl语法： ssl_crl文件 默认值： 无 上下文： http，服务器 该指令（0.8.7+）在PEM中指定一个带有吊销证书（CRL）的文件，该文件用于检查客户端证书。 ssl_prefer_server_ciphers语法： ssl_prefer_server_ciphers [on | off] 默认值： ssl_prefer_server_ciphers关闭 上下文： 主服务器 要求协议SSLv3和TLSv1服务器密码优先于客户端密码。 ssl_protocols语法： ssl_protocols [SSLv2] [SSLv3] [TLSv1] 默认值： ssl_protocols SSLv2 SSLv3 TLSv1 上下文： 主服务器 指令启用指示的协议。 ssl_verify_client语法： ssl_verify_client on | off | ask 默认值： ssl_verify_client关闭 上下文： 主服务器 指令启用验证客户端证书。参数“询问”检查是否提供了客户端证书。 ssl_verify_depth语法： ssl_verify_depth数字 默认值： ssl_verify_depth 1 上下文： 主服务器 在客户端证书链中设置深度检查。 ssl_session_cache语法： ssl_session_cache off | none | builtin：size和&#x2F;或shared：name：size 默认值： ssl_session_cache关闭 上下文： 主服务器 该指令设置用于存储SSL会话的缓存的类型和大小。缓存类型为： 关闭-困难重重：nginx对客户端明确表示无法重用会话。 无-正常运行：nginx对客户端说可以重新开始会话，但是nginx实际上从不重用它们。对于某些邮件客户端，这是解决方法，因为ssl_session_cache可以在邮件代理以及HTTP服务器中使用。 内置-OpenSSL内置缓存仅在一个工作进程中使用。缓存大小在会话数中分配。注意：使用此方法似乎存在内存碎片问题，请在使用此方法时加以考虑。请参阅下面的“参考”。 共享-缓存在所有工作进程之间共享。缓存的大小以字节为单位分配，1 MB的缓存可以包含大约4000个会话。每个共享缓存必须具有任意名称。具有相同名称的缓存可以在多个虚拟服务器中使用。 可以同时使用两种类型的缓存，例如： ssl_session_cache内置：1000共享：SSL：10m; 但是，只有内置的共享缓存使用情况才更有效。 ssl_session_timeout语法： ssl_session_timeout时间 默认值： ssl_session_timeout 5m 上下文： 主服务器 指定客户端可以重复使用会话参数的时间，该参数存储在缓存中。 该模块支持几种非标准错误代码，这些错误代码可在伪指令error_page的帮助下用于调试： 495-错误检查客户端证书 496-客户未授予所需的证书 497-正常请求已发送到HTTPS 完全取消请求后即可进行调试，并可以通过$ request_uri，$ uri，$ arg等变量进行访问。内置变量模块ngx_http_ssl_module支持多个内置变量： $ ssl_cipher返回那些用于建立SSL连接的密码的行 $ ssl_client_serial返回已建立的SSL连接的客户端证书的序列号 $ ssl_client_s_dn为建立的SSL连接返回客户端证书的行主题DN $ ssl_client_i_dn为建立的SSL连接返回客户端证书的行发行者DN $ ssl_protocol返回已建立的SSL连接的协议 ssl_engine语法： ssl_engine 这允许指定要使用的OpenSSL引擎，例如Padlock。它需要更新版本的OpenSSL。"},{"title":"nginx","date":"2019-10-23T16:00:00.000Z","url":"/2019/10/24/2019/10/241138/","tags":[["web","/tags/web/"],["nginx","/tags/nginx/"]],"categories":[["work","/categories/work/"]],"content":"nginx优化简介在大多数情况下，一个常规安装的Nginx对网站来说已经能很好地工作了。然而如果想挤压出Nginx的性能，就需要了解哪些指令会影响Nginx性能，在本文中将解释Nginx的哪些设置可以微调。需要注意一点，这是一个简单的预览——那些可以通过微调来提高性能设置的概述，不同的环境可能情况不太一样。 对于Nginx的调优，可以大致从如下指令着手： 1. worker_processesworker_processes表示工作进程的数量，一般情况设置成CPU核的数量即可，一个cpu配置多于一个worker数，对Nginx而言没有任何益处，另外不要忘了设置worker_cpu_affinity，这个配置用于将worker process与指定cpu核绑定，降低由于多CPU核切换造成的寄存器等现场重建带来的性能损耗。 grep processor &#x2F;proc&#x2F;cpuinfo | wc -l这个命令会告诉你当前机器是多少核，输出为2即表示2核。 2. worker_connectionsworker_connections配置表示每个工作进程的并发连接数，默认设置为1024。 可以更新如下配置文件来修改该值： sudo vim &#x2F;etc&#x2F;nginx&#x2F;nginx.conf 3. BuffersBuffers：另一个很重要的参数为buffer，如果buffer太小，Nginx会不停的写一些临时文件，这样会导致磁盘不停的去读写，现在我们先了解设置buffer的一些相关参数： client_body_buffer_size:允许客户端请求的最大单个文件字节数 client_header_buffer_size:用于设置客户端请求的Header头缓冲区大小，大部分情况1KB大小足够 client_max_body_size:设置客户端能够上传的文件大小，默认为1m large_client_header_buffers:该指令用于设置客户端请求的Header头缓冲区大小 具体可参考配置如下： 4. Timeoutsclient_header_timeout和client_body_timeout设置请求头和请求体(各自)的超时时间，如果没有发送请求头和请求体，Nginx服务器会返回408错误或者request time out。 keepalive_timeout给客户端分配keep-alive链接超时时间。服务器将在这个超时时间过后关闭链接，我们将它设置低些可以让Nginx持续工作的时间更长。 send_timeout 指定客户端的响应超时时间。这个设置不会用于整个转发器，而是在两次客户端读取操作之间。如果在这段时间内，客户端没有读取任何数据，Nginx就会关闭连接。 具体可参考配置如下： 5. Gzip Compression开启Gzip，gzip可以帮助Nginx减少大量的网络传输工作，另外要注意gzip_comp_level的设置，太高的话，Nginx服务会浪费CPU的执行周期。 具体可参考配置如下： 6. Static File Caching 以上的文件类型可以根据Nginx服务器匹配增加或减少。 7. loggingaccess_log设置Nginx是否将存储访问日志。关闭这个选项可以让读取磁盘IO操作更快。 可以修改配置文件将该功能关闭： 然后重启Nginx服务： Ngnix使用hash表来协助完成请求的快速处理考虑到保存键及其值的hash表存储单元的大小不至于超出设定参数(hash bucket size)， 在启动和每次重新配置时，Nginx为hash表选择尽可能小的尺寸。 直到hash表超过参数(hash max size)的大小才重新进行选择. 对于大多数hash表都有指令来修改这些参数。例如，保存服务器名字的hash表是由指令 和 所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键值。因此，如果Nginx给出需要增大 hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. 使用systemctl管理Nginx服务您可以像任何其他systemd单位一样管理Nginx服务。 要停止Nginx服务，请运行： 要再次启动，请键入： 重新启动Nginx服务： 在进行一些配置更改后重新加载Nginx服务： 如果你想禁用Nginx服务在启动时启动： 并重新启用它： 配置文件详解 nginx 设置最大连接数 资料扩展nginx中文文档："},{"title":"Java8新的日期和时间API","date":"2019-10-21T16:00:00.000Z","url":"/2019/10/22/2019/10/221148/","tags":[["Time","/tags/Time/"]],"categories":[["Java","/categories/Java/"]],"content":"LocalDate、LocalTime、Instant、Duration以及Periodjava.time包中的是类是不可变且线程安全的。新的时间及日期API位于java.time中，下面是一些关键类 Instant——它代表的是时间戳 LocalDate——不包含具体时间的日期，比如2014-01-14。它可以用来存储生日，周年纪念日，入职日期等。 LocalTime——它代表的是不含日期的时间 LocalDateTime——它包含了日期及时间，不过还是没有偏移信息或者说时区。 ZonedDateTime——这是一个包含时区的完整的日期时间，偏移量是以UTC&#x2F;格林威治时间为基准的。 java8是如何处理时间及日期的 创建一个LocalDate对象并读取其值 使用TemporalField读取LocalDate的值 TemporalField是一个接口，它定义了如何访问temporal对象某个字段的值。ChronoField枚举实现了这一接口，所以你可以很方便地使用get方法得到枚举元素的值，如下所示。 创建LocalTime并读取其值 LocalDate和LocalTime都可以通过解析代表它们的字符串创建。使用静态方法parse，你可以实现这一目的： 你可以向parse方法传递一个DateTimeFormatter。该类的实例定义了如何格式化一个日期或者时间对象。 合并日期和时间LocalDateTime java.time.InstantInstant的设计初衷是为了便于机器使用。它包含的是由秒及纳秒所构成的数字。它包含的是由秒及纳秒所构成的数字。所以，它无法处理那些我们非常容易理解的时间单位。 表示时间点的日期-时间类的通用方法 方法名 是否是静态方法 描述 from 是 依据传入的Temporal对象创建对象实例 now 是 依据系统时钟创建Temporal对象 of 是 由Temporal对象的某个部分创建该对象的实例 parse 是 由字符串创建Temporal对象的实例 atOffset 否 将Temporal对象和某个时区偏移相结合 atZone 否 将Temporal对象和某个时区相结合 format 否 使用某个指定的格式器将Temporal对象转换为字符串（Instant类不提供该方法） get 否 读取Temporal对象的某一部分的值 minus 否 创建Temporal对象的一个副本，通过将当前Temporal对象的值减去一定的时长创建该副本 plus 否 创建Temporal对象的一个副本，通过将当前Temporal对象的值加上一定的时长创建该副本 with 否 以该Temporal对象为模板，对某些状态进行修改创建该对象的副本 TemporalAdjuster TemporalAdjuster类中的工厂方法 方法名 描述 dayOfWeekInMonth 创建一个新的日期，它的值为同一个月中每一周的第几天 firstDayOfMonth 创建一个新的日期，它的值为当月的第一天 firstDayOfNextMonth 创建一个新的日期，它的值为下月的第一天 firstDayOfNextYear 创建一个新的日期，它的值为明年的第一天 firstDayOfYear 创建一个新的日期，它的值为当年的第一天 firstInMonth 创建一个新的日期，它的值为同一个月中，第一个符合星期几要求的值 lastDayOfMonth 创建一个新的日期，它的值为下月的最后一天 lastDayOfNextMonth 创建一个新的日期，它的值为下月的最后一天 lastDayOfNextYear 创建一个新的日期，它的值为明年的最后一天 lastDayOfYear 创建一个新的日期，它的值为今年的最后一天 lastInMonth 创建一个新的日期，它的值为同一个月中，最后一个符合星期几要求的值 next/previous 创建一个新的日期，并将其值设定为日期调整后或者调整前，第一个符合指定星期几要求的日期 nextOrSame/previousOrSame 创建一个新的日期，并将其值设定为日期调整后或者调整前，第一个符合指定星期几要求的日期，如果该日期已经符合要求，直接返回该对象 实现一个定制的TemporalAdjuster 使用Lambda表达式定义TemporalAdjuster对象，推荐使用TemporalAdjusters类的静态工厂方法ofDateAdjuster，它接受一个UnaryOperator&lt;LocalDate&gt;类型的参数，代码如下： 打印输出及解析日期-时间对象DateTimeFormatter 和老的java.util.DateFormat相比较，所有的DateTimeFormatter实例都是线程安全的。所以，你能够以单例模式创建格式器实例，就像DateTimeFormatter所定义的那些常量，并能在多个线程间共享这些实例。DateTimeFormatter类还支持一个静态工厂方法，它可以按照某个特定的模式创建格式器，代码清单如下。 处理不同的时区和历法 小结 Java 8之前老版的java.util.Date类以及其他用于建模日期时间的类有很多不一致及设计上的缺陷，包括易变性以及糟糕的偏移值、默认值和命名。 新版的日期和时间API中，日期-时间对象是不可变的。 新的API提供了两种不同的时间表示方式，有效地区分了运行时人和机器的不同需求。 你可以用绝对或者相对的方式操纵日期和时间，操作的结果总是返回一个新的实例，老的日期时间对象不会发生变化。 TemporalAdjuster让你能够用更精细的方式操纵日期，不再局限于一次只能改变它的一个值，并且你还可按照需求定义自己的日期转换器。 你现在可以按照特定的格式需求，定义自己的格式器，打印输出或者解析日期-时间对象。这些格式器可以通过模板创建，也可以自己编程创建，并且它们都是线程安全的。 你可以用相对于某个地区&#x2F;位置的方式，或者以与UTC&#x2F;格林尼治时间的绝对偏差的方式表示时区，并将其应用到日期-时间对象上，对其进行本地化。 你现在可以使用不同于ISO-8601标准系统的其他日历系统了。分别是ThaiBuddhistDate、MinguoDate、JapaneseDate以及HijrahDate。 "},{"title":"一个很巧合的日子","date":"2019-10-20T16:00:00.000Z","url":"/2019/10/21/2019/10/211437/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"Why如果是农历的10月21那就是我的生日了。距离我表妹的生日也就差了九天，好想还停留在高中的那段时光，听到这个消息的时候。 继续努力。。 已经过去了一天，要写的东西却还停留在昨天的时光里。 最近的工作，有个实名审核的地方一直过不去，就提交了工单，到现在还没解决。工商的网站查询不到相关信息。 基本的编码工作都已经完成了，现在需要部署，然后进行接口对接，数据处理等。 2019年10月23日一周的时间过得还真是飞快，转眼间已经周三了，今天看了一下我的网易云账号，还是很久以前的状态，除了听歌记录会有所变化吧。 因为版权的问题，很多收藏的歌曲已经变成了灰色，无法继续播放了，不得已，手机端已经转移到QQ音乐了，财大气粗，版权足。但是有些功能方面真的不如网易云，在Linux也没有相关的客户端，只有网易云有，做的还不错，还有个云盘的功能。在uwp端也是遥遥领先的。 付费版权的大环境下，诚然QQ音乐的方式才是更好的。白嫖是没有未来的。 因为曲库的缺少，也就导致了最近网易云的推荐总是反反复复的那么些，实在是没有想听下去的欲望了。 目前主要使用的是linux系统，所以一时半会还是离不开的，也不习惯开着浏览器听音乐呀。 还有一个比较赞的就是输入法了，搜狗输入法。在linux上的使用也非常的方便。用了很久的原皮，才想起来去换个皮肤和字体，功能我是一直都知道的，但是一直也没有去更改。还是很好看的。 看到一个意识流笔记，感觉跟我现在写的东西好像啊，O(∩_∩)O哈哈哈~。 2019年10月24日没有玩游戏的想法，看游戏都觉得累了，之前还会看别人打游戏，也是一种娱乐。 一个简单的问题到现在没有解决，域名的实名制审核一直没有通过。导致现在也没办法去线上测试。 更新&amp;解决了一些BUG,很多事情觉得很简单，动手去做的时候，会发现有很多容易被忽略的地方。 关于域名，关于nginx服务器配置文件等等。不过由于docker的出现，一切变得简单了许多，也有些不同。 还有一个就是，今天是程序员节，节日快乐吧！ 2019年10月28日 终于还是快要到来了,生日这一天,说想要什么礼物吧,也没有特别的需求,希望越来越好吧. 因为亲戚家生孩子,老妈需要到郑州一趟,顺便看一下腿,一直有腿疼的问题,家里那边的医疗条件有限,希望可以找到原因,及时治疗,想给她买的护手霜也没办法寄回家了,等她回去的时候再买好了. 2019年11月6日非常苦闷的日子，说不清道不明，整夜的噩梦围绕，我这是怎么了. 不要被打败，现实十残酷，要相信自己可以做的更好。 2019-11-22不是忘了这里，而是不是很开心。 生活不易，猫猫叹气.jpg 新买了《古墓丽影-暗影》，打折让人心动啊，却是没有太多的时间。为什么要玩游戏呢，主要还是生活中的不如意，在虚拟世界里寻找一些安慰吧，化身侠客，神魔，往来如风。当然，也不全是，为了快乐吧。羡慕那些能把心中所思所想付诸文字的人，轻轻流淌的文字，伴随着思绪蜿蜒。 周末愉快！ 2019-11-25做了个梦，是十一时候的事情了，带妈妈和奶奶来上海玩，哥哥和表哥也都在。发生的事情也会在梦中再次浮现，可能和最近两天玩古剑三有关系吧–寄灵族。"},{"title":"死锁&重入锁","date":"2019-10-11T16:00:00.000Z","url":"/2019/10/12/2019/10/121053/","tags":[["Lock","/tags/Lock/"],["ReentrantLock","/tags/ReentrantLock/"]],"categories":[["Java","/categories/Java/"]],"content":"通俗的说，死锁就是两个或者多个线程，相互占用对方需要的资源，而都不进行释放，导致彼此之间都相互等待对方释放资源，产生了无限制等待的现象。死锁一旦发生，如果没有外力介入，这种等待将永远存在，从而对程序产生严重的影响。如果想避免死锁，除了使用无锁的函数外，另外一种有效的做法是使用第三章节介绍的重入锁，通过重入锁的中断或者限时等待可以有效规避死锁带来的问题。 java.util.concurrent.locks.ReentrantLock 上述代码第7～12行，使用重入锁保护临界区资源i，确保多线程对i操作的安全性。从这段代码可以看到，与synchronized相比，重入锁有着显示的操作过程。开发人员必须手动指定何时加锁，何时释放锁。也正因为这样，重入锁对逻辑控制的灵活性要远远好于synchronized。但值得注意的是，在退出临界区时，必须记得释放锁（代码第11行），否则，其他线程就没有机会再访问临界区了。 中断响应 锁申请等待限时 公平锁 对上面ReentrantLock的几个重要方法整理如下。 lock()：获得锁，如果锁已经被占用，则等待。lockInterruptibly()：获得锁，但优先响应中断。tryLock()：尝试获得锁，如果成功，返回true，失败返回false。该方法不等待，立即返回。tryLock(long time, TimeUnit unit)：在给定时间内尝试获得锁。unlock()：释放锁。就重入锁的实现来看，它主要集中在Java层面。在重入锁的实现中，主要包含三个要素： 第一，是原子状态。原子状态使用CAS操作来存储当前锁的状态，判断锁是否已经被别的线程持有。 第二，是等待队列。所有没有请求到锁的线程，会进入等待队列进行等待。待有线程释放锁后，系统就能从等待队列中唤醒一个线程，继续工作。 第三，是阻塞原语park()和unpark()，用来挂起和恢复线程。没有得到锁的线程将会被挂起。"},{"title":"无锁","date":"2019-10-08T16:00:00.000Z","url":"/2019/10/09/2019/10/090921/","tags":[["Java","/tags/Java/"],["Thread","/tags/Thread/"],["Lock","/tags/Lock/"],["CAS","/tags/CAS/"]],"categories":[["Java","/categories/Java/"]],"content":"对于并发控制而言，锁是一种悲观的策略。它总是假设每一次的临界区操作会产生冲突，因此，必须对每次操作都小心翼翼。如果有多个线程同时需要访问临界区资源，就宁可牺牲性能让线程进行等待，所以说锁会阻塞线程执行。而无锁是一种乐观的策略，它会假设对资源的访问是没有冲突的。既然没有冲突，自然不需要等待，所以所有的线程都可以在不停顿的状态下持续执行。那遇到冲突怎么办呢？无锁的策略使用一种叫做比较交换的技术（CAS Compare And Swap）来鉴别线程冲突，一旦检测到冲突产生，就重试当前操作直到没有冲突为止。 比较交换（CAS）与锁相比，使用比较交换（下文简称CAS）会使程序看起来更加复杂一些。但由于其非阻塞性，它对死锁问题天生免疫，并且，线程间的相互影响也远远比基于锁的方式要小。更为重要的是，使用无锁的方式完全没有锁竞争带来的系统开销，也没有线程间频繁调度带来的开销，因此，它要比基于锁的方式拥有更优越的性能。 CAS算法的过程是这样：它包含三个参数CAS(V,E,N)。V表示要更新的变量，E表示预期值，N表示新值。仅当V值等于E值时，才会将V的值设为N，如果V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。最后，CAS返回当前V的真实值。CAS操作是抱着乐观的态度进行的，它总是认为自己可以成功完成操作。当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理，CAS操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。 简单地说，CAS需要你额外给出一个期望值，也就是你认为这个变量现在应该是什么样子的。如果变量不是你想象的那样，那说明它已经被别人修改过了。你就重新读取，再次尝试修改就好了。 在硬件层面，大部分的现代处理器都已经支持原子化的CAS指令。在JDK 5.0以后，虚拟机便可以使用这个指令来实现并发操作和并发数据结构，并且，这种操作在虚拟机中可以说是无处不在。 无锁的线程安全整数：AtomicInteger"},{"title":"ThreadLocal","date":"2019-10-07T16:00:00.000Z","url":"/2019/10/08/2019/10/081655/","tags":[["Java","/tags/Java/"],["ThreadLocal","/tags/ThreadLocal/"]],"categories":[["Java","/categories/Java/"]],"content":"ThreadLocal的简单使用 SimipleDateFormat.parse()方法并不是线程安全的。因此，在线程池中共享这个对象必然导致错误。 ThreadLocal的实现原理ThreadLocal的set()方法和get()方法 在set时，首先获得当前线程对象，然后通过getMap()拿到线程的ThreadLocalMap，并将值设入ThreadLocalMap中。而ThreadLocalMap可以理解为一个Map（虽然不是，但是你可以把它简单地理解成HashMap），但是它是定义在Thread内部的成员。注意下面的定义是从Thread类中摘出来的： 而设置到ThreadLocal中的数据，也正是写入了threadLocals这个Map。其中，key为ThreadLocal当前对象，value就是我们需要的值。而threadLocals本身就保存了当前自己所在线程的所有“局部变量”，也就是一个ThreadLocal变量的集合。 get()操作 首先，get()方法也是先取得当前线程的ThreadLocalMap对象。然后，通过将自己作为key取得内部的实际数据。 在了解了ThreadLocal的内部实现后，我们自然会引出一个问题。那就是这些变量是维护在Thread类内部的（ThreadLocalMap定义所在类），这也意味着只要线程不退出，对象的引用将一直存在。 当线程退出时，Thread类会进行一些清理工作，其中就包括清理ThreadLocalMap，注意下述代码的加粗部分： 因此，如果我们使用线程池，那就意味着当前线程未必会退出（比如固定大小的线程池，线程总是存在）。如果这样，将一些大大的对象设置到ThreadLocal中（它实际保存在线程持有的threadLocals Map内），可能会使系统出现内存泄露的可能（这里我的意思是：你设置了对象到ThreadLocal中，但是不清理它，在你使用几次后，这个对象也不再有用了，但是它却无法被回收）。 此时，如果你希望及时回收对象，最好使用ThreadLocal.remove()方法将这个变量移除。就像我们习惯性地关闭数据库连接一样。如果你确实不需要这个对象了，那么就应该告诉虚拟机，请把它回收掉，防止内存泄露。 另外一种有趣的情况是JDK也可能允许你像释放普通变量一样释放ThreadLocal。比如，我们有时候为了加速垃圾回收，会特意写出类似obj&#x3D;null之类的代码。如果这么做，obj所指向的对象就会更容易地被垃圾回收器发现，从而加速回收。 同理，如果对于ThreadLocal的变量，我们也手动将其设置为null，比如tl&#x3D;null。那么这个ThreadLocal对应的所有线程的局部变量都有可能被回收。 ThreadLocalMap的实现使用了弱引用。弱引用是比强引用弱得多的引用。Java虚拟机在垃圾回收时，如果发现弱引用，就会立即回收。ThreadLocalMap内部由一系列Entry构成，每一个Entry都是WeakReference＜ThreadLocal＞： 这里的参数k就是Map的key，v就是Map的value。其中k也就是ThreadLocal实例，作为弱引用使用（super(k)就是调用了WeakReference的构造函数）。因此，虽然这里使用ThreadLocal作为Map的key，但是实际上，它并不真的持有ThreadLocal的引用。而当ThreadLocal的外部强引用被回收时，ThreadLocalMap中的key就会变成null。当系统进行ThreadLocalMap清理时（比如将新的变量加入表中，就会自动进行一次清理，虽然JDK不一定会进行一次彻底的扫描，但显然在我们这个案例中，它奏效了），就会自然将这些垃圾数据回收. 性能为每一个线程分配一个独立的对象对系统性能也许是有帮助的。当然了，这也不一定，这完全取决于共享对象的内部逻辑。如果共享对象对于竞争的处理容易引起性能损失，我们还是应该考虑使用ThreadLocal为每个线程分配单独的对象。一个典型的案例就是在多线程下产生随机数。"},{"title":"国庆节","date":"2019-09-30T16:00:00.000Z","url":"/2019/10/01/2019/10/011010/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"国庆快乐祝伟大祖国母亲繁荣昌盛。"},{"title":"Linux下的权限","date":"2019-09-22T16:00:00.000Z","url":"/2019/09/23/2019/09/231831/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"文件权限Linux系统中的每个文件和目录都有访问许可权限，用它来确定谁可以通过何种方式对文件和目录进行访问和操作。 文件或目录的访问权限分为只读，只写和可执行三种。以文件为例，只读权限表示只允许读其内容，而禁止对其做任何的更改操作。可执行权限表示允许将该文件作为一个程序执行。文 件被创建时，文件所有者自动拥有对该文件的读、写和可执行权限，以便于对文件的阅读和修改。用户也可根据需要把访问权限设置为需要的任何组合。 有三种不同类型的用户可对文件或目录进行访问：文件所有者，同组用户、其他用户。所有者一般是文件的创建者。所有者可以允许同组用户有权访问文件，还可以将文件的访问权限赋予系统中的其他用户。在这种情况下，系统中每一位用户都能访问该用户拥有的文件或目录。 每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。当用ls -l命令显示文件或目录的详细信息时，最左边的一列为文件的访问权限。例如： 横线代表空许可。r代表只读，w代表写，x代表可执行。注意这里共有10个位置。第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是d，表示是一个目录。例如： – rw- r– r– 普通文件 文件主 组用户 其他用户 是文件sobsrc.tgz 的访问权限，表示sobsrc.tgz是一个普通文件；sobsrc.tgz的属主有读写权限；与sobsrc.tgz属主同组的用户只有读权限；其他用户也只有读权限。 确定了一个文件的访问权限后，用户可以利用Linux系统提供的chmod命令来重新设定不同的访问权限。也可以利用chown命令来更改某个文件或目录的所有者。利用chgrp命令来更改某个文件或目录的用户组。 下面分别对这些命令加以介绍。 chmod 命令chmod命令是非常重要的，用于改变文件或目录的访问权限。用户用它控制文件或目录的访问权限。 以主文件夹下的一个名为“cc”的文件夹为例。下面一步一步介绍如何修改权限： 打开终端。输入su 接下来会要你输入密码，输入你的root密码。 假设我的文件夹在主目录里，地址为/var/home/dengchao/cc。假设我要修改文件权限为777，则在终端输入 chmod 777 /var/home/userid/cc 文件夹的权限就变为了777。 如果是修改文件夹及子文件夹权限可以用 chmod -R 777 /var/home/userid/cc 具体的权限(例如777的含意等)在下面解释下： 777有3位，最高位7是设置文件所有者访问权限，第二位是设置群组访问权限，最低位是设置其他人访问权限。 其中每一位的权限用数字来表示。具体有这些权限： r(Read，读取，权限值为4)：对文件而言，具有读取文件内容的权限；对目录来说，具有浏览目 录的权限。 w(Write,写入，权限值为2)：对文件而言，具有新增、修改文件内容的权限；对目录来说，具有删除、移动目录内文件的权限。 x(eXecute，执行，权限值为1)：对文件而言，具有执行文件的权限；对目录了来说该用户具有进入目录的权 限。 首先我们来看如何确定单独一位上的权限数值，例如最高位表示文件所有者权限数值，当数字为7时，7用“rwx”表示 –{4(r)+2(w)+1(x)&#x3D;7}–又如果数值为6，则用“rw-”表示–{4(r)+2(w)+0(x)&#x3D;6}–，”-”表示不具备权限，这里表 示不具备“执行”权限。 假如我们设定其他用户的访问权限为 “r–”，则数值为4+0+0&#x3D;4 一开始许多初学者会被搞糊涂，其实很简单，我们将rwx看成二进制数，如果有则用1表示，没有则有0表示，那么rwx则可以表示成为：111 而二进制的111就是7。 我们再来看下怎么确定3个数位上的权限。假如我们要给一个文件设置权限，具体权限如下： 文件所有者有“读”、“写”、“执行”权限，群组用户有“读”权限，其他用户有“读”权限，则对应的字母表示为”rwx r– r–“，对应的数字为744 一般都是最高位表示文件所有者权限值，第二位表示群组用户权限，最低位表示其他用户权限。 下面来举些例子熟悉下。 权限 数值 rwx rw- r– 764 rw- r– r– 644 rw- rw- r– 664 该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 文字设定法 命令中各选项的含义为： 操作对象who可是下述字母中的任一个或者它们的组合： u 表示“用户（user）”，即文件或目录的所有者。 g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。 o 表示“其他（others）用户”。 a 表示“所有（all）用户”。它是系统默认值。 操作符号可以是： + 添加某个权限。 – 取消某个权限。 &#x3D; 赋予给定权限并取消其他所有权限（如果有的话）。 设置mode所表示的权限可用下述字母的任意组合： r 可读。 w 可写。 x 可执行。 X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加x 属性。 s 在文件执行时把进程的属主或组ID置为该文件的文件属主。方式“u＋s”设置文件的用户ID位，“g＋s”设置组ID位。 t 保存程序的文本到交换设备上。 u 与文件属主拥有一样的权限。 g 与和文件属主同组的用户拥有一样的权限。 o 与其他用户拥有一样的权限。 -c : 若该档案权限确实已经更改，才显示其更改动作 -f : 若该档案权限无法被更改也不要显示错误讯息 -v : 显示权限变更的详细资料 -R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更) –help : 显示辅助说明 –version : 显示版本 文件名：以空格分开的要改变权限的文件列表，支持通配符。在一个命令行中可给出多个权限方式，其间用逗号隔开。例如：chmod g+r，o+r example使同组和其他用户对文件example 有读权限。 例1： 即设定文件sort的属性为： 文件属主（u） 增加执行权限 与文件属主同组用户（g） 增加执行权限 其他用户（o） 增加执行权限 例2： 即设定文件text的属性为： 文件属主（u） 增加写权限 与文件属主同组用户（g） 增加写权限 其他用户（o） 删除执行权限 例3： 假设执行chmod后a.out的权限为（可以用ls – l a.out命令来看）： 并且这个执行文件要用到一个文本文件shiyan1.c，其文件存取权限为“–rw——-”，即该文件只有其属主具有读写权限。 当其他用户执行a.out这个程序时，他的身份因这个程序暂时变成inin（由于chmod命令中使用了s选项），所以他就能够读取shiyan1.c这个文件（虽然这个文件被设定为其他人不具备任何权限），这就是s的功能。 因此，在整个系统中特别是root本身，最好不要过多的设置这种类型的文件（除非必要）这样可以保障系统的安全，避免因为某些程序的bug而使系统遭到入侵。 例4： 以上这三个命令都是将文件mm.txt的执行权限删除，它设定的对象为所有使用者。 数字设定法我们必须首先了解用数字表示的属性的含义：0表示没有权限，1表示可执行权限，2表示可写权限，4表示可读权限，然后将其相加。所以数字属性的格式应为3个从0到7的八进制数，其顺序是（u）（g）（o）。 例如，如果想让某个文件的属主有“读&#x2F;写”二种权限，需要把4（可读）+2（可写）＝6（读&#x2F;写）。 数字设定法的一般形式为：chmod ［mode］ 文件名¼ 例1： 即设定文件mm.txt的属性为： 文件属主（u）inin 拥有读、写权限 与文件属主同组人用户（g） 拥有读权限 其他人（o） 拥有读权限 例2： 即设定wchtxt这个文件的属性为： 文件主本人（u）inin 可读&#x2F;可写&#x2F;可执行权 与文件主同组人（g） 可读&#x2F;可执行权 其他人（o） 没有任何权限 chgrp命令功能：改变文件或目录所属的组。 语法：chgrp［选项］ group filename¼ 参数： -c或–changes 效果类似”-v”参数，但仅回报更改的部分。 -f或–quiet或–silent 不显示错误信息。 -h或–no-dereference 只对符号连接的文件作修改，而不更动其他任何相关文件。 -R或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -v或–verbose 显示指令执行过程。 –help 在线帮助。 –reference&#x3D;&lt;参考文件或目录&gt; 把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同。 –version 显示版本信息。 该命令改变指定指定文件所属的用户组。其中group可以是用户组ID，也可以是&#x2F;etc&#x2F;group文件中用户组的组名。文件名是以空格分开的要改变属组的文件列表，支持通配符。如果用户不是该文件的属主或超级用户，则不能改变该文件的组。 该命令的各选项含义为： – R 递归式地改变指定目录及其下的所有子目录和文件的属组。 例1： 改变/opt/local /book/及其子目录下的所有文件的属组为book。 chown命令功能：更改某个文件或目录的属主和属组。这个命令也很常用。例如root用户把自己的一个文件拷贝给用户yusi，为了让用户yusi能够存取这个文件，root用户应该把这个文件的属主设为yusi，否则，用户yusi无法存取这个文件。 语法：chown［选项］ 用户或组 文件 说明：chown将指定文件的拥有者改为指定的用户或组。用户可以是用户名或用户ID。组可以是组名或组ID。文件是以空格分开的要改变权限的文件列表，支持通配符。 参数说明： user新的档案拥有者的使用者 ID group : 新的档案拥有者的使用者群体(group) -c : 若该档案拥有者确实已经更改，才显示其更改动作 -f : 若该档案拥有者无法被更改也不要显示错误讯息 -h : 只对于连结(link)进行变更，而非该 link 真正指向的档案 -v : 显示拥有者变更的详细资料 -R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更) –help : 显示辅助说明 –version : 显示版本 例1：把文件yusi123.com的所有者改为yusi。 例2：把目录&#x2F;demo及其下的所有文件和子目录的属主改成yusi，属组改成users。 例如：chown qq /home/qq (把home目录下的qq目录的拥有者改为qq用户) 例如：chown -R qq /home/qq (把home目录下的qq目录下的所有子文件的拥有者改为qq用户) 注：部分参考自百度百科。 原文地址：Linux命令:修改文件权限命令chmod、chgrp、chown详解"},{"title":"Bookmarks","date":"2019-09-22T16:00:00.000Z","url":"/2019/09/23/2019/09/231605/","tags":[["person","/tags/person/"]],"categories":[["congco","/categories/congco/"]],"content":" 导航 开发GitLab 应用程序MSDN, 我告诉你 Download EagleGet - Download Accelerator, Video Downloader Redis Chrome - 小众软件 MPC-BE - Browse Files at SourceForge.net 壁纸图片CGWallpapers.com Awesome Wallpapers - wallhaven.cc 小说 视频YY影院-最新电影,最新电视剧 阳光电影_免费电影_迅雷电影下载_电影天堂 哔哩哔哩 音乐listen1 工具腾讯文档 drawio - draw.io PDF转Word转换器在线 - 免费将PDF转换成Docx - 加加PDF 在线Cron表达式生成器 game【新提醒】【2017&#x2F;09&#x2F;10更新 支持Steam版本】仙剑4宽屏&amp;UI修正工具 v1.41 - 第66页 - 《仙剑奇侠传五前传》 - 3DMGAME论坛 - Powered by Discuz! otherNginx (简体中文) - ArchWiki congcoMcHigh Sierra - www.gnome-look.org pytorch主页 - PyTorch中文文档"},{"title":"理想中的工作场景","date":"2019-09-22T16:00:00.000Z","url":"/2019/09/23/2019/09/231928/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"理想中的工作台，应该就是和上面的图差不多吧。 接了一些其他的工作，为了目标而努力。 表妹的入伍通知书下来了，希望一切都好，选择了远方，风雨兼程。"},{"title":"Be Happy","date":"2019-09-10T16:00:00.000Z","url":"/2019/09/11/2019/09/112122/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"痛苦与折磨&emsp;&emsp;快乐其实也很简单，不知道为什么要让自己很痛苦。 中秋节快到了，家里并没有说喜欢吃的月饼，于是就买了上次的豌豆零食，外加了一些其他的小吃，收到了，很开心。快乐就是这么简单啊！ 今天看到一个小故事：古代有个大儒，教他孙子特别严格，他儿子就看不下去了，常常在他教育孙子的时候躲在一旁。又一次，外面下着雪，孙子不喜欢读书 跑到外面，就被罚跪在雪地里，他儿子看不下去了，但是也知道劝不了他爸爸呀，于是也跪在了雪地里，大儒看见了就很不解的问他儿子，怎么回事呀？他儿子说，我心疼儿子，可是你不心疼孙子，你惩罚我儿子，我就惩罚你儿子. 2019年9月13日中秋快乐！ 2019年9月17日新的一周，无心工作，只想为祖国母亲庆生。 2019年09月18日19:46:36缺少了继续学习下去的动力，沉迷享乐了嘛，也不是。 手上还是有一些事情要做的，而且，很多东西需要学习掌握，还有很多不会的东西。 对于大型项目的把握还是很欠缺的，就是经验不足的原因吧。 继续好好学习才行。 2019年10月09日09:28:00国庆假期结束，开始了新的工作。今年的法定节假日已经过完，唯一可以期待的就是生日了。 节日期间，本来的计划是到杭州，2号的票都已经买好，早上接到电话，太后和太皇太后都已经坐上了来上海的车了，于是赶紧退票，联系行宫。后来才想到，还不如在我这边租个房间来的实在，不多，一个月就行。 旅途还算愉快，看了没见过的事物，吃了一些没有吃过的东西，趁身体还算硬朗。第一天走了12000多步，我都有点震惊了。"},{"title":"InnoDB","date":"2019-09-09T16:00:00.000Z","url":"/2019/09/10/2019/09/101146/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":"InnoDB有一道MySQL的面试题，为什么MySQL的索引要使用B+树而不是其它树形结构?比如B树？ 简单回答是： 因为B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出） 指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； 实际上很多存储引擎使用的是B+Tree，即每一个叶子节点都包含指向下一个叶子节点的指针，从而方便叶子节点的范围遍历。 B-Tree索引存储引擎以不同的方式使用B-Tree索引，性能也各有不同，各有优劣。例如，MyISAM使用前缀压缩技术使得索引更小，但InnoDB则按照原数据格式进行存储。再如MyISAM索引通过数据的物理位置引用被索引的行，而InnoDB则根据主键引用被索引的行。 B-Tree通常意味着所有的值都是按顺序存储的，并且每一个叶子页到根的距离相同。 B-Tree索引能够加快访问数据的速度，因为存储引擎不再需要进行全表扫描来获取需要的数据，取而代之的是从索引的根节点（图示并未画出）开始进行搜索。根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找。通过比较节点页的值和要查找的值可以找到合适的指针进入下层子节点，这些指针实际上定义了子节点页中值的上限和下限。最终存储引擎要么是找到对应的值，要么该记录不存在。 叶子节点比较特别，它们的指针指向的是被索引的数据，而不是其他的节点页（不同引擎的“指针”类型不同）。其实在根节点和叶子节点之间可能有很多层节点页。树的深度和表的大小直接相关。 B-Tree对索引列是顺序组织存储的，所以很适合查找范围数据。例如，在一个基于文本域的索引树上，按字母顺序传递连续的值进行查找是非常合适的，所以像“找出所有以I到K开头的名字”这样的查找效率会非常高。"},{"title":"NIO","date":"2019-09-06T16:00:00.000Z","url":"/2019/09/07/2019/09/071101/","tags":[["Java","/tags/Java/"],["NIO","/tags/NIO/"]],"categories":[["Java","/categories/Java/"]],"content":"NIOJava NIO中涉及的基础内容有通道（Channel）和缓冲区（Buffer）、文件IO和网络IO。  NIO（Non-blocking I&#x2F;O，在Java领域，也称为New I&#x2F;O），是一种同步非阻塞的I&#x2F;O模型，也是I&#x2F;O多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I&#x2F;O处理问题的有效方式。 那么NIO的本质是什么样的呢？它是怎样与事件模型结合来解放线程、提高系统吞吐的呢？ 本文会从传统的阻塞I&#x2F;O和线程池模型面临的问题讲起，然后对比几种常见I&#x2F;O模型，一步步分析NIO怎么利用事件模型处理I&#x2F;O，解决线程池瓶颈处理海量连接，包括利用面向事件的方式编写服务端&#x2F;客户端程序。最后延展到一些高级主题，如Reactor与Proactor模型的对比、Selector的唤醒、Buffer的选择等。 注：本文的代码都是伪代码，主要是为了示意，不可用于生产环境。 传统BIO模型分析让我们先回忆一下传统的服务器端同步阻塞I&#x2F;O处理（也就是BIO，Blocking I&#x2F;O）的经典编程模型： 这是一个经典的每连接每线程的模型，之所以使用多线程，主要原因在于socket.accept()、socket.read()、socket.write()三个主要函数都是同步阻塞的，当一个连接在处理I&#x2F;O的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里；但CPU是被释放出来的，开启多线程，就可以让CPU去处理更多的事情。其实这也是所有使用多线程的本质： 利用多核。 当I&#x2F;O阻塞系统，但CPU空闲的时候，可以利用多线程使用CPU资源。 现在的多线程一般都使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的I&#x2F;O并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。 不过，这个模型最本质的问题在于，严重依赖于线程。但线程是很”贵”的资源，主要表现在： 线程的创建和销毁成本很高，在Linux这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 线程本身占用较大内存，像Java的线程栈，一般至少分配512K～1M的空间，如果系统中的线程数过千，恐怕整个JVM的内存都会被吃掉一半。 线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统load偏高、CPU sy使用率特别高（超过20%以上)，导致系统几乎陷入不可用的状态。 容易造成锯齿状的系统负载。因为系统负载是用活动线程数或CPU核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。 所以，当面对十万甚至百万级连接的时候，传统的BIO模型是无能为力的。随着移动端应用的兴起和各种网络游戏的盛行，百万级长连接日趋普遍，此时，必然需要一种更高效的I&#x2F;O处理模型。 NIO是怎么工作的很多刚接触NIO的人，第一眼看到的就是Java相对晦涩的API，比如：Channel，Selector，Socket什么的；然后就是一坨上百行的代码来演示NIO的服务端Demo……瞬间头大有没有？ 我们不管这些，抛开现象看本质，先分析下NIO是怎么工作的。 常见I&#x2F;O模型对比所有的系统I&#x2F;O都分为两个阶段：等待就绪和操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。 需要说明的是等待就绪的阻塞是不使用CPU的，是在“空等”；而真正的读写操作的阻塞是使用CPU的，真正在”干活”，而且这个过程非常快，属于memory copy，带宽通常在1GB&#x2F;s级别以上，可以理解为基本不耗时。 下图是几种常见I&#x2F;O模型的对比： 以socket.read()为例子： 传统的BIO里面socket.read()，如果TCP RecvBuffer里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据。 对于NIO，如果TCP RecvBuffer有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回0，永远不会阻塞。 最新的AIO(Async I&#x2F;O)里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的。 换句话说，BIO里用户最关心“我要读”，NIO里用户最关心”我可以读了”，在AIO模型里用户更需要关注的是“读完了”。 NIO一个重要的特点是：socket主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的I&#x2F;O操作是同步阻塞的（消耗CPU但性能非常高）。 如何结合事件模型使用NIO同步非阻塞特性回忆BIO模型，之所以需要多线程，是因为在进行I&#x2F;O操作的时候，一是没有办法知道到底能不能写、能不能读，只能”傻等”，即使通过各种估算，算出来操作系统没有能力进行读写，也没法在socket.read()和socket.write()函数中返回，这两个函数无法进行有效的中断。所以除了多开线程另起炉灶，没有好的办法利用CPU。 NIO的读写函数可以立刻返回，这就给了我们不开线程利用CPU的最好机会：如果一个连接不能读写（socket.read()返回0或者socket.write()返回0），我们可以把这件事记下来，记录的方式通常是在Selector上注册标记位，然后切换到其它就绪的连接（channel）继续进行读写。 下面具体看下如何利用事件模型单线程处理所有I&#x2F;O请求： NIO的主要事件有几个：读就绪、写就绪、有新连接到来。 我们首先需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于accept，一般是服务器刚启动的时候；而对于connect，一般是connect失败需要重连或者直接异步调用connect的时候。 其次，用一个死循环选择就绪的事件，会执行系统调用（Linux 2.6之前是select、poll，2.6之后是epoll，Windows是IOCP），还会阻塞的等待新事件的到来。新事件到来的时候，会在selector上注册标记位，标示可读、可写或者有连接到来。 注意，select是阻塞的，无论是通过操作系统的通知（epoll）还是不停的轮询(select，poll)，这个函数是阻塞的。所以你可以放心大胆地在一个while(true)里面调用这个函数而不用担心CPU空转。 所以我们的程序大概的模样是： 这个程序很简短，也是最简单的Reactor模式：注册所有感兴趣的事件处理器，单线程轮询选择就绪事件，执行事件处理器。 优化线程模型由上面的示例我们大概可以总结出NIO是怎么解决掉线程的瓶颈并处理海量连接的： NIO由原来的阻塞读写（占用线程）变成了单线程轮询事件，找到可以进行读写的网络描述符进行读写。除了事件的轮询是阻塞的（没有可干的事情必须要阻塞），剩余的I&#x2F;O操作都是纯CPU操作，没有必要开启多线程。 并且由于线程的节约，连接数大的时候因为线程切换带来的问题也随之解决，进而为处理海量连接提供了可能。 单线程处理I&#x2F;O的效率确实非常高，没有线程切换，只是拼命的读、写、选择事件。但现在的服务器，一般都是多核处理器，如果能够利用多核心进行I&#x2F;O，无疑对效率会有更大的提高。 仔细分析一下我们需要的线程，其实主要包括以下几种： 事件分发器，单线程选择就绪的事件。 I&#x2F;O处理器，包括connect、read、write等，这种纯CPU操作，一般开启CPU核心个线程就可以。 业务线程，在处理完I&#x2F;O后，业务一般还会有自己的业务逻辑，有的还会有其他的阻塞I&#x2F;O，如DB操作，RPC等。只要有阻塞，就需要单独的线程。 Java的Selector对于Linux系统来说，有一个致命限制：同一个channel的select不能被并发的调用。因此，如果有多个I&#x2F;O线程，必须保证：一个socket只能属于一个IoThread，而一个IoThread可以管理多个socket。 另外连接的处理和读写的处理通常可以选择分开，这样对于海量连接的注册和读写就可以分发。虽然read()和write()是比较高效无阻塞的函数，但毕竟会占用CPU，如果面对更高的并发则无能为力。 NIO在客户端的魔力通过上面的分析，可以看出NIO在服务端对于解放线程，优化I&#x2F;O和处理海量连接方面，确实有自己的用武之地。那么在客户端上，NIO又有什么使用场景呢? 常见的客户端BIO+连接池模型，可以建立n个连接，然后当某一个连接被I&#x2F;O占用的时候，可以使用其他连接来提高性能。 但多线程的模型面临和服务端相同的问题：如果指望增加连接数来提高性能，则连接数又受制于线程数、线程很贵、无法建立很多线程，则性能遇到瓶颈。 每连接顺序请求的Redis对于Redis来说，由于服务端是全局串行的，能够保证同一连接的所有请求与返回顺序一致。这样可以使用单线程＋队列，把请求数据缓冲。然后pipeline发送，返回future，然后channel可读时，直接在队列中把future取回来，done()就可以了。 伪代码如下： 这样做，能够充分的利用pipeline来提高I&#x2F;O能力，同时获取异步处理能力。 多连接短连接的HttpClient类似于竞对抓取的项目，往往需要建立无数的HTTP短连接，然后抓取，然后销毁，当需要单机抓取上千网站线程数又受制的时候，怎么保证性能呢? 何不尝试NIO，单线程进行连接、写、读操作？如果连接、读、写操作系统没有能力处理，简单的注册一个事件，等待下次循环就好了。 如何存储不同的请求&#x2F;响应呢？由于http是无状态没有版本的协议，又没有办法使用队列，好像办法不多。比较笨的办法是对于不同的socket，直接存储socket的引用作为map的key。 常见的RPC框架，如Thrift，Dubbo这种框架内部一般维护了请求的协议和请求号，可以维护一个以请求号为key，结果的result为future的map，结合NIO+长连接，获取非常不错的性能。 NIO高级主题Proactor与Reactor一般情况下，I&#x2F;O 复用机制需要事件分发器（event dispatcher）。 事件分发器的作用，即将那些读写事件源分发给各读写事件的处理者，就像送快递的在楼下喊: 谁谁谁的快递到了， 快来拿吧！开发人员在开始的时候需要在分发器那里注册感兴趣的事件，并提供相应的处理者（event handler)，或者是回调函数；事件分发器在适当的时候，会将请求的事件分发给这些handler或者回调函数。 涉及到事件分发器的两种模式称为：Reactor和Proactor。 Reactor模式是基于同步I&#x2F;O的，而Proactor模式是和异步I&#x2F;O相关的。在Reactor模式中，事件分发器等待某个事件或者可应用或个操作的状态发生（比如文件描述符可读写，或者是socket可读写），事件分发器就把这个事件传给事先注册的事件处理函数或者回调函数，由后者来做实际的读写操作。 而在Proactor模式中，事件处理者（或者代由事件分发器发起）直接发起一个异步读写操作（相当于请求），而实际的工作是由操作系统来完成的。发起时，需要提供的参数包括用于存放读到数据的缓存区、读的数据大小或用于存放外发数据的缓存区，以及这个请求完后的回调函数等信息。事件分发器得知了这个请求，它默默等待这个请求的完成，然后转发完成事件给相应的事件处理者或者回调。举例来说，在Windows上事件处理者投递了一个异步IO操作（称为overlapped技术），事件分发器等IO Complete事件完成。这种异步模式的典型实现是基于操作系统底层异步API的，所以我们可称之为“系统级别”的或者“真正意义上”的异步，因为具体的读写是由操作系统代劳的。 举个例子，将有助于理解Reactor与Proactor二者的差异，以读操作为例（写操作类似）。 在Reactor中实现读 注册读就绪事件和相应的事件处理器。 事件分发器等待事件。 事件到来，激活分发器，分发器调用事件对应的处理器。 事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。 在Proactor中实现读 处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，处理器无视IO就绪事件，它关注的是完成事件。 事件分发器等待操作完成事件。 在分发器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分发器读操作完成。 事件分发器呼唤处理器。 事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分发器。 可以看出，两个模式的相同点，都是对某个I&#x2F;O事件的事件通知（即告诉某个模块，这个I&#x2F;O操作可以进行或已经完成)。在结构上，两者也有相同点：事件分发器负责提交IO操作（异步)、查询设备是否可操作（同步)，然后当条件满足时，就回调handler；不同点在于，异步情况下（Proactor)，当回调handler时，表示I&#x2F;O操作已经完成；同步情况下（Reactor)，回调handler时，表示I&#x2F;O设备可以进行某个操作（can read 或 can write)。 下面，我们将尝试应对为Proactor和Reactor模式建立可移植框架的挑战。在改进方案中，我们将Reactor原来位于事件处理器内的Read&#x2F;Write操作移至分发器（不妨将这个思路称为“模拟异步”），以此寻求将Reactor多路同步I&#x2F;O转化为模拟异步I&#x2F;O。以读操作为例子，改进过程如下： 注册读就绪事件和相应的事件处理器。并为分发器提供数据缓冲区地址，需要读取数据量等信息。 分发器等待事件（如在select()上等待）。 事件到来，激活分发器。分发器执行一个非阻塞读操作（它有完成这个操作所需的全部信息），最后调用对应处理器。 事件处理器处理用户自定义缓冲区的数据，注册新的事件（当然同样要给出数据缓冲区地址，需要读取的数据量等信息），最后将控制权返还分发器。 如我们所见，通过对多路I&#x2F;O模式功能结构的改造，可将Reactor转化为Proactor模式。改造前后，模型实际完成的工作量没有增加，只不过参与者间对工作职责稍加调换。没有工作量的改变，自然不会造成性能的削弱。对如下各步骤的比较，可以证明工作量的恒定： 标准&#x2F;典型的Reactor 步骤1：等待事件到来（Reactor负责）。 步骤2：将读就绪事件分发给用户定义的处理器（Reactor负责）。 步骤3：读数据（用户处理器负责）。 步骤4：处理数据（用户处理器负责）。 改进实现的模拟Proactor 步骤1：等待事件到来（Proactor负责）。 步骤2：得到读就绪事件，执行读数据（现在由Proactor负责）。 步骤3：将读完成事件分发给用户处理器（Proactor负责）。 步骤4：处理数据（用户处理器负责）。 对于不提供异步I&#x2F;O API的操作系统来说，这种办法可以隐藏Socket API的交互细节，从而对外暴露一个完整的异步接口。借此，我们就可以进一步构建完全可移植的，平台无关的，有通用对外接口的解决方案。 代码示例如下： Selector.wakeup()主要作用解除阻塞在Selector.select()&#x2F;select(long)上的线程，立即返回。 两次成功的select之间多次调用wakeup等价于一次调用。 如果当前没有阻塞在select上，则本次wakeup调用将作用于下一次select——“记忆”作用。 为什么要唤醒？ 注册了新的channel或者事件。 channel关闭，取消注册。 优先级更高的事件触发（如定时器事件），希望及时处理。 原理Linux上利用pipe调用创建一个管道，Windows上则是一个loopback的tcp连接。这是因为win32的管道无法加入select的fd set，将管道或者TCP连接加入select fd set。 wakeup往管道或者连接写入一个字节，阻塞的select因为有I&#x2F;O事件就绪，立即返回。可见，wakeup的调用开销不可忽视。 Buffer的选择通常情况下，操作系统的一次写操作分为两步： 将数据从用户空间拷贝到系统空间。 从系统空间往网卡写。同理，读操作也分为两步： ① 将数据从网卡拷贝到系统空间； ② 将数据从系统空间拷贝到用户空间。 对于NIO来说，缓存的使用可以使用DirectByteBuffer和HeapByteBuffer。如果使用了DirectByteBuffer，一般来说可以减少一次系统空间到用户空间的拷贝。但Buffer创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。 如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer；反之可以用directBuffer。 NIOServer NIOClient NIO存在的问题使用NIO !&#x3D; 高性能，当连接数&lt;1000，并发程度不高或者局域网环境下NIO并没有显著的性能优势。 NIO并没有完全屏蔽平台差异，它仍然是基于各个操作系统的I&#x2F;O系统实现的，差异仍然存在。使用NIO做网络编程构建事件驱动模型并不容易，陷阱重重。 推荐大家使用成熟的NIO框架，如Netty，MINA等。解决了很多NIO的陷阱，并屏蔽了操作系统的差异，有较好的性能和编程模型。 总结最后总结一下到底NIO给我们带来了些什么： 事件驱动模型 避免多线程 单线程处理多任务 非阻塞I&#x2F;O，I&#x2F;O读写不再阻塞，而是返回0 基于block的传输，通常比基于流的传输更高效 更高级的IO函数，zero-copy IO多路复用大大提高了Java网络应用的可伸缩性和实用性 "},{"title":"并行搜索","date":"2019-09-05T16:00:00.000Z","url":"/2019/09/06/2019/09/061402/","tags":[["Future","/tags/Future/"]],"categories":[["Java","/categories/Java/"]],"content":"代码"},{"title":"线程池","date":"2019-09-03T16:00:00.000Z","url":"/2019/09/04/2019/09/041126/","tags":[["Java","/tags/Java/"],["Thread","/tags/Thread/"]],"categories":[["Java","/categories/Java/"]],"content":"线程池简单的线程创建和回收 实际生产环境中，线程的数量必须得到控制. 线程池做的工作主要是控制运行的线程的数量，处理过程中将任务放到队列，然后再线程创建后启动这些任务，如果线程数量超过了最大数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 主要特点：线程复用，控制最大并发数，管理线程。 降低资源消耗。通过重复利用已创建的线程降低线程创建和消费造成的资源消耗。 提高响应速度。当任务到达时，不需要等待线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 JDK中的线程 Executor框架提供了各种类型的线程池，主要有以下工厂方法： 说明： newFixedThreadPool()方法：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 newSingleThreadExecutor()方法：该方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 newCachedThreadPool()方法：该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 newSingleThreadScheduledExecutor()方法：该方法返回一个ScheduledExecutorService对象，线程池大小为1。ScheduledExecutorService接口在ExecutorService接口之上扩展了在给定时间执行某任务的功能，如在某个固定的延时之后执行，或者周期性执行某个任务。 newScheduledThreadPool()方法：该方法也返回一个ScheduledExecutorService对象，但该线程池可以指定线程数量。 ThreadPoolExecutor corePoolSize：指定了线程池中的线程数量。 maximumPoolSize：指定了线程池中的最大线程数量。 keepAliveTime：当线程池线程数量超过corePoolSize时，多余的空闲线程的存活时间。即，超过corePoolSize的空闲线程，在多长时间内，会被销毁。 unit：keepAliveTime的单位。 workQueue：任务队列，被提交但尚未被执行的任务。 threadFactory：线程工厂，用于创建线程，一般用默认的即可。 handler：拒绝策略。当任务太多来不及处理，如何拒绝任务。 workQueue参数workQueue指被提交但未执行的任务队列，它是一个BlockingQueue接口的对象，仅用于存放Runnable对象。根据队列功能分类，在ThreadPoolExecutor的构造函数中可使用以下几种BlockingQueue。 直接提交的队列：该功能由SynchronousQueue对象提供。SynchronousQueue是一个特殊的BlockingQueue。SynchronousQueue没有容量，每一个插入操作都要等待一个相应的删除操作，反之，每一个删除操作都要等待对应的插入操作。如果使用SynchronousQueue，提交的任务不会被真实的保存，而总是将新任务提交给线程执行，如果没有空闲的进程，则尝试创建新的进程，如果进程数量已经达到最大值，则执行拒绝策略。因此，使用SynchronousQueue队列，通常要设置很大的maximumPoolSize值，否则很容易执行拒绝策略。 有界的任务队列：有界的任务队列可以使用ArrayBlockingQueue实现。ArrayBlockingQueue的构造函数必须带一个容量参数，表示该队列的最大容量，如下所示。public ArrayBlockingQueue(int capacity)当使用有界的任务队列时，若有新的任务需要执行，如果线程池的实际线程数小于corePoolSize，则会优先创建新的线程，若大于corePoolSize，则会将新任务加入等待队列。若等待队列已满，无法加入，则在总线程数不大于maximumPoolSize的前提下，创建新的进程执行任务。若大于maximumPoolSize，则执行拒绝策略。可见，有界队列仅当在任务队列装满时，才可能将线程数提升到corePoolSize以上，换言之，除非系统非常繁忙，否则确保核心线程数维持在在corePoolSize。 无界的任务队列：无界任务队列可以通过LinkedBlockingQueue类实现。与有界队列相比，除非系统资源耗尽，否则无界的任务队列不存在任务入队失败的情况。当有新的任务到来，系统的线程数小于corePoolSize时，线程池会生成新的线程执行任务，但当系统的线程数达到corePoolSize后，就不会继续增加。若后续仍有新的任务加入，而又没有空闲的线程资源，则任务直接进入队列等待。若任务创建和处理的速度差异很大，无界队列会保持快速增长，直到耗尽系统内存。 优先任务队列：优先任务队列是带有执行优先级的队列。它通过PriorityBlockingQueue实现，可以控制任务的执行先后顺序。它是一个特殊的无界队列。无论是有界队列ArrayBlockingQueue，还是未指定大小的无界队列LinkedBlockingQueue都是按照先进先出算法处理任务的。而PriorityBlockingQueue则可以根据任务自身的优先级顺序先后执行，在确保系统性能的同时，也能有很好的质量保证（总是确保高优先级的任务先执行）。回顾newFixedThreadPool()方法的实现。它返回了一个corePoolSize和maximumPoolSize大小一样的，并且使用了LinkedBlockingQueue任务队列的线程池。因为对于固定大小的线程池而言，不存在线程数量的动态变化，因此corePoolSize和maximumPoolSize可以相等。同时，它使用无界队列存放无法立即执行的任务，当任务提交非常频繁的时候，该队列可能迅速膨胀，从而耗尽系统资源。 newSingleThreadExecutor()返回的单线程线程池，是newFixedThreadPool()方法的一种退化，只是简单的将线程池线程数量设置为1。 newCachedThreadPool()方法返回corePoolSize为0，maximumPoolSize无穷大的线程池，这意味着在没有任务时，该线程池内无线程，而当任务被提交时，该线程池会使用空闲的线程执行任务，若无空闲线程，则将任务加入SynchronousQueue队列，而SynchronousQueue队列是一种直接提交的队列，它总会迫使线程池增加新的线程执行任务。当任务执行完毕后，由于corePoolSize为0，因此空闲线程又会在指定时间内（60秒）被回收。 对于newCachedThreadPool()，如果同时有大量任务被提交，而任务的执行又不那么快时，那么系统便会开启等量的线程处理，这样做法可能会很快耗尽系统的资源。 注意：使用自定义线程池时，要根据应用的具体情况，选择合适的并发队列作为任务的缓冲。当线程资源紧张时，不同的并发队列对系统行为和性能的影响均不同。 拒绝策略 AbortPolicy策略：该策略会直接抛出异常，阻止系统正常工作。 CallerRunsPolicy策略：只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。显然这样做不会真的丢弃任务，但是，任务提交线程的性能极有可能会急剧下降。 DiscardOledestPolicy策略：该策略将丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。 DiscardPolicy策略：该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，我觉得这可能是最好的一种方案了吧！ 自定义线程池和拒绝策略的使用： 创建方式线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors 返回的线程池对象的弊端如下： 1） FixedThreadPool 和 SingleThreadPool：允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 2） CachedThreadPool：允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 线程池命名的三种方式 spring CustomizableThreadFactory guava ThreadFactoryBuilder lang3 BasicThreadFactory Positive example 1： Positive example 2： Positive example 3： 合理配置线程池CPU密集型CPU密集的意思是该任务需要大量的运算，而没有阻塞，CPU一直全速运行。 CPU密集型的任务配置尽可能少的线程数量 CPU核数+1个线程的线程数。 IO密集型 由于IO密集型任务的线程并不是一直在执行任务，则应配置尽可能多的线程，如CPU核数*2 IO密集型，即任务需要大量的IO，即大量的阻塞。在单线程上运行IO密集型的任务会导致浪费大量的CPU运算能力浪费在等待。所以在IO密集型任务中使用多线程可以大大的加速程序运行。故需要·多配置线程数： 参考公式：CPU核数&#x2F;（1-阻塞系数 ） 阻塞系数在（0.8-0.9）之间 比如8核CPU：8&#x2F;（1-0.9） &#x3D; 80个线程数"},{"title":"Ubuntu18.04搭建docker的DNS解析服务","date":"2019-09-02T16:00:00.000Z","url":"/2019/09/03/2019/09/031013/","tags":[["docker","/tags/docker/"],["DNS","/tags/DNS/"],["Ubuntu","/tags/Ubuntu/"]],"categories":[["Linux","/categories/Linux/"]],"content":"更新 使用adguard home创建dns解析服务器 需求内部服务器使用docker，80多个项目分到不同的tomcat中，项目之间还有相互调用的需要，当然，如果有单独的注册中心，那肯定是没什么问题了。调用是基于内部的域名进行访问的，为每一个tomcat容器配置dns解析也不是不可以，那么多，就很烦躁了，link也无能为力，于是打起了DNS解析的主意。 环境镜像： 有两个，一个是命令行版，一个是带有web配置界面，看个人需求，这里选择第二种，毕竟不一定是我使用。jpillora/dnsmasq 配置文件 运行容器 注意：这里可能会有53端口冲突，Ubuntu自有的DNS解析会占用 查看端口占用 解决办法：How to disable systemd-resolved in Ubuntu? DNS解析配置运行成功以后，查看dns解析所在的服务ip 编辑/etc/resolv.conf WEB界面服务器ip：5380，port和上面的运行时参数设置的一致，如果出现占用请自行设置. dns:192.168.0.151:5380 账号密码 验证修改docker的DNS解析 重启docker服务Ubuntu下 进入容器内部 结果： 配置文件说明"},{"title":"月初小记","date":"2019-09-01T16:00:00.000Z","url":"/2019/09/02/2019/09/012205/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"惊梦 昨晚的疼痛，让人记忆尤新，不知道吃坏了什么东西，凌晨两点半的时候，腹痛难忍，翻来覆去，不知为何。 天气开始进入秋雨连绵的时节了，没了盛夏的燥热，虽说上海的夏天也是极热的，但是对夏天的感觉缺少了些什么，空调房呆太久了？还是说少了知了，冰棒，亦或是漫长的暑假呢？ 早上出门，因为下雨天，就没有骑车，路面积水，不得不注意过往的车辆，不然和容易就溅一身泥水。天气已经转凉，T恤外面加了外套，走路太急，到了公司，微微有些发汗，就将外套脱了下来，开始了一天的工作。谁知下班走的时候，就忘记了外套这件事情呢。"},{"title":"Spring test 单元测试","date":"2019-08-30T16:00:00.000Z","url":"/2019/08/31/2019/08/311334/","tags":[["Spring test","/tags/Spring-test/"]],"categories":[["work","/categories/work/"]],"content":"起步:::tip为什么会分类在work呢，其实一开始也在纠结，然而还是选择了，嘿嘿。 使用Spring测试套件，Spring容器只会初始化一次。 使用Spring测试套件，测试用例类中的属性会被自动填充Spring容器的对应Bean。 使用Spring测试套件，Spring会在你验证后，自动回滚对数据库的操作，保证数据库的现场不被破坏，因此重复测试不会发生问题！ 只要你继承Spring的测试套件的用例类，你就可以通过jdbcTemplate在同一事务中访问数据库，查询数据的变化，验证操作的正确性！::: 依赖请根据具体的项目需要选择对应的jar版本 测试在项目的test文件中创建测试类，没有则新建，一般推荐创建一个和src平级的源文件目录。因为src内的类都是为日后产品准备的，而此处的类仅仅用于测试。而包的名称可以和src中的目录同名，这样由于在test源目录中，所以不会有冲突，而且名称又一模一样，更方便检索。这也是Maven的约定。 创建基类（鸡肋），用于加载配置文件。 自己的测试类（继承基类） "},{"title":"Spring Boot Test","date":"2019-08-30T16:00:00.000Z","url":"/2019/08/31/2019/08/311354/","tags":[["spring","/tags/spring/"]],"categories":[["work","/categories/work/"]],"content":"pom 基类 自己的测试类继承该类 "},{"title":"VsCode背景设置","date":"2019-08-28T16:00:00.000Z","url":"/2019/08/29/2019/08/290950/","tags":[["vscode","/tags/vscode/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 安装插件搜索background-cover Arch&#x2F;Manjaro查看vscode安装位置 插件原理是修改 /opt/visual-studio-code/resources/app/out/vs/workbench/workbench.desktop.main.css 这个 css 文件来 work的, 然而这个文件 owner 是 root, 普通用户没权限写.给普通用户写权限就行了, 也不用 chown 那么麻烦. 新版位置(名称修改了) $sudo chmod o+w &#x2F;opt&#x2F;visual-studio-code&#x2F;resources&#x2F;app&#x2F;out&#x2F;vs&#x2F;workbench&#x2F;workbench.main.css 然后重启 vscode 就可以了. Windows管理员权限启动即可 修改css文件,找到对应的安装位置 注意文件引用位置url为编码后的 直接修改文件workbench.main.css这里最好也修改一下文件权限 MacOS 下修改背景图片文件位置 "},{"title":"Jupyter 远程登录设置","date":"2019-08-27T16:00:00.000Z","url":"/2019/08/28/2019/08/281116/","tags":[["Python","/tags/Python/"],["Jupyter","/tags/Jupyter/"]],"categories":[["Python","/categories/Python/"]],"content":"在服务器上配置jupyter设置远程登录 生成配置文件（~&#x2F;.jupyter&#x2F;jupyter_notebook_config.py）jupyter notebook --generate-config 生成密钥先进入python环境，执行 此时会让你两次输入密码（该密码作为客户端登录jupyter用），然后就会生成秘钥 （秘钥作为配置文件用）＊＊＊＊＊＊＊＊＊＊＊＊ 编辑配置文件vim ~/.jupyter/jupyter_notebook_config.py 如果要配置默认目录：c.NotebookApp.notebook_dir = u&#39;/home/xxx&#39; 在服务器端启动 jupyter notebook本地地址栏输入服务器ip:8888 就可访问了 "},{"title":"尼尔·机械纪元","date":"2019-08-25T16:00:00.000Z","url":"/2019/08/26/2019/08/261118/","tags":[["game","/tags/game/"]],"categories":[["wuw","/categories/wuw/"]],"content":"起源游戏是给人带来快乐的。 接触游戏也有很长很长的时间了，当然这里主要说的是电子游戏。 童年的小霸王。超级玛丽，坦克大战，魂斗罗，忍者神龟。真是美好的回忆。 到了后来，渐渐的接触了电脑游戏。对于游戏的喜爱也一直没有改变过，但不知什么时候，就感觉力不从心，凭本事买的游戏，为什么要玩。还不如看别人玩游戏来的快乐。直播行业的兴起也直接提供了便利。回去看一些比赛之类的直播。昨天的dota2比赛，中国队输了，之前所了解的都是cndota，希望继续努力，胜不骄，败不馁。 大学时期，玩的最多的应该是《英雄联盟》了，这款游戏至今依然火爆，只是我实在是没有玩的动力了，曾经觉得自己戒除一款游戏会很困难，其实也简单的很。 说出来你可能不信，注册steam并不是为了购买游戏，wallpaper engine才是我的初衷，这就和当时注册B站差不多吧，并不是为了番剧，而是因为宅舞和鬼畜，哈哈哈哈。 游戏大赏没有人能够逃得了喜+1，渐渐地库存多了起来，但是打开steam的次数越来越少，工作电脑也换了Linux系统，虽然也可以使用steam，兴致缺缺。 剖析自己的游戏史，感觉是一件很丢人事情，可能和一直以来的教育有关，适当的游戏还是可以的，沉迷其中就很不好了。 尼尔初识，是通过网上一些图片见到的，第一眼就爱了，当时还以为是什么动漫里面的人物，知道了以后，就很想入手，一开始正版的价格实在是太高了，就玩了盗版，后来遇上打折就买了，再后来，这游戏定价直接就变成原来的一半了。据说当时刚上架steam的时候就是这个价格。游戏质量真的很好，就是SE的吃相有些难看，没有简体中文，需要自己网上找补丁，还有就是一些DLCf服装之类的。 买的时间也比较久了，网上也找了通关版本的存档覆盖进去了，要说自己真正的开始玩，3个小时，这本都没回啊，买游戏不玩，究竟是为了啥呢，一堆游戏，通关的也就一个《古墓丽影·崛起》，支线啥的也都没做，快速通关主线的那种。 何必为难自己呢。 昨晚再次启动了游戏，没有选择通关存档，而是自己的初识存档，还没有之前玩的打的距离远，对于剧情的走向也没有一个初步印象，这也是我想继续慢慢玩下去的动力了吧。 偏离主线最初提笔想记录的无非就是重新开始玩《尼尔·机械纪元》，但是还有其他的想说的一些话，关于其他游戏的东西，又不想重开一个，索性都记录在这里吧，感觉会越来越长的。 357当然是《真三国无双7》了，虽然7不是我玩的第一个真三系列，但却是玩的最久的。同系列还有4568都有玩过，割草游戏，爽就完事了。对于三国题材的，这个系列是难得的佳作，推荐。 其中五代的变动比较大， 武器招式之类的有很大变化。 僵尸吃掉了你的脑子不多说，基本每年都会玩一下吧，经典了。 云无月新买的《古剑奇谭·叁》，还是十分热乎的，没玩多久，吐槽一下云存档的服务器啊，是真的垃圾，在大哥阿育那租的吧。经常掉线，体验不好。 我永远喜欢云五月+岑樱。 暗黑血统三act游戏，今天打开邮箱，发现steam有一封邮件，说的是愿望单打折了，o((&gt;ω&lt; ))o，买啊。一二三齐了。"},{"title":"Spring Boot Docker","date":"2019-08-22T16:00:00.000Z","url":"/2019/08/23/2019/08/231651/","tags":[["docker","/tags/docker/"],["spring","/tags/spring/"]],"categories":[["Java","/categories/Java/"]],"content":"壹创建一个Spring Boot应用 项目结构 贰Dockerfile 叁pom.xml 使用dockerfile-maven-plugin构建镜像 肆build&amp;push 伍附录：替换*为自己的加速器名称 1.阿里云镜像加速器 网址 2.idea中的配置使用 3.参考 Spring-Boot-Docker"},{"title":"请把最好的脾气留给亲近的人","date":"2019-08-22T16:00:00.000Z","url":"/2019/08/23/2019/08/232104/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"一件小事 网购了一些东西寄回家，因为之前也寄过东西，老妈说把电话改成她的电话号码，省的到时候不好联系，收货人是她，但电话却是家里的另一部手机，很早之前的号码了，算是我的第一个手机号码。 但是今天到货了，貌似没有打电话给她，问了一下快递，东西已经到了镇上，需要自己去取，老妈已经熟知各大快递公司的取货点了。 因为起送价的问题，我再买完粽子的时候，加了一个豌豆零食，不同的货物，也就不同发货了，就导致豌豆提前到了，老妈听到我买了豌豆，就感觉很奇怪，说家里有很多，干嘛买这个，解释了一下，也不知道她有没有听明白，还好，到货之后，很好吃。说以后多买点。 和父母的沟通 虽说每周都会打电话回家，这是一个习惯了，但是每次通话的时间不会太久，也就聊一些小事情。也会因为一些事情争执，不觉中语气就会有些重了。 和家人总是聚少离多，沟通也是少的可怜，所以请把坏脾气收起来，面对家人，拿出自己十二分的朝气。 续&#x2F;&#x2F;TODO 未完待续"},{"title":"人间疾苦","date":"2019-08-20T16:00:00.000Z","url":"/2019/08/21/2019/08/212211/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"这是什么人间疾苦 轮胎在今天再次爆了，好几次了，明天走路吧。 一年有余吧，已经爆了好几次了，前胎&#x3D;》后胎&#x3D;》前胎，还是循环来的。 真是够了啊，估计只有实心胎才可以防止爆掉吧。 也没晒到，也没有急刹车，甚至没打很多气，就这样爆掉了，爆掉了。。。 2019-09-05再次上演，回来的路上，下起了雨。很庆幸自己带了伞吗，结果雨越下越大，路上遇到一个骑电瓶车的小哥，利马刹车，停在公交车站躲雨。奈何雨很大，伞也挡不住了，可恶的十字路口，没有红绿灯，车很多，雨很急，停车等待的时候，伞开始漏雨了，外面大雨，里面小雨，全都湿了，这破伞，根本不行啊。又是湿漉漉的一天。 2019年10月31日事情发生在昨天,让我不得不翻出这篇日记,加上这段. 我下班去找自行车,开锁,推走的时候,发现不对劲了,前轮一点气都没有了,以为又是爆胎了,但是不像之前几次,没看到口子,推回去,打气,好吧,估计是内胎坏了,早上骑过来的时候还好好的啊. 前一日还和朋友说这周过完就不骑车了,毕竟有些冷了.可能它偷听我们的聊天了,怕没有机会坑我.太惨了吧…"},{"title":"SkypeForLinux新版启动问题","date":"2019-08-19T16:00:00.000Z","url":"/2019/08/20/2019/08/201704/","tags":[["skype","/tags/skype/"],["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"今天打开skype发现有新的版本可以更新，于是更新了新版，然后重启发现出现问题，没有任何界面显示，于是开始了问题的排查和解决。 首先查找日志系统为manjaro ,Skype的启动文件位于usr/bin/skypeforlinux 打开文件 日志位于$HOME/.config/skypeforlinux/logs 查看日志 这里可以看出主要问题是chrome-sandbox的权限问题 修改权限文件位于/usr/share/skypeforlinux/chrome-sandbox 这里说明一下4755权限问题 chmod 4755与chmod 755对比多了附加权限值4，这个4表示其他用户执行文件时，具有与所有者同样的权限（设置了SUID）。 为什么要设置4755 而不是 755？假设netlogin是root用户创建的一个上网认证程序，如果其他用户要上网也要用到这个程序，那就需要root用户运行chmod 755 netlogin命令使其他用户也能运行netlogin。但假如netlogin执行时需要访问一些只有root用户才有权访问的文件，那么其他用户执行netlogin时可能因为权限不够还是不能上网。这种情况下，就可以用 chmod 4755 netlogin 设置其他用户在执行netlogin也有root用户的权限，从而顺利上网。 ————————————————版权声明：本文为CSDN博主「林20」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。原文链接： 再次启动skype "},{"title":"Stream API","date":"2019-08-13T16:00:00.000Z","url":"/2019/08/14/2019/08/141046/","tags":[["Java","/tags/Java/"],["Stream","/tags/Stream/"]],"categories":[["Java","/categories/Java/"]],"content":"流是什么流是Java API的新成员，它允许你以声明性方式处理数据集合（通过查询语句来表达，而不是临时编写一个实现）。就现在来说，你可以把它们看成遍历数据集的高级迭代器。此外，流还可以透明地并行处理，你无需写任何多线程代码了！ java7 java8 并行执行 使用流筛选和切片filter 筛选各异的元素distinct 截短流limit(n) 请注意limit也可以用在无序流上，比如源是一个Set。这种情况下，limit的结果不会以任何顺序排列。 跳过元素skip(n) 返回一个扔掉了前n个元素的流。如果流中元素不足n个，则返回一个空流。请注意，limit(n)和skip(n)是互补的！ 筛选你将如何利用流来筛选前两个荤菜呢？ 答案：你可以把filter和limit复合在一起来解决这个问题，并用collect(toList())将流转换成一个列表。 映射map 对流中每一个元素应用函数 流支持map方法，它会接受一个函数作为参数。这个函数会被应用到每个元素上，并将其映射成一个新的元素（使用映射一词，是因为它和转换类似，但其中的细微差别在于它是“创建一个新版本”而不是去“修改”） 流的扁平化Q:对于一张单词表，如何返回一张列表，列出里面各不相同的字符呢？例如，给定单词列表[&quot;Hello&quot;,&quot;World&quot;]，你想要返回列表[&quot;H&quot;,&quot;e&quot;,&quot;l&quot;, &quot;o&quot;,&quot;W&quot;,&quot;r&quot;,&quot;d&quot;]。 flatMap 使用flatMap方法的效果是，各个数组并不是分别映射成一个流，而是映射成流的内容。所有使用map(Arrays::stream)时生成的单个流都被合并起来，即扁平化为一个流。 一言以蔽之，flatmap方法让你把一个流中的每个值都换成另一个流，然后把所有的流连接起来成为一个流。 (1) 给定一个数字列表，如何返回一个由每个数的平方构成的列表呢？例如，给定[1, 2, 3, 4, 5]，应该返回[1, 4, 9, 16, 25]。 答案：你可以利用map方法的Lambda，接受一个数字，并返回该数字平方的Lambda来解决这个问题。 (2) 给定两个数字列表，如何返回所有的数对呢？例如，给定列表[1, 2, 3]和列表[3, 4]，应该返回[(1, 3), (1, 4), (2, 3), (2, 4), (3, 3), (3, 4)]。为简单起见，你可以用有两个元素的数组来代表数对。 答案：你可以使用两个map来迭代这两个列表，并生成数对。但这样会返回一个Stream&lt;Stream&lt;Integer[]&gt;&gt;。你需要让生成的流扁平化，以得到一个Stream&lt;Integer[]&gt;。这正是flatMap所做的： (3) 如何扩展前一个例子，只返回总和能被3整除的数对呢？例如(2, 4)和(3, 3)是可以的。 答案：你在前面看到了，filter可以配合谓词使用来筛选流中的元素。因为在flatMap操作后，你有了一个代表数对的int[]流，所以你只需要一个谓词来检查总和是否能被3整除就可以了： 其结果是[(2, 4), (3, 3)]。 查找和匹配另一个常见的数据处理套路是看看数据集中的某些元素是否匹配一个给定的属性。Stream API通过allMatch、anyMatch、noneMatch、findFirst和findAny方法提供了这样的工具。 检查谓词是否至少匹配一个元素anyMatch方法可以回答“流中是否有一个元素能匹配给定的谓词”。比如，你可以用它来看看菜单里面是否有素食可选择： anyMatch方法返回一个boolean，因此是一个终端操作。 检查谓词是否匹配所有元素allMatch方法的工作原理和anyMatch类似，但它会看看流中的元素是否都能匹配给定的谓词。比如，你可以用它来看看菜品是否有利健康（即所有菜的热量都低于1000卡路里）： noneMatch 和allMatch相对的是noneMatch。它可以确保流中没有任何元素与给定的谓词匹配。比如，你可以用noneMatch重写前面的例子： anyMatch、allMatch和noneMatch这三个操作都用到了我们所谓的短路，这就是大家熟悉的Java中&amp;&amp;和||运算符短路在流中的版本。 短路求值 有些操作不需要处理整个流就能得到结果。例如，假设你需要对一个用and连起来的大布尔表达式求值。不管表达式有多长，你只需找到一个表达式为false，就可以推断整个表达式将返回false，所以用不着计算整个表达式。这就是短路。 对于流而言，某些操作（例如allMatch、anyMatch、noneMatch、findFirst和findAny）不用处理整个流就能得到结果。只要找到一个元素，就可以有结果了。同样，limit也是一个短路操作：它只需要创建一个给定大小的流，而用不着处理流中所有的元素。在碰到无限大小的流的时候，这种操作就有用了：它们可以把无限流变成有限流。 查找元素 Optional简介 Optional&lt;T&gt;类（java.util.Optional）是一个容器类，代表一个值存在或不存在。在上面的代码中，findAny可能什么元素都没找到。Java 8的库设计人员引入了Optional&lt;T&gt;，这样就不用返回众所周知容易出问题的null了。我们在这里不会详细讨论Optional，因为第10章会详细解释你的代码如何利用Optional，避免和null检查相关的bug。不过现在，了解一下Optional里面几种可以迫使你显式地检查值是否存在或处理值不存在的情形的方法也不错。 isPresent()将在Optional包含值的时候返回true, 否则返回false。 ifPresent(Consumer&lt;T&gt; block)会在值存在的时候执行给定的代码块。我们在第3章介绍了Consumer函数式接口；它让你传递一个接收T类型参数，并返回void的Lambda表达式。 T get()会在值存在时返回值，否则抛出一个NoSuchElement异常。 T orElse(T other)会在值存在时返回值，否则返回一个默认值。 查找第一个元素findFirst 何时使用findFirst和findAny 你可能会想，为什么会同时有findFirst和findAny呢？答案是并行。找到第一个元素在并行上限制更多。如果你不关心返回的元素是哪个，请使用findAny，因为它在使用并行流时限制较少。 归约元素求和reduce 无初始值 最大值和最小值 斐波纳契元组序列 表 中间操作和终端操作 操作 类型 返回类型 使用的类型&#x2F;函数式接口 函数描述符 filter 中间 Stream&lt;T&gt; Predicate&lt;T&gt; T -&gt; boolean distinct 中间(有状态-无界) Stream&lt;T&gt; skip 中间(有状态-有界) Stream&lt;T&gt; long limit 中间(有状态-有界) Stream&lt;T&gt; long map 中间 Stream&lt;R&gt; Function&lt;T, R&gt; T -&gt; R flatMap 中间 Stream&lt;R&gt; Function&lt;T, Stream&lt;R&gt;&gt; T -&gt; Stream&lt;R&gt; sorted 中间(有状态-无界) Stream&lt;T&gt; Comparator&lt;T&gt; (T, T) -&gt; int anyMatch 终端 boolean Predicate&lt;T&gt; T -&gt; boolean noneMatch 终端 boolean Predicate&lt;T&gt; T -&gt; boolean allMatch 终端 boolean Predicate&lt;T&gt; T -&gt; boolean findAny 终端 Optional&lt;T&gt; findFirst 终端 Optional&lt;T&gt; forEach 终端 void Consumer&lt;T&gt; T -&gt; void collect 终端 R Collector&lt;T, A, R&gt; reduce 终端(有状态-有界) Optional&lt;T&gt; BinaryOperator&lt;T&gt; (T, T) -&gt; T count 终端 long 小结 Streams API可以表达复杂的数据处理查询。常用的流操作总结在表5-1中。 你可以使用filter、distinct、skip和limit对流做筛选和切片。 你可以使用map和flatMap提取或转换流中的元素。 你可以使用findFirst和findAny方法查找流中的元素。你可以用allMatch、noneMatch和anyMatch方法让流匹配给定的谓词。 这些方法都利用了短路：找到结果就立即停止计算；没有必要处理整个流。 你可以利用reduce方法将流中所有的元素迭代合并成一个结果，例如求和或查找最大元素。 filter和map等操作是无状态的，它们并不存储任何状态。reduce等操作要存储状态才能计算出一个值。sorted和distinct等操作也要存储状态，因为它们需要把流中的所有元素缓存起来才能返回一个新的流。这种操作称为有状态操作。 流有三种基本的原始类型特化：IntStream、DoubleStream和LongStream。它们的操作也有相应的特化。 流不仅可以从集合创建，也可从值、数组、文件以及iterate与generate等特定方法创建。 无限流是没有固定大小的流。 "},{"title":"HashMap分析","date":"2019-08-11T16:00:00.000Z","url":"/2019/08/12/2019/08/121707/","tags":[["HashMap","/tags/HashMap/"]],"categories":[["Java","/categories/Java/"]],"content":"摘要HashMap是使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 对于之前的版本数组+链表来说，当hash冲突多的时候，会导致一个桶上的链表很长，查找的复杂度O(n)。而Java8 HashMap，当一条链长度超过8，便会转化成红黑树。红黑树特点：插入、查找、删除的时间复杂度为O(log n)。 红黑树的关键性质：从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。这样最坏情况也可以是高效的。 每个节点使用Node存储信息，如果转化为红黑树，则使用TreeNode存储树的节点。 put ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]&#x3D;&#x3D;null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 resize loadFactor为什么是0.75 如果是0.5 ， 那么每次达到容量的一半就进行扩容，默认容量是16， 达到8就扩容成32，达到16就扩容成64， 最终使用空间和未使用空间的差值会逐渐增加，空间利用率低下。 如果是1，那意味着每次空间使用完毕才扩容，在一定程度上会增加put时候的时间。 为一般规则，默认负载因子（0.75）在时间和空间成本上提供了很好的折衷。较高的值会降低空间开销，但提高查找成本（体现在大多数的HashMap类的操作，包括get和put）。设置初始大小时，应该考虑预计的entry数在map及其负载系数，并且尽量减少rehash操作的次数。如果初始容量大于最大条目数除以负载因子，rehash操作将不会发生。 HashMap的构造函数提供了设置加载因子和初始化大小。 HashMap 的⻓度为什么是2的幂次⽅为了能让 HashMap 存取⾼效，尽量减少碰撞，也就是要尽量把数据分配均匀。Hash 值的范围值-2147483648到2147483647，前后加起来⼤概40亿的映射空间，只要哈希函数映射得⽐᫾均匀松散，⼀般应⽤是很难出现碰撞的。但问题是⼀个40亿⻓度的数组，内存是放不下的。所以这个散列值是不能直接拿来⽤的。⽤之前还要先做对数组的⻓度取模运算，得到的余数才能⽤来要存放的位置也就是对应的数组下标。这个数组下标的计算⽅法是“ (n - 1) &amp;hash ”。（n代表数组⻓度）。这也就解释了 HashMap 的⻓度为什么是2的幂次⽅。 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，JDK1.7在并发的多线程使用场景中使用HashMap可能造成死循环。 主要原因在于 并发下的Rehash 会造成元素之间会形成⼀个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使⽤ HashMap,因为多线程下使⽤ HashMap 还是会存在其他问题⽐如数据丢失。并发环境下推荐使⽤ ConcurrentHashMap ConcurrentHashMap 和 Hashtable 的区别ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的⽅式上不同。 底层数据结构： JDK1.7 的 ConcurrentHashMap 底层采⽤ 分段的数组+链表 实现，JDK1.8采⽤的数据结构跟 HashMap1.8 的结构⼀样，数组+链表&#x2F;红⿊⼆叉树。 Hashtable 和JDK1.8 之前的 HashMap 的底层数据结构类似都是采⽤ 数组+链表 的形式，数组是HashMap 的主体，链表则是主要为了解决哈希冲突⽽存在的； 实现线程安全的⽅式（重要）： ① 在 JDK1.7 的时候， ConcurrentHashMap （分段锁）对整个桶数组进⾏了分割分段( Segment )，每⼀把锁只锁容器其中⼀部分数据，多线程访问容器⾥不同数据段的数据，就不会存在锁竞争，提⾼并发访问率。 到了 JDK1.8 的时候已经摒弃了 Segment 的概念，⽽是直接⽤ Node 数组+链表+红⿊树的数据结构来实现，并发控制使⽤ synchronized 和 CAS 来操作。（JDK1.6 以后 对 synchronized 锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap ，虽然在 JDK1.8 中还能看到Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本； ② Hashtable (同⼀把锁) :使⽤ synchronized 来保证线程安全，效率⾮常低下。当⼀个线程访问同步⽅法时，其他线程也访问同步⽅法，可能会进⼊阻塞或轮询状态，如使⽤ put 添加元素，另⼀个线程不能使⽤ put 添加元素，也不能使⽤ get，竞争会越来越激烈效率越低。 JDK1.7ConcurrentHashMap： ⾸先将数据分为⼀段⼀段的存储，然后给每⼀段数据配⼀把锁，当⼀个线程占⽤锁访问其中⼀个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。Segment 实现了 ReentrantLock ,所以 Segment 是⼀种可重⼊锁，扮演锁的⻆⾊。 HashEntry ⽤于存储键值对数据。 ⼀个 ConcurrentHashMap ⾥包含⼀个 Segment 数组。 Segment 的结构和 HashMap 类似，是⼀种数组和链表结构，⼀个 Segment 包含⼀个 HashEntry 数组，每个 HashEntry 是⼀个链表结构的元素，每个 Segment 守护着⼀个 HashEntry 数组⾥的元素，当对 HashEntry 数组的数据进⾏修改时，必须⾸先获得对应的 Segment 的锁。 JDK1.8 的 ConcurrentHashMap：JDK1.8 的 ConcurrentHashMap 不在是 Segment 数组 + HashEntry 数组 + 链表，⽽是 Node 数组 + 链表 &#x2F; 红⿊树。不过，Node 只能⽤于链表的情况，红⿊树的情况需要使⽤ TreeNode 。当冲突链表达到⼀定⻓度时，链表会转换成红⿊树。 ConcurrentHashMap 取消了 Segment 分段锁，采⽤ CAS 和 synchronized 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组+链表&#x2F;红⿊⼆叉树。Java 8 在链表⻓度超过⼀定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红⿊树（寻址时间复杂度为 O(log(N))） synchronized 只锁定当前链表或红⿊⼆叉树的⾸节点，这样只要 hash 不冲突，就不会产⽣并发，效率⼜提升 N 倍。 参考资料 HashMap的loadFactor为什么是0.75？"},{"title":"Start","date":"2019-08-10T16:00:00.000Z","url":"/2019/08/11/2019/08/112104/","tags":[["daily","/tags/daily/"],["music","/tags/music/"]],"categories":[["wuw","/categories/wuw/"]],"content":"开始的开始&emsp;&emsp;万事开头难，很难得就是开始了，迟迟没有动手去写一些东西。会有想到很多的想法，做很多的梦，也有想过将每次的梦都记录下来。结果往往是醒了就忘记了很多细节。"},{"title":"GC垃圾收集器","date":"2019-08-09T16:00:00.000Z","url":"/2019/08/10/2019/08/101112/","tags":[["JVM","/tags/JVM/"],["GC","/tags/GC/"]],"categories":[["Java","/categories/Java/"]],"content":"概述简单来说，垃圾收集由两步构成：查找不再使用的对象，以及释放这些对象所管理的内存。 分代垃圾收集器根据情况将堆划分成不同的代（Generation）。这些代被称为“老年代”（Old Generation 或 Tenured Generation）和“新生代”（Young Generation）。新生代又被进一步地划分为不同的区段，分别称为 Eden 空间和 Survivor 空间（不过 Eden 有时会被错误地用于指代整个新生代）。 采用分代机制的原因是很多对象的生存时间非常短。 Java 中，这种操作是非常普遍的，所以垃圾收集器设计时就特别考虑要处理大量（有时候是大多数）的临时对象。这也是分代设计的初衷之一。新生代是堆的一部分，对象首先在新生代中分配。新生代填满时，垃圾收集器会暂停所有的应用线程，回收新生代空间。不再使用的对象会被回收，仍然在使用的对象会被移动到其他地方。这种操作被称为 Minor GC。 采用这种设计有两个性能上的优势。其一，由于新生代仅是堆的一部分，与处理整个堆相比，处理新生代的速度更快。而这意味着应用线程停顿的时间会更短。你可能也看到了这其中的权衡，这意味着应用程序线程会更频繁地发生停顿，因为 JVM 不再等到整个堆都填满才进行垃圾收集；本章后续部分会针对其利弊进行深入的讨论。然而，就目前而言，更短的停顿显然能带来更多的优势，即使发生的频率更高。 第二个优势源于新生代中对象分配的方式。对象分配于 Eden 空间（占据了新生代空间的绝大多数）。垃圾收集时，新生代空间被清空，Eden 空间中的对象要么被移走，要么被回收；所有的存活对象要么被移动到另一个 Survivor 空间，要么被移动到老年代。由于所有的对象都被移走，相当于新生代空间在垃圾收集时自动地进行了一次压缩整理。 所有的垃圾收集算法在对新生代进行垃圾回收时都存在“时空停顿(stop-the-world)”现象。 对象不断地被移动到老年代，最终老年代也会被填满，JVM 需要找出老年代中不再使用的对象，并对它们进行回收。而这便是垃圾收集算法差异最大的地方。简单的垃圾收集算法直接停掉所有的应用线程，找出不再使用的对象，对其进行回收，接着对堆空间进行整理。这个过程被称为 Full GC，通常导致应用程序线程长时间的停顿。 垃圾收集分类 部分收集（Partial GC）：指目标不是完整收集整个Java堆的垃圾收集器，其中又分为： 新生代收集（ Minor GC&#x2F;Young GC）:指目标只是新生代的垃圾收集。 老年代收集（Manjor GC&#x2F;Old GC）:指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。另外请注意“Major GC”这个说法在不同的资料上常又不同所指。 混合收集（Mixed GC）:指目标是收集整个新生代及部分老年代的垃圾收集。目前只有G1收集器会有这种行为。 整堆收集（Full GC）:收集整个Java堆和方法区的垃圾收集。 收集算法 标记-清除：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可也反过来。 标记-复制：按比例划分内存，每次使用其中一块，用完之后，将还存活的对象复制到另外一块上，清理已使用的那一块。 标记-整理：老年代一般不能使用标记-复制，让所有存活的对象都向内存空间一端移动，然后直接清理掉边界的内存。 GC收集器JVM 提供了以下几种不同的垃圾收集器。 Serial垃圾收集器Serial 垃圾收集器是四种垃圾收集器中最简单的一种。如果应用运行在 Client 型虚拟机（Windows 平台上的 32 位 JVM 或者是运行在单处理器机器上的 JVM）上，这也是默认的垃圾收集器。 Serial 收集器使用单线程清理堆的内容。使用 Serial 收集器，无论是进行 Minor GC 还是 Full GC，清理堆空间时，所有的应用线程都会被暂停。进行 Full GC 时，它还会对老年代空间的对象进行压缩整理。通过 -XX:+UseSerialGC 标志可以启用 Serial 收集器（大多数情况下，如果可以使用这个标志，默认就会开启）。注意，跟大多数的 JVM 标志不同，关闭 Serial 收集器不能简单地将加号符变成减号符（譬如，使用 -XX:-UseSerialGC）。在 Serial 收集器作为默认收集器的系统上，如果需要关闭 Serial 收集器，可以通过指定另一种垃圾收集器来实现。 Throughput&#x2F;PS&amp;PO垃圾收集器**Throughput 收集器是 Server 级虚拟机（多 CPU 的 Unix 机器以及任何 64 位虚拟机）的默认收集器。 Throughput 收集器使用多线程回收新生代空间，Minor GC 的速度比使用 Serial 收集器快得多。处理老年代时 Throughput 收集器也能使用多线程方式。这已经是 JDK 7u4 及之后的版本的默认行为，对于之前老版本的 JDK 7 虚拟机，通过 -XX:+UseParallelOldGC 标志可以开启这个功能。由于 Throughput 收集器使用多线程，Throughput 收集器也常常被称为 Parallel 收集器。Throughput 收集器在 Minor GC 和 Full GC 时会暂停所有的应用线程，同时在 Full GC 过程中会对老年代空间进行压缩整理。由于在大多数适用的场景，它已经是默认的收集器，所以你基本上不需要显式地启用它。如果需要，可以使用 -XX:+UseParallelGC、-XX:+UseParallelOldGC 标志启用 Throughput 收集器。 CMS收集器 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） CMS 收集器设计的初衷是为了消除 Throughput 收集器和 Serial 收集器 Full GC 周期中的长时间停顿。CMS 收集器在 Minor GC 时会暂停所有的应用线程，并以多线程的方式进行垃圾回收。然而，这其中最显著的不同是，CMS 不再使用 Throughput 的收集算法（-XX:+UseParallelGC），改用新的算法来收集新生代对象（使用 -XX:+UseParNewGC 标志）。 CMS 收集器在 Full GC 时不再暂停应用线程，而是使用若干个后台线程定期地对老年代空间进行扫描，及时回收其中不再使用的对象。这种算法帮助 CMS 成为一个低延迟的收集器：应用线程只在 Minor GC 以及后台线程扫描老年代时发生极其短暂的停顿。应用程序线程停顿的总时长与使用 Throughput 收集器比起来短得多。 这里额外付出的代价是更高的 CPU 使用：必须有足够的 CPU 资源用于运行后台的垃圾收集线程，在应用程序线程运行的同时扫描堆的使用情况。除此之外，后台线程不再进行任何压缩整理的工作，这意味着堆会逐渐变得碎片化。如果 CMS 的后台线程无法获得完成他们任务所需的 CPU 资源，或者如果堆变得过度碎片化以至于无法找到连续空间分配对象，CMS 就蜕化到 Serial 收集器的行为：暂停所有应用线程，使用单线程回收、整理老年代空间。这之后又恢复到并发运行，再次启动后台线程（直到下一次堆变得过度碎片化）。通过 -XX:+UseConcMarkSweepGC、-XX:+UseParNewGC 标志（默认情况下，这两个标志都是禁用的）可以启用 CMS 垃圾收集器。 CMS是一款基于“标记—清除”算法实现的收集器，如果读者对前面这种算法介绍还有印象的话，就可能想到这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。为了解决这个问题，CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数（默认就是开启的，JDK9废弃），用于在CMS收集器顶不住要进行FullGC时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间不得不变长。虚拟机设计者还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction(JDK9废弃)，这个参数是用于设置执行多少次不压缩的Full GC后，跟着来一次带压缩的（默认值为0，表示每次进入Full GC时都进行碎片整理）。 ParNew使之上是Serial收集器的多线程并行版本，只有它能够与CMS收集器配合工作. CMS不能和ParallelScavenge配合工作的原因 一个面向低延迟，一个面向高吞吐量，目标不一致 技术上的原因是PS收集器及G1收集器等都没有使用Hotspot中原本设计的垃圾收集器的分带框架，而选择另外独立实现。Serial、ParNew则共用了这部分代码。 G1垃圾收集器G1 垃圾收集器（或者垃圾优先收集器）的设计初衷是为了尽量缩短处理超大堆（大于 4 GB）时产生的停顿。G1 收集算法将堆划分为若干个区域（Region），不过它依旧属于分代收集器。这些区域中的一部分包含新生代，新生代的垃圾收集仍然采用暂停所有应用线程的方式，将存活对象移动到老年代或者 Survivor 空间。同其他的收集算法一样，这些操作也利用多线程的方式完成。 G1 收集器属于 Concurrent 收集器：老年代的垃圾收集工作由后台线程完成，大多数的工作不需要暂停应用线程。由于老年代被划分到不同的区域，G1 收集器通过将对象从一个区域复制到另一个区域，完成对象的清理工作，这也意味着在正常的处理过程中，G1 收集器实现了堆的压缩整理（至少是部分的整理）。因此，使用 G1 收集器的堆不大容易发生碎片化——虽然这种问题无法避免。 同 CMS 收集器一样，避免 Full GC 的代价是消耗额外的 CPU 周期：负责垃圾收集的多个后台线程必须能在应用线程运行的同时获得足够的 CPU 运行周期。通过标志 -XX:+UseG1GC（默认值是关闭的）可以启动 G1 垃圾收集器。 相关书籍：Title : Java性能权威指南Author(s) : [美] 奥克斯（Oaks,S.）Publisher : 人民邮电出版社深入理解Java虚拟机(第3版) "},{"title":"有关台风的回忆","date":"2019-08-09T16:00:00.000Z","url":"/2019/08/10/2019/08/101445/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"零&emsp;&emsp;台风“利奇马”今天凌晨在浙江温岭登陆了，受其影响，上海今天是狂风暴雨。虽然今天是周六，但还是需要到公司工作。由于昨天晚上就开始下雨，就没有骑自行车回去，今天也是打车到的公司，去买早餐的一小段路上，就有点羽化而登仙的感觉了。 这可恶的体重。 一&emsp;&emsp;台风登陆的是浙江温岭，很巧合的是我之前的公司总部就在此地，上海是技术部门，后来取消了，就在温岭工作了一段时间。刚好也是夏天，遇台风，到处都是雨水，你的伞根本挡不住，狂风暴雨，360度围绕着你。再说了，也没办法稳住伞啊，说不定人和伞就一起飞向空中了，类似这样： 温岭其实是属于台州的，可能就是因为台风多在此地登录，所以叫台州？ 二&emsp;&emsp;印象中还有一次台风的经历是在大学读书的时候，也是在上海，学校位于滴水湖畔，临港的妖风那是处了名的厉害呀，网上有图为证，来了台风的加持，那更是势不可挡。 记得去食堂打饭都步履维艰啊。 三 &emsp;&emsp;此时此刻外面以已经停雨了，但是我还是不敢开窗，因为昨天一开窗就把桌子上的植物给吹倒了，里面的水也流的到处都是。 不知明天会不会好一些呢？"},{"title":"Manjaro下网易云音乐中文输入问题","date":"2019-08-09T16:00:00.000Z","url":"/2019/08/10/2019/08/101727/","tags":[["manjaro","/tags/manjaro/"],["netease","/tags/netease/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Manjaro解决网易云音乐无法在搜索框输入中文的问题更新： 新版中已经失效 先安装qcef这个软件包。 编辑/opt/netease/netease-cloud-music/netease-cloud-music.bash，把它改成这样： 可以了。 原理：网易云音乐他是利用deepin自家的qcef这个qt库，这个库在ubuntu上没有，于是ubuntu版的网易云音乐就内置了这个库。但在Arch系的Linux发行版上，使用这个自带的qcef会造成中文无法输入。于是我们就把启动脚本里的重定向到自家的库的命令去掉，强制让他使用arch自带的库，就完事了。至于为什么要模拟桌面环境是dde吗，因为如果不修改，在xfce上会出现双重标题栏，kde有可能会无法弹出右键菜单。 百度贴吧地址"},{"title":"线程安全与锁优化","date":"2019-08-07T16:00:00.000Z","url":"/2019/08/08/2019/08/081654/","tags":[["JVM","/tags/JVM/"],["thread","/tags/thread/"]],"categories":[["Java","/categories/Java/"]],"content":" Java虚拟机的锁优化锁偏向锁偏向是一种针对加锁操作的优化手段。它的核心思想是：如果一个线程获得了锁，那么锁就进入偏向模式。当这个线程再次请求锁时，无须再做任何同步操作。这样就节省了大量有关锁申请的操作，从而提高了程序性能。因此，对于几乎没有锁竞争的场合，偏向锁有比较好的优化效果，因为连续多次极有可能是同一个线程请求相同的锁。而对于锁竞争比较激烈的场合，其效果不佳。因为在竞争激烈的场合，最有可能的情况是每次都是不同的线程来请求相同的锁。这样偏向模式会失效，因此还不如不启用偏向锁。使用Java虚拟机参数-XX:+UseBiasedLocking可以开启偏向锁。 轻量级锁如果偏向锁失败，虚拟机并不会立即挂起线程。它还会使用一种称为轻量级锁的优化手段。轻量级锁的操作也很轻便，它只是简单地将对象头部作为指针，指向持有锁的线程堆栈的内部，来判断一个线程是否持有对象锁。如果线程获得轻量级锁成功，则可以顺利进入临界区。如果轻量级锁加锁失败，则表示其他线程抢先争夺到了锁，那么当前线程的锁请求就会膨胀为重量级锁。 自旋锁锁膨胀后，虚拟机为了避免线程真实地在操作系统层面挂起，虚拟机还会在做最后的努力——自旋锁。由于当前线程暂时无法获得锁，但是什么时候可以获得锁是一个未知数。也许在几个CPU时钟周期后，就可以得到锁。如果这样，简单粗暴地挂起线程可能是一种得不偿失的操作。因此，系统会进行一次赌注：它会假设在不久的将来，线程可以得到这把锁。因此，虚拟机会让当前线程做几个空循环（这也是自旋的含义），在经过若干次循环后，如果可以得到锁，那么就顺利进入临界区。如果还不能获得锁，才会真实地将线程在操作系统层面挂起。 锁消除锁消除是一种更彻底的锁优化。Java虚拟机在JIT编译时，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁。通过锁消除，可以节省毫无意义的请求锁时间。 说到这里，细心的读者可能会产生疑问，如果不可能存在竞争，为什么程序员还要加上锁呢？这是因为在Java软件开发过程中，我们必然会使用一些JDK的内置API，比如StringBuffer、Vector等。你在使用这些类的时候，也许根本不会考虑这些对象到底内部是如何实现的。比如，你很有可能在一个不可能存在并发竞争的场合使用Vector。而众所周知，Vector内部使用了synchronized请求锁。比如下面的代码： 注意上述代码中的Vector，由于变量v只在createStrings()函数中使用，因此，它只是一个单纯的局部变量。局部变量是在线程栈上分配的，属于线程私有的数据，因此不可能被其他线程访问。所以，在这种情况下，Vector内部所有加锁同步都是没有必要的。如果虚拟机检测到这种情况，就会将这些无用的锁操作去除。 锁消除涉及的一项关键技术为逃逸分析。所谓逃逸分析就是观察某一个变量是否会逃出某一个作用域。在本例中，变量v显然没有逃出createStrings()函数之外。以次为基础，虚拟机才可以大胆地将v内部的加锁操作去除。如果createStrings()返回的不是String数组，而是v本身，那么就认为变量v逃逸出了当前函数，也就是说v有可能被其他线程访问。如果是这样，虚拟机就不能消除v中的锁操作。 逃逸分析必须在-server模式下进行，可以使用-XX:+DoEscapeAnalysis参数打开逃逸分析。使用-XX:+EliminateLocks参数可以打开锁消除。"},{"title":"虚拟机类加载机制","date":"2019-08-06T16:00:00.000Z","url":"/2019/08/07/2019/08/070839/","tags":[["JVM","/tags/JVM/"]],"categories":[["Java","/categories/Java/"]],"content":"类加载机制 虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 加载、验证、准备、解析、初始化、使用、卸载。 加载下面是对于加载过程最为官方的描述。 加载阶段是类加载过程的第一个阶段。在这个阶段，JVM 的主要目的是将字节码从各个位置（网络、磁盘等）转化为二进制字节流加载到内存中，接着会为这个类在 JVM 的方法区创建一个对应的 Class 对象，这个 Class 对象就是这个类各种数据的访问入口。 其实加载阶段用一句话来说就是：把代码数据加载到内存中。这个过程对于我们解答这道问题没有直接的关系，但这是类加载机制的一个过程，所以必须要提一下。 验证当 JVM 加载完 Class 字节码文件并在方法区创建对应的 Class 对象之后，JVM 便会启动对该字节码流的校验，只有符合 JVM 字节码规范的文件才能被 JVM 正确执行。这个校验过程大致可以分为下面几个类型： JVM规范校验。JVM 会对字节流进行文件格式校验，判断其是否符合 JVM 规范，是否能被当前版本的虚拟机处理。例如：文件是否是以 0x cafe bene开头，主次版本号是否在当前虚拟机处理范围之内等。代码逻辑校验。JVM 会对代码组成的数据流和控制流进行校验，确保 JVM 运行该字节码文件后不会出现致命错误。例如一个方法要求传入 int 类型的参数，但是使用它的时候却传入了一个 String 类型的参数。一个方法要求返回 String 类型的结果，但是最后却没有返回结果。代码中引用了一个名为 Apple 的类，但是你实际上却没有定义 Apple 类。当代码数据被加载到内存中后，虚拟机就会对代码数据进行校验，看看这份代码是不是真的按照JVM规范去写的。这个过程对于我们解答问题也没有直接的关系，但是了解类加载机制必须要知道有这个过程。 准备（重点）当完成字节码文件的校验之后，JVM 便会开始为类变量分配内存并初始化。这里需要注意两个关键点，即内存分配的对象以及初始化的类型。 内存分配的对象。Java 中的变量有「类变量」和「类成员变量」两种类型，「类变量」指的是被 static 修饰的变量，而其他所有类型的变量都属于「类成员变量」。在准备阶段，JVM 只会为「类变量」分配内存，而不会为「类成员变量」分配内存。「类成员变量」的内存分配需要等到初始化阶段才开始。例如下面的代码在准备阶段，只会为 factor 属性分配内存，而不会为 website 属性分配内存。 public static int factor &#x3D; 3;public String website &#x3D; “www.cnblogs.com/chanshuyi“;初始化的类型。在准备阶段，JVM 会为类变量分配内存，并为其初始化。但是这里的初始化指的是为变量赋予 Java 语言中该数据类型的零值，而不是用户代码里初始化的值。例如下面的代码在准备阶段之后，sector 的值将是 0，而不是 3。 public static int sector &#x3D; 3;但如果一个变量是常量（被 static final 修饰）的话，那么在准备阶段，属性便会被赋予用户希望的值。例如下面的代码在准备阶段之后，number 的值将是 3，而不是 0。 public static final int number &#x3D; 3;之所以 static final 会直接被复制，而 static 变量会被赋予零值。其实我们稍微思考一下就能想明白了。 两个语句的区别是一个有 final 关键字修饰，另外一个没有。而 final 关键字在 Java 中代表不可改变的意思，意思就是说 number 的值一旦赋值就不会在改变了。既然一旦赋值就不会再改变，那么就必须一开始就给其赋予用户想要的值，因此被 final 修饰的类变量在准备阶段就会被赋予想要的值。而没有被 final 修饰的类变量，其可能在初始化阶段或者运行阶段发生变化，所以就没有必要在准备阶段对它赋予用户想要的值。 解析当通过准备阶段之后，JVM 针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符 7 类引用进行解析。这个阶段的主要任务是将其在常量池中的符号引用替换成直接其在内存中的直接引用。 其实这个阶段对于我们来说也是几乎透明的，了解一下就好。 初始化（重点）到了初始化阶段，用户定义的 Java 程序代码才真正开始执行。在这个阶段，JVM 会根据语句执行顺序对类对象进行初始化，一般来说当 JVM 遇到下面 5 种情况的时候会触发初始化： 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象的时候、读取或设置一个类的静态字段（被final修饰、已在编译器把结果放入常量池的静态字段除外）的时候，以及调用一个类的静态方法的时候。使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。当使用 JDK1.7 动态语言支持时，如果一个 java.lang.invoke.MethodHandle实例最后的解析结果 REF_getstatic,REF_putstatic,REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行初始化，则需要先出触发其初始化。看到上面几个条件你可能会晕了，但是不要紧，不需要背，知道一下就好，后面用到的时候回到找一下就可以了。 使用当 JVM 完成初始化阶段之后，JVM 便开始从入口方法开始执行用户的程序代码。这个阶段也只是了解一下就可以。 卸载当用户程序代码执行完毕后，JVM 便开始销毁创建的 Class 对象，最后负责运行的 JVM 也退出内存。这个阶段也只是了解一下就可以。 类初始化方法。编译器会按照其出现顺序，收集类变量的赋值语句、静态代码块，最终组成类初始化方法。类初始化方法一般在类初始化的时候执行。 对象初始化方法。编译器会按照其出现顺序，收集成员变量的赋值语句、普通代码块，最后收集构造函数的代码，最终组成对象初始化方法。对象初始化方法一般在实例化类对象的时候执行。 方法论从上面几个例子可以看出，分析一个类的执行顺序大概可以按照如下步骤： 确定类变量的初始值。在类加载的准备阶段，JVM 会为类变量初始化零值，这时候类变量会有一个初始的零值。如果是被 final 修饰的类变量，则直接会被初始成用户想要的值。 初始化入口方法。当进入类加载的初始化阶段后，JVM 会寻找整个 main 方法入口，从而初始化 main 方法所在的整个类。当需要对一个类进行初始化时，会首先初始化类构造器（），之后初始化对象构造器（）。 初始化类构造器。JVM 会按顺序收集类变量的赋值语句、静态代码块，最终组成类构造器由 JVM 执行。 初始化对象构造器。JVM 会按照收集成员变量的赋值语句、普通代码块，最后收集构造方法，将它们组成对象构造器，最终由 JVM 执行。 如果在初始化 main 方法所在类的时候遇到了其他类的初始化，那么就先加载对应的类，加载完成之后返回。如此反复循环，最终返回 main 方法所在类。 输出 "},{"title":"单例模式","date":"2019-08-06T16:00:00.000Z","url":"/2019/08/07/2019/08/singleton/","tags":[["Java","/tags/Java/"],["Singleton","/tags/Singleton/"]],"categories":[["Java","/categories/Java/"]],"content":"第一种（懒汉，线程不安全） 这种写法lazy loading很明显，但是致命的是在多线程不能正常工作。 第二种（懒汉，线程安全） 这种写法能够在多线程中很好的工作，而且看起来它也具备很好的lazy loading，但是，遗憾的是，效率很低，99%情况下不需要同步。 第三种（饿汉） 这种方式基于classloder机制避免了多线程的同步问题，不过，instance在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用getInstance方法， 但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化instance显然没有达到lazy loading的效果。 第四种（饿汉，变种） 表面上看起来差别挺大，其实更第三种方式差不多，都是在类初始化即实例化instance。 第五种（静态内部类） 这种方式同样利用了classloder的机制来保证初始化instance时只有一个线程，它跟第三种和第四种方式不同的是（很细微的差别）：第三种和第四种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果），而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。想象一下，如果实例化instance很消耗资源，我想让他延迟加载，另外一方面，我不希望在Singleton类加载时就实例化，因为我不能确保Singleton类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化instance显然是不合适的。这个时候，这种方式相比第三和第四种方式就显得很合理。 第六种（枚举） 这种方式是Effective Java作者Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象。 第七种（双重校验锁） 需要注意singleton采用volatile关键字修饰也是很有必要。singleton = new Singleton();这段代码其实是分为三步执行： 为singleton分配内存空间 初始化singleton 将singleton指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用getsingleton() 后发现singleton不为空，因此返回singleton，但此时singleton还未被初始化。 使用volatile可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行 总结有两个问题需要注意： 1.如果单例由不同的类装载器装入，那便有可能存在多个单例类的实例。假定不是远端存取，例如一些servlet容器对每个servlet使用完全不同的类装载器，这样的话如果有两个servlet访问一个单例类，它们就都会有各自的实例。 2.如果Singleton实现了java.io.Serializable接口，那么这个类的实例就可能被序列化和复原。不管怎样，如果你序列化一个单例类的对象，接下来复原多个那个对象，那你就会有多个单例类的实例。 对第一个问题修复的办法是： 对第二个问题修复的办法是： 使用场景需要频繁的进行创建和销毁的对象,创建对象时耗时过多或者耗费资源过多(重量级对象),但又经常用到的对象,工具类对象,频繁访问数据库或者文件的对象(数据源,session工厂等)."},{"title":"Java虚拟机：JVM","date":"2019-08-04T16:00:00.000Z","url":"/2019/08/05/2019/08/051409/","tags":[["JVM","/tags/JVM/"]],"categories":[["Java","/categories/Java/"]],"content":" JVM的主要组成部分及其作用 类加载器-ClassLoader 运行时数据区-Runtime Data Area 执行引擎-Execution Engine 本地库接口-Native Interface 组件的作用： 首先通过类加载器（ClassLoader）会加载类文件到内存，Class loader只管加载，只要符合文件结构就加载。运行时数据区（Runtime Data Area)是jvm的重点，我们所有所写的程序都被加载到这里，之后才开始运行。而字节码文件只是 JVM 的一套指令集规范，并不能直接交个底层操作系统去执行，因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来融合不同的语言为java所用,从而实现整个程序的功能 内存区域 程序计数器&emsp;&emsp;程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是**当前线程所执行的字节码的行号指示器。**在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 指向当前线程执行的字节码指令的地址（ 行号 ）。 这样做的用处是多线程操作时， 挂起的线程在重新激活后能够知道上次执行的位置。 &emsp;&emsp;由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 &emsp;&emsp;如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java虚拟机栈&emsp;&emsp;与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：**每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。**每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。&emsp;&emsp;局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 &emsp;&emsp;其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 &emsp;&emsp;在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈&emsp;&emsp;本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。在虚拟机规范中对本地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 Java堆&emsp;&emsp;对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有的对象实例以及数组都要在堆上分配[1]，但是随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换[2]优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么“绝对”了。 &emsp;&emsp;Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”（Garbage Collected Heap，幸好国内没翻译成“垃圾堆”）。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer,TLAB）。不过无论如何划分，都与存放内容无关，无论哪个区域，存储的都仍然是对象实例，进一步划分的目的是为了更好地回收内存，或者更快地分配内存。在本章中，我们仅仅针对内存区域的作用进行讨论，Java堆中的上述各个区域的分配、回收等细节将是第3章的主题。 &emsp;&emsp;根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 堆和栈的区别 Java堆与Java虚拟机栈的区别在于：Java堆是所有线程共享的，而Java虚拟机栈是每个线程私有的。 栈内存的生命周期与线程相同，而堆内存的生命周期则与虚拟机相同。 栈内存存放的变量生命周期一旦结束就会被释放，而堆内存存放的实体会被垃圾回收机制不定时的回收。、 栈内存存储的是局部变量、方法参数、返回值、异常信息等数据，而堆内存存储的是对象实例、数组、类信息等数据。 栈内存的更新速度快，而堆内存的更新速度慢，因为局部变量的生命周期很短，而堆内存的生命周期相对较长。 方法区&emsp;&emsp;方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 &emsp;&emsp;Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说，这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是必要的。在Sun公司的BUG列表中，曾出现过的若干个严重的BUG就是由于低版本的HotSpot虚拟机对此区域未完全回收而导致内存泄漏。 &emsp;&emsp;根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 Java虚拟机对Class文件每一部分（自然也包括常量池）的格式都有严格规定，每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、装载和执行，但对于运行时常量池，Java虚拟机规范没有做任何细节的要求，不同的提供商实现的虚拟机可以按照自己的需要来实现这个内存区域。不过，一般来说，除了保存Class文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池中[1]。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现，所以我们放到这里一起讲解。 在JDK 1.4中新加入了NIO（New Input&#x2F;Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I&#x2F;O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 显然，本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存（包括RAM以及SWAP区或者分页文件）大小以及处理器寻址空间的限制。服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现OutOfMemoryError异常。 对象的内存布局在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 对象的访问定位建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。目前主流的访问方式有使用句柄和直接指针两种。 垃圾收集器与内存分配 GC的历史比Java久远，1960年诞生于MIT的Lisp是第一门真正使用内存动态分配和垃圾收集技术的语言。 判定对象死亡引用计数算法给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。 可达性分析算法通过一系列的称为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。 GCRoots在Java语言中，可作为GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI（即一般说的Native方法）引用的对象。 Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（NullPointExection、OutOfMemoryError）等，还有系统类加载器。 所有被同步锁（Synchronized关键字）持有的对象 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象‘临时性’地加入，共同构成完成的GC Roots集合，比如：分代收集和局部回收（Partial GC）。 由于Root采用栈方式存放变量和指针，所以如果一个指针，它保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个Root 自我拯救 任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行 引用类型 强 普遍存在的引用赋值 软 一些还有用，但非必要，内存不够时回收 弱 非必须，遇到就回收 虚 在对象被回收时收到一个系统通知 回收方法区永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 垃圾收集算法 垃圾收集器主要介绍CMS和G1 CMS初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 优缺点： CMS是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿，Sun公司的一些官方文档中也称之为并发低停顿收集器（Concurrent Low Pause Collector）。但是CMS还远达不到完美的程度，它有以下3个明显的缺点： CMS收集器对CPU资源非常敏感。 CMS收集器无法处理浮动垃圾（Floating Garbage），可能出现”Concurrent Mode Failure”失败而导致另一次Full GC的产生。 CMS是一款基于“标记—清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。 G1 特点 并行与并发 分代收集 空间整合，不会产生内存碎片 可预测的停留 G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。 在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。 G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 在G1收集器中，Region之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，虚拟机都是使用Remembered Set（Rset，记忆集）来避免全堆扫描的。G1中每个Region都有一个与之对应的Remembered Set，虚拟机发现程序在对Reference类型的数据进行写操作时，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之中（在分代的例子中就是检查是否老年代中的对象引用了新生代中的对象），如果是，便通过CardTable把相关引用信息记录到被引用对象所属的Region的Remembered Set之中。当进行内存回收时，在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会有遗漏。 步骤 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Data Counting and Evacuation） 初始标记:仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。借用minorGC时同步完成。 并发标记:从GC Root开始对堆中对象进行可达性分析，递归扫描整堆的对象图，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完以后，还要重新处理SATB记录下的在并发时有引用变动的对象。 最终标记:阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。用于处理并发阶段结束后仍遗留下来的最后那少量的SATB记录。 筛选回收:阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划.涉及对象的移动，必须暂停用户线程，由多条收集线程并行执行。 与CMS相比 G1不会产生内存碎片 可以预测的停顿时间 CMS采用增量更新，G1采用原始快照（SATB）保证并发标记不干扰用户线程 如果回收的速度赶不上分配的速度，G1也会FullGC JVM参数JVM初始分配的内存由-Xms指定，默认是物理内存的1&#x2F;64；JVM最大分配的内存由-Xmx指定，默认是物理内存的1&#x2F;4。默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC后调整堆的大小。 标配参数 x参数 XX参数 jvm常用参数"},{"title":"Class类文件的结构","date":"2019-08-04T16:00:00.000Z","url":"/2019/08/05/2019/08/051931/","tags":[["JVM","/tags/JVM/"]],"categories":[["Java","/categories/Java/"]],"content":"文件结构"},{"title":"伞下铭","date":"2019-08-02T16:00:00.000Z","url":"/2019/08/03/2019/08/032056/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":"一个神奇的周末&emsp;&emsp;之所以神奇，是因为他的天气。今天的天气真的很是奇怪，中午没有下去吃饭，点的外卖，吃完准备午休的时候，打开窗，迎面而来的不是热浪，而是暴雨（因为拉了窗帘，所以一开始并不知道外面在下雨。）。早上出发的时候，天猫精灵说是有雨的，我是不信的，你看外面的太阳，他会不会放过我？还真怕会被晒化，整个手臂已经是两种颜色了，这个天气。&emsp;&emsp;时间来到了下午，这时候打开窗户，天空湛蓝，云彩稀疏，真不知道前面的雨哪来的。神奇的是过了一会又下起了暴雨，地面都开始积水了，等到下班，然而，地面已经彻底干透，甚至还有一丝丝热气上升。&emsp;&emsp;气温并没有因为这两场雨而下降，你甚至感觉不到他们留下的任何痕迹，只有蓝蓝的天空和洁白的云彩。&emsp;&emsp;希望是一个愉快的周末。"},{"title":"MySql:too many connections","date":"2019-08-02T16:00:00.000Z","url":"/2019/08/03/2019/08/031349/","tags":[["MySQL","/tags/MySQL/"]],"categories":[["work","/categories/work/"]],"content":"Ubuntu18.04下MySQL出现”too many connections”问题的解决 查看MySQL版本 MySQL对应的配置文件位置注意，该系统环境下配置文件位于/etc/mysql/my.cnf 配置文件 重启MySQL服务"},{"title":"docker部署web项目","date":"2019-07-31T16:00:00.000Z","url":"/2019/08/01/2019/08/010913/","tags":[["docker","/tags/docker/"],["web","/tags/web/"]],"categories":[["Java","/categories/Java/"]],"content":" 项目分析&emsp;&emsp;项目数量较多,需要使用多个tomcat,并且每个tomcat下的项目数量也比较多。目前项目改动可能较为频繁，不能使用打包成docker镜像的方式，这里选择挂载数据卷。&emsp;&emsp;需要用到多个镜像服务，nginx,tomcat,MySQL,Redis等必须，所以采用docker-compose的方式。 环境准备系统环境 软件环境需要安装docker和docker-compose验证docker-compose 目录 docker-compose注意，使用空格，不要使用tab host配置 nginx.conf MySQL和Redis配置MySQL Redis 启动 停止 查看 docker重新加载nginx.conf 配置文件未同步更新原因 将宿主机配置文件挂载到容器，在宿主机修改配置文件，但容器内挂载的文件并未发生变化，直至docker重启。 docker挂载文件基于inode。vim等编辑工具保存文件时，并非直接保存，而是将一份新的临时文件覆盖了旧文件。对于inode而言，原文件并未被修改。 解决方案： 换用nano、cat等直接更新文件的编辑工具 改为挂载目录。，注意这里不要直接挂载到nginx目录，因为该目录下还有nginx运行必须的其他文件。 修改vim配置，添加：set backupcopy&#x3D;yes 查看inode stat nginx.conf ls -i nginx.conf "},{"title":"七月","date":"2019-07-30T16:00:00.000Z","url":"/2019/07/31/2019/07/312125/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 没有任何回忆的七月&emsp;&emsp;真的没有任何会议嘛，当前不是，草灰蛇线，雪泥鸿爪。总归会留下些什么。但要是说值得纪念的，则是没有什么了。&emsp;&emsp;记流水账什么的就放到自己的日记里吧，这里还是算了。学到了一些东西，也有一些收获感悟，更多的是为生活困顿的挣扎。要多读书，明理，切不可自以为是，变成自己讨厌的那种人。 &emsp;&emsp;想起了经常会用错的一个成语–七月流火。虽然确实很热，但是真的与之无关啊。不过看来已经改了。 七月流火是一个汉语成语，拼音是qī yuè liú huǒ，出自《诗经》，指大火星(心宿二，天蝎座主星)西行，最终逐渐离开天际，古人以此作为夏去秋来，天气转凉的标志。第七版现代汉语词典已修改：也可形容天气炎热。 &emsp;&emsp;有生命力的中华文化。&emsp;&emsp;找到了一个很好的电子书网站，下载了很多书籍，好好充电吧。工作上还是那样吧，搬了新的办公室。希望越来越好。 "},{"title":"vsCode的git提交出现ssh_askpass错误","date":"2019-07-29T16:00:00.000Z","url":"/2019/07/30/2019/07/301705/","tags":[["vscode","/tags/vscode/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 如果出现askpass的相关错误，请安装x11-ssh-askpass，然后在提交的确认框中输入yes,点击ok即可。 Arch Linux "},{"title":"Manjaro多壁纸设置","date":"2019-07-25T16:00:00.000Z","url":"/2019/07/26/2019/07/261026/","tags":[["manjaro","/tags/manjaro/"],["wallpaper","/tags/wallpaper/"]],"categories":[["Linux","/categories/Linux/"]],"content":" 2020-05-23 更新 选取两张和显示器分辨率一致的壁纸图片 使用GIMP新建图像，分辨率为两个图片的和 将两张图片拼接（左右拼接） 在优化外观中选择背景方案wallpaper 使用两个显示器，就想在显示器上设置不同的壁纸，这里简单介绍一下如何设置。 系统信息 主要和使用的桌面环境有关，使用 GNOME 桌面环境为 Linux 发行版上的每个显示器设置不同的壁纸 将使用一个名为 HydraPaper 的小工具在不同的显示器上设置不同的背景。HydraPaper 是一个基于 GTK 的应用，用于为 GNOME 桌面环境中的每个显示器设置不同的背景。 它还支持 MATE 和 Budgie 桌面环境。 使用 FlatPak 在 Linux 上安装 HydraPaperManjaro默认已经安装了FlatPak，如果没有，请在软件中心下载安装， HydraPaper网站地址 安装命令 Run 然后就可以在里面自行设置了。 注意HydraPaper 的一大缺点在于它的设计工作方式。你可以看到，HydraPaper将你选择的壁纸拼接成一张图像并将其拉伸到屏幕上，给人的印象是每个显示器上都有不同的背景。当你移除外部显示器时，这将成为一个问题。"},{"title":"JMM","date":"2019-07-22T16:00:00.000Z","url":"/2019/07/23/2019/07/jmm/","tags":[["JMM","/tags/JMM/"]],"categories":[["Java","/categories/Java/"]],"content":" &emsp;&emsp;JMM,Java的内存模型。 原子性（Atomicity） 原子性是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 注意，对于32位系统来说，long型数据的读写不是原子性的，因为long有64位。 可见性（Visibility） 可见性是指当一个线程修改了某一个共享变量的值，其他线程是否能够立即知道这个修改。显然，对于串行程序来说，可见性问题是不存在的。因为你在任何一个操作步骤中修改了某个变量，那么在后续的步骤中，读取这个变量的值，一定是修改后的新值。 &emsp;&emsp;但是这个问题在并行程序中就不见得了。如果一个线程修改了某一个全局变量，那么其他线程未必可以马上知道这个改动。图1.14展示了发生可见性问题的一种可能。如果在CPU1和CPU2上各运行了一个线程，它们共享变量t，由于编译器优化或者硬件优化的缘故，在CPU1上的线程将变量t进行了优化，将其缓存在cache中或者寄存器里。这种情况下，如果在CPU2上的某个线程修改了变量t的实际值，那么CPU1上的线程可能并无法意识到这个改动，依然会读取cache中或者寄存器里的数据。因此，就产生了可见性问题。外在表现为：变量t的值被修改，但是CPU1上的线程依然会读到一个旧值。可见性问题也是并行程序开发中需要重点关注的问题之一。&emsp;&emsp;除了上述提到的缓存优化或者硬件优化（有些内存读写可能不会立即触发，而会先进入一个硬件队列等待）会导致可见性问题外，指令重排（这个问题将在下一节中更详细讨论）以及编辑器的优化，都有可能导致一个线程的修改不会立即被其他线程察觉。 有序性（Ordering）&emsp;&emsp;有序性问题可能是三个问题中最难理解的了。对于一个线程的执行代码而言，我们总是习惯地认为代码的执行是从先往后，依次执行的。这么理解也不能说完全错误，因为就一个线程内而言，确实会表现成这样。但是，在并发时，程序的执行可能就会出现乱序。给人直观的感觉就是：写在前面的代码，会在后面执行。听起来有些不可思议，是吗？有序性问题的原因是因为程序在执行时，可能会进行指令重排，重排后的指令与原指令的顺序未必一致。 &emsp;&emsp;假设线程A首先执行writer()方法，接着线程B执行reader()方法，如果发生指令重排，那么线程B在代码第10行时，不一定能看到a已经被赋值为1了。&emsp;&emsp;这确实是一个看起来很奇怪的问题，但是它确实可能存在。注意：我这里说的是可能存在。因为如果指令没有重排，这个问题就不存在了，但是指令是否发生重排、如何重排，恐怕是我们无法预测的。因此，对于这类问题，我认为比较严谨的描述是：线程A的指令执行顺序在线程B看来是没有保证的。如果运气好的话，线程B也许真的可以看到和线程A一样的执行顺序。&emsp;&emsp;不过这里还需要强调一点，对于一个线程来说，它看到的指令执行顺序一定是一致的（否则的话我们的应用根本无法正常工作，不是吗？）。也就是说指令重排是有一个基本前提的，就是保证串行语义的一致性。指令重排不会使串行的语义逻辑发生问题。因此，在串行代码中，大可不必担心。 指令重排&emsp;&emsp;注意：指令重排可以保证串行语义一致，但是没有义务保证多线程间的语义也一致。&emsp;&emsp;那么，好奇的你可能马上就会在脑海里闪出一个疑问，为什么要指令重排呢？让他一步一步执行多好呀！也不会有那么多奇葩的问题。&emsp;&emsp;之所以那么做，完全是因为性能考虑。我们知道，一条指令的执行是可以分为很多步骤的。简单地说，可以分为以下几步： 取指IF 译码和取寄存器操作数ID 执行或者有效地址计算EX 存储器访问MEM 写回WB &emsp;&emsp;我们的汇编指令也不是一步就可以执行完毕的，在CPU中实际工作时，它还是需要分为多个步骤依次执行的。当然，每个步骤所涉及的硬件也可能不同。比如，取指时会用到PC寄存器和存储器，译码时会用到指令寄存器组，执行时会使用ALU，写回时需要寄存器组。&emsp;&emsp;注意：ALU指算术逻辑单元。它是CPU的执行单元，是CPU的核心组成部分，主要功能是进行二进制算术运算。&emsp;&emsp;由于每一个步骤都可能使用不同的硬件完成，因此，聪明的工程师们就发明了流水线技术来执行指令，如下所示，显示了流水线的工作原理。 &emsp;&emsp;可以看到，当第2条指令执行时，第1条执行其实并未执行完，确切地说第一条指令还没开始执行，只是刚刚完成了取值操作而已。这样的好处非常明显，假如这里每一个步骤都需要花费1毫秒，那么指令2等待指令1完全执行后，再执行，则需要等待5毫秒，而使用流水线后，指令2只需要等待1毫秒就可以执行了。如此大的性能提升，当然让人眼红。更何况，实际的商业CPU的流水线级别甚至可以达到10级以上，则性能提升就更加明显。&emsp;&emsp;流水线带来性能的提升，但是一旦中断，再次满载的话耗费时间，损失性能。指令重排的原因就是尽量减少中断流水线，这只是其中一种技术。 &emsp;&emsp;由于R2中的数据还没有准备好，ADD在这里会中断，后面的指令都会慢一拍。 哪些指令不能重排：Happen-Before规则&emsp;&emsp;在前文已经介绍了指令重排，虽然Java虚拟机和执行系统会对指令进行一定的重排，但是指令重排是有原则的，并非所有的指令都可以随便改变执行位置，以下罗列了一些基本原则，这些原则是指令重排不可违背的。 程序顺序原则：一个线程内保证语义的串行性 volatile规则：volatile变量的写，先发生于读，这保证了volatile变量的可见性 锁规则：解锁（unlock）必然发生在随后的加锁（lock）前 传递性：A先于B，B先于C，那么A必然先于C 线程的start()方法先于它的每一个动作 线程的所有操作先于线程的终结（Thread.join()） 线程的中断（interrupt()）先于被中断线程的代码 对象的构造函数执行、结束先于finalize()方法 以程序顺序原则为例，重排后的指令绝对不能改变原有的串行语义。比如： &emsp;&emsp;由于第2条语句依赖第一条的执行结果。如果冒然交换两条语句的执行顺序，那么程序的语义就会修改。因此这种情况是绝对不允许发生的。因此，这也是指令重排的一条基本原则。&emsp;&emsp;此外，锁规则强调，unlock操作必然发生在后续的对同一个锁的lock之前。也就是说，如果对一个锁解锁后，再加锁，那么加锁的动作绝对不能重排到解锁动作之前。很显然，如果这么做，加锁行为是无法获得这把锁的。&emsp;&emsp;其他几条原则也是类似的，这些原则都是为了保证指令重排不会破坏原有的语义结构。 Volatile volatile美 [‘vɑlət(ə)l]英 [‘vɒlətaɪl] 内存可见性 防止指令重排 volatile修饰符适用于以下场景：某个属性被多个线程共享，其中有一个线程修改了此属性，其他线程可以立即得到修改后的值。 "},{"title":"坤伦决","date":"2019-07-21T16:00:00.000Z","url":"/2019/07/22/2019/07/072221/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" &emsp;&emsp;“不蒸馒头争口气”，过去几日里，“华语天王”周杰伦被网友质疑“数据差没流量”，气得一众“夕阳红老粉丝”跳脚，与蔡徐坤粉丝激战16小时，上演一场与流量明星对飙数据的“行为艺术”。&emsp;&emsp;最终战场上只有两只队伍，一只是蔡徐坤粉丝战队，另一只是其他人战队。&emsp;&emsp;全网民统一战线。&emsp;&emsp;以上内容摘抄自网络。&emsp;&emsp;的确，我虽然不追星，也不关注微博，但还是被这条消息给轰炸了，二者选其一，那肯定是我杰伦了。鸡你太美，实在喜欢不起来。结局也不出意外，杰伦登上了紫禁城之颠，俯瞰全场。&emsp;&emsp;还有种说法是微博在背后策划的，这样的话，怕不是药丸？"},{"title":"使用VuePress搭建个人博客","date":"2019-07-21T16:00:00.000Z","url":"/2019/07/22/2019/07/build_blog/","tags":[["daily","/tags/daily/"],["vuepress","/tags/vuepress/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 效果图 安装yarnyarn地址 在使用 Yarn 之前，首先要在您的系统上安装 Yarn 。 以下列出了多种不同的安装方式，并且新的安装方式仍在继续添加： 操作系统: Arch LinuxOn Arch Linux, Yarn can be installed through the official package manager. Path Setup如果未在 PATH 环境变量中找到 yarn，请按照以下步骤添加 yarn 到 PATH 环境变量中，使其可以随处运行。 注意：您的配置文件可能是 .profile、.bash_profile、.bashrc、.zshrc 等。 将此项加入您的配置文件： export PATH=&quot;$PATH:/opt/yarn-[version]/bin&quot; （路径可能根据您安装 Yarn 的位置而有差异） 在终端中，执行登录并登出以使更改生效 为了可以全局访问 Yarn 的可执行文件，你需要在您的终端中设置 PATH 环境变量。若要执行此操作，请添加 export PATH=&quot;$PATH:yarn global bin&quot; 到您的配置文件中。 通过如下命令测试 Yarn 是否安装成功： 如果是Windows系统，请选择对应的系统 Windows在 Windows 系统中有三种安装 Yarn 的方式。 下载安装程序你将下载到一个 .msi 文件，当它运行时会指引你将 Yarn 安装到 Windows 上。 如果你使用此安装程序，需要先安装 Node.js。 下载安装程序 通过 Chocolatey 安装Chocolatey 是一个 Windows 专用的软件包管理工具。 请按照此 说明 安装 Chocolatey 。 安装 Chocolatey 之后，你就可以在控制台执行如下命令安装 Yarn 了： 这也会确保你安装了 Node.js 。 通过 Scoop 安装Scoop 是一个用于 Windows 的基于命令行的安装工具。 请按照此 说明 安装 Scoop 。 Scoop 安装后，你就可以在控制台执行如下命令安装 Yarn 了： 如果 Node.js 未被安装，scoop 会提示你安装。 例如： 注意请将您的项目目录和 Yarn 的缓存目录 (%LocalAppData%\\Yarn) 列入杀毒软件的白名单中，否则会因为每次向磁盘写入文件时而被扫描，导致安装软件包变得很慢。 通过如下命令测试 Yarn 是否安装成功： VuePressVuePress中文网 全局安装如果你只是想随便用下 VuePress，你可以在全局安装它： 在已有项目中安装如果你想要在一个已有项目中维护文档，就应该将 VuePress安装为本地依赖。此设置还允许你使用 CI 或 Netlify 服务，在推送时自动部署。 或者，给 package.json 添加一些 scripts 脚本： 然后你就可以开始编写文档了： 要生成静态资源，请运行： 默认情况下，构建的文件会位于 .vuepress/dist 中，该文件可以通过 .vuepress/config.js 中的 dest 字段进行配置。构建的文件可以部署到任何静态文件服务器。关于如何部署到一些常用服务，请参考 部署指南。 使用主题vuepress-theme-reco # vuepress-theme-reco-cli Blog generation tool for vuepress-theme-reco ## Version |Version|VuePress|VuePress-theme-reco| |-|:-:|:-:| |0.x|0.x|0.x| |1.x|1.x|1.x| ## Experience *if yarn* 以上完成之后，你将会获得一个基础的博客网站，接下来是相关替换改造，选择一个IDE或者文本编辑器打开文件夹。 比如VSCode 文件结构目录如下： ![23g](批注 2019-07-22 221844.png) 修该配置文件，位于.vuepress/config.js,.vuepress/public文件夹为图片资源。 主题作者已经做了详细的注释在配置文件中，以下可以修改的部分 部署 首先需要创建一个使用用户名创建的仓库,例如： GitHub 页面 将 .vuepress/config.js 中的 base 设置为你的仓库名称。例如，如果你的仓库是  ，则已部署的页面将在  上可用。在这种情况下，你应该将base设置为 &quot;/bar/&quot; 。 在你的项目中，运行： 你可以在 CI 设置中运行此脚本以启用每次推送时的自动部署。 访问&lt;USERNAME&gt;.github.io即可看到效果。"},{"title":"一个普通的周末","date":"2019-07-20T16:00:00.000Z","url":"/2019/07/21/2019/07/211821/","tags":[["daily","/tags/daily/"]],"categories":[["wuw","/categories/wuw/"]],"content":" &emsp;&emsp;一周一次的休息时间，转瞬即逝。&emsp;&emsp;上海终于进入了火炉般的夏天，之前气温正好，微风不燥。刚有点热浪袭来的时候，就开始下雨，导致一度入夏失败。&emsp;&emsp;现在是真的很热了，鼓噪不安的知了也在宣告着夏天的热烈。大好的太阳啊，于是今天就把一些冬天穿的衣服做了洗晒，要么怎么说太阳毒辣呢，将近中午的时候才洗好的，下去去收凉薄衣服的时候，顺带摸了一下，已近基本干了，明天也是个大晴天，再晒一天，应该就可以收纳起来了。&emsp;&emsp;其中有一个插曲是，我把厚的衣服收进一个纸箱子里面了，不知道什么时候有只小强爬进去了，估计是当成自己的新家了，今天拿去洗顺带也把它给惊起来，爬到洗衣机的阳台上去了。&emsp;&emsp;一直觉得自己好像本周打过电话回家了，然后往往就是没有打电话回家，这不，老头亲自打电话过来询问我了，怎么没有打电话回去呢。&emsp;&emsp;虽然每周一次的打电话回家频率有些低了，但是毕竟是从初中开始就养成的习惯了，你要是隔三岔五的打电话回家，这个时候家人就该想了，你是不是在外面有什么事情呀？虽然我一再强调说真的没啥，他们还是不相信，结果就是其他家人也都知道了，爸爸打电话过来询问，还说要是和他们不方便说，可以找我哥哥谈谈也行。哈哈哈。真的是不能随便更改频率呀，毕竟都习惯了。不同的是，我的同事们还有一些其他的人，要么基本不给家里人打电话，要么是想起来就打，没有任何规律可言。也会面临一些苦笑不得的问题。&emsp;&emsp;A同事是基本不打电话回去的那种，都是家里人直接打电话过来的。有时候听听不同地方的方言还是很有趣的。如果你发现他主动打电话回去了，好吧，那可能是有事情了，缺钱或者是其他的事情了。B就是每天都要打电话的那种了，每次吃完晚饭都会和家里人开视频。&emsp;&emsp;如果可以的话，谁也不想背井离乡，跑到这千里之外的地方希望早日实现自己的理想，和家人团聚。"},{"title":"tomcat","date":"2019-07-19T16:00:00.000Z","url":"/2019/07/20/2019/07/tomcat/","tags":[["tomcat","/tags/tomcat/"],["docker","/tags/docker/"]],"categories":[["work","/categories/work/"]],"content":" 基本部署地址 下载之后解压 将war包放到webapps下 配置文件在&lt;tomcat&gt;/conf下，端口配置在server.xml 启动 &lt;tomcat&gt;/bin下的 startup.sh 开启远程管理版本为tomcat9.* 首先添加用户权限，编辑 &lt;tomcat&gt;/conf/tomcat-users.xml 注意对于tomcat9来说，不能同时赋予用户manager-script和manager-gui角色。 因为管理webapp只允许本地管理,远程登陆不可以 修改&lt;tomcat&gt;/webapps/manager/META-INF/context.xml,第一个方法是注释掉地址限制 第二个方法，添加允许的地址\\d+\\.\\d+\\.\\d+\\.\\d+ docker部署开发环境查找容器 拉取镜像 启动 成功后可以访问&lt;ip&gt;:8888 挂载执行 如果是部署多个web项目，这里可以选择挂载目录即可，例如： 生产环境制作新的镜像vim Dockerfile 生成新的镜像 启动新的镜像 遇到的问题以及解决方案tomcat的版本，9.*版本中某些配置文件做了改动"},{"title":"Flask","date":"2019-07-18T16:00:00.000Z","url":"/2019/07/19/2019/07/191033/","tags":[["flask","/tags/flask/"],["python","/tags/python/"]],"categories":[["Python","/categories/Python/"]],"content":" 图片分类系统训练完成，需要部署成web项目，对外提供接口服务 Flask简介 简要介绍如何使用Flask框架及其一些扩展开发Web程序 1.1 使用虚拟环境安装Flask最便捷的方式是使用虚拟环境。虚拟环境是Python解释器的一个私有副本，在这个环境中你可以安装私有包，而且不会影响系统中安装的全局Python解释器。 虚拟环境非常有用，可以在系统的Python解释器中避免包的混乱和版本的冲突。为每个程序单独创建虚拟环境可以保证程序只能访问虚拟环境中的包，从而保持全局解释器的干净整洁，使其只作为创建（更多）虚拟环境的源。使用虚拟环境还有个好处，那就是不需要管理员权限。 使用conda创建虚拟环境PyCharm创建项目过程中选择conda会自动创建虚拟环境 运行产品服务器当运行公开服务器而不是进行开发的时候，应当不使用内建的开发服务器 （ flask run ）。开发服务器由 Werkzeug 提供，目的是为了方便开发，但是 不够高效、稳定和安全。 替代地，应当选用一个产品级的 WSGI 服务器。例如，使用 Waitress 。首先在 虚拟环境中安装它： 需要把应用告知 Waitree ，但是方式与 flask run 那样使用 FLASK_APP 不同。需要告知 Waitree 导入并调用应用工厂来得到一个应用对象。 Serving on  以多种不同方式部署应用的列表参见 部署方式 。使用 Waitress 只是一个示例，选择它是因为它同时支持 Windows 和 Linux 。还有其他许多 WSGI 服务器和部署选项可供选择。 Gunicorn (Green Unicorn) 是一个 Python WSGI HTTP 服务器，适用于部署 Flask 应用。以下是使用 Gunicorn 部署 Flask 应用的基本步骤： 安装 Gunicorn:使用以下命令安装 Gunicorn： 启动应用:在你的 Flask 应用目录中，使用以下命令启动 Gunicorn： 通过 Gunicorn 配置文件启动:你还可以使用一个 Gunicorn 配置文件来配置服务器。创建一个名为 gunicorn_config.py 的文件，内容如下： 然后使用以下命令启动 Gunicorn： 这样可以更方便地管理配置。 通过 systemd 启动:如果你使用的是 Linux 系统，你可以使用 systemd 将 Gunicorn 作为服务运行。创建一个名为 your_app.service 的文件，内容如下： 替换 your_username、your_group、&#x2F;path&#x2F;to&#x2F;your&#x2F;app、&#x2F;path&#x2F;to&#x2F;venv&#x2F;bin&#x2F;gunicorn 和 your_app:app。 然后使用以下命令启动服务： 以及开机自启动： 这些步骤将会启动 Gunicorn 服务器并在生产环境中运行你的 Flask 应用。确保在生产环境中采取其他必要的安全措施，例如设置适当的防火墙规则、配置 SSL&#x2F;TLS、使用适当的代理服务器等。"},{"title":"Anaconda","date":"2019-07-18T16:00:00.000Z","url":"/2019/07/19/2019/07/anaconda/","tags":[["python","/tags/python/"],["anaconda","/tags/anaconda/"]],"categories":[["work","/categories/work/"]],"content":" 安装Anaconda网址 下载地址 清华开源镜像清华开源镜像地址 选择对应的版本，以下是Linux系统环境安装 验证 输出 如果没有，需要配置环境变量，在终端输入sudo vim /etc/profile，打开profile文件。添加语句export PATH=/home/congco/anaconda3/bin:$PATH(注意：这里是自己本机的安装路径)，保存，退出。 ~~~~這裏會引起一個問題，就是如果是gnome桌面環境的話，會導致gnome-tweaks 啓動失敗，參考博客解決方案就是刪除上面的環境變量，使用conda init，如果使用安裝過程中的初始化，則沒有這個問題使用conda init初始化環境變量信息 这样的话，启动终端，每次都会默认激活base环境，建议还是在自己的终端中添加对应的环境变量，而不是在系统中全局添加。bash在~/.bashrc,zsh在~/.zshrc中。 国内镜像vim ~/.condarc添加以下内容 卸载Anaconda由于Anaconda在Linux下是安装在一个文件夹里&#x2F;root&#x2F;anaconda ,如果安装过程中出错问题，或者想更新另一个版本，删除anaconda也很方便，执行下面命令 镜像使用帮助 内容来自清华大学开源镜像站—-start—- Anaconda 镜像使用帮助Anaconda 是一个用于科学计算的 Python 发行版，支持 Linux, Mac, Windows, 包含了众多流行的科学计算、数据分析的 Python 包。 Anaconda 安装包可以到  下载。 TUNA 还提供了 Anaconda 仓库的镜像，运行以下命令: 即可添加 Anaconda Python 免费仓库。 运行 conda install numpy 测试一下吧。 Miniconda 镜像使用帮助Miniconda 是一个 Anaconda 的轻量级替代，默认只包含了 python 和 conda，但是可以通过 pip 和 conda 来安装所需要的包。 Miniconda 安装包可以到  下载。 Conda 三方源当前tuna还维护了一些anaconda三方源。 Conda Forge msys2 bioconda menpo pytorch 其他三方源对于conda的其他三方源，如有需要请在这个issue中提出请求，我们会综合考虑多方因素来酌情增减。 —————end——————– "},{"title":"蒸虾有感","date":"2019-07-15T16:00:00.000Z","url":"/2019/07/16/2019/07/0716/","tags":[["cook","/tags/cook/"]],"categories":[["wuw","/categories/wuw/"]],"content":" 有的时候，还是需要控制自己的情绪才行。今天去老贺那里蹭饭了。到了菜市场买菜，买了喜欢吃的青椒鸡蛋，还有就是虾，其他的我就不关心了。因为过年的时候，有做过一次蒜蓉蒸虾，这次也想露一手。最困难的就是初级处理了吧，需要把虾开背，这个虾，它还是活得，在盆里跳动，杀生，对我来说有点残忍了。然而还是下手了。生命真是顽强啊，为了减轻痛苦，后面直接用开水烫死了。。。然后加生抽，盐，味精腌制一会儿。由于虾买的多，一个盘子装不下，就换了个大点的盘子，装了两层，哈哈哈哈。问题也在这里了，盘子厚，虾也叠的厚了。后面很难蒸熟，等了好久，大家都饿了。结局还是很好吃的，有点饱。"},{"title":"天命风流","date":"2019-07-15T16:00:00.000Z","url":"/2019/07/16/2019/07/1009/","tags":[["person","/tags/person/"],["music","/tags/music/"]],"categories":[["congco","/categories/congco/"]],"content":" 播放列表网易云音乐 "},{"title":"收藏夹","date":"2019-07-15T16:00:00.000Z","url":"/2019/07/16/2019/07/1350/","tags":[["person","/tags/person/"],["soft","/tags/soft/"]],"categories":[["congco","/categories/congco/"]],"content":" 壁纸 Wallpaper Engine 人工桌面 wallhaven 高清免费图片 高清免费图片 2 音乐 listen1 Foobar2000 musicxfox 笔记 onenote typora PicGo(图片上传) 视频 Mpc-be mpv VLC iina 浏览器 Chrome Firefox Edge 电子书 Calibre 图书：Mac自带 SoBooks 强大的电子书资源网站 优书网 阅读 下载工具 motrix bt 迅雷 截图 Flameshot Sharex Snipaste 远程桌面 Remmina-Linux WindowsAPP-Mac Microsoft Remote Desktop-Mac&amp;Windows UU远程 邮件 Mailspring 压缩 7zip Keka - Mac 清理工具 ccleaner-win&#x2F;Mac BleachBit-Linux CLI ffmpeg You-get yt-dlp curl 画图 Draw.io 终端 Sakura- Linux Windows Terminal Tabby- Mac iTerm2- Mac Kitty - Mac,Linux 游戏 steam 密码 KeePassXC 2fs 开发 Idea- Java DataGrip-数据库 vscode cursor  文档 Spring:Common Application Properties  DevDoc 录屏 OBS 存储 CyberDuck openList rclone 飞牛 网络工具 Clashx Clash Verge 有趣 电脑恶搞 收集了一些恶搞小网页，比如xp系统蓝屏、黑客界面等 neocities 上面托管了很多有趣的网站 奇趣网站收藏家 收藏了很多有趣的网站 勾栏 语雀 简书 知乎 bilibili GitHub 电影天堂  SmallPDF pdf24 小众 nginx nginx 可视化配置工具 Python PyTorch Miniforge3 uv 生活 Ventusky 风雨气温图 curl wttr.in 其他 switchhosts!：hosts文件 runcat Linux命令手册 carbon代码图片生成器 生成好看的代码图片 VisualVM "}]